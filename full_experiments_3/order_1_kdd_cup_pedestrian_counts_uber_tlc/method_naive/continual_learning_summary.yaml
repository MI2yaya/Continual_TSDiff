continual_learning_setup:
  method: naive
  method_params: {}
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_naive/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.34739400485128047
      NRMSE: 0.7188056646111465
      mean_wQuantileLoss: 0.26758701693180537
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4071304213721305
      - 0.28527926199603826
      - 0.24266602855641395
      - 0.21505996061023325
      - 0.21605609002290294
      - 0.19848138827364892
      - 0.20301890303380787
      - 0.19415317627135664
      - 0.19152033561840653
      - 0.18651806632988155
      - 0.18398446106584743
      - 0.18203601625282317
      - 0.18476871604798362
      - 0.18029593670507893
      - 0.17766290361760184
      - 0.17820188257610425
      - 0.1798101378371939
      - 0.18028255959507078
      - 0.17821022809948772
      - 0.17121266218600795
      - 0.17536392208421603
      - 0.1736987071344629
      - 0.17021458793897182
      - 0.17656294273911044
      - 0.17851688875816762
      - 0.16487188678001985
      - 0.16565894638188183
      - 0.1735006813541986
      - 0.1661145075922832
      - 0.1709712197771296
      - 0.16893084294861183
      - 0.17021388944704086
      - 0.16359280893811956
      - 0.1689486286486499
      - 0.1696487077861093
      - 0.16529412055388093
      - 0.16540814796462655
      - 0.1670446171774529
      - 0.1667629737057723
      - 0.16126685467315838
      - 0.16495300689712167
      - 0.16700761765241623
      - 0.16497744317166507
      - 0.1572569411364384
      - 0.16575651202583686
      - 0.16113951877923682
      - 0.1595075767254457
      - 0.1633632286102511
      - 0.1622137352824211
      - 0.16176278615603223
      - 0.1619499758235179
      - 0.16135911521269009
      - 0.15851540945004672
      - 0.16369654762092978
      - 0.1577631630934775
      - 0.16552169679198414
      - 0.15679583814926445
      - 0.1647327471873723
      - 0.1562289489666
      - 0.16268401162233204
      - 0.15558037388836965
      - 0.1630926663056016
      - 0.15786141314310953
      - 0.15922433219384402
      - 0.16236170602496713
      - 0.16468357242411003
      - 0.1604298867750913
      - 0.15830043330788612
      - 0.15910545754013583
      - 0.15832642320310697
      - 0.15830894868122414
      val_loss:
      - 0.29046722650527956
      - 0.28527926199603826
      - 0.24266602855641395
      - 0.21505996061023325
      - 0.21605609002290294
      - 0.19848138827364892
      - 0.20301890303380787
      - 0.19415317627135664
      - 0.19152033561840653
      - 0.18651806632988155
      - 0.18398446106584743
      - 0.18203601625282317
      - 0.18476871604798362
      - 0.18029593670507893
      - 0.17766290361760184
      - 0.17820188257610425
      - 0.1798101378371939
      - 0.18028255959507078
      - 0.17821022809948772
      - 0.17121266218600795
      - 0.17536392208421603
      - 0.1736987071344629
      - 0.17021458793897182
      - 0.17656294273911044
      - 0.17851688875816762
      - 0.16487188678001985
      - 0.16565894638188183
      - 0.1735006813541986
      - 0.1661145075922832
      - 0.1709712197771296
      - 0.16893084294861183
      - 0.17021388944704086
      - 0.16359280893811956
      - 0.1689486286486499
      - 0.1696487077861093
      - 0.16529412055388093
      - 0.16540814796462655
      - 0.1670446171774529
      - 0.1667629737057723
      - 0.16126685467315838
      - 0.16495300689712167
      - 0.16700761765241623
      - 0.16497744317166507
      - 0.1572569411364384
      - 0.16575651202583686
      - 0.16113951877923682
      - 0.1595075767254457
      - 0.1633632286102511
      - 0.1622137352824211
      - 0.16176278615603223
      - 0.1444309026002884
      - 0.16135911521269009
      - 0.15851540945004672
      - 0.16369654762092978
      - 0.1577631630934775
      - 0.16552169679198414
      - 0.15679583814926445
      - 0.1647327471873723
      - 0.1562289489666
      - 0.16268401162233204
      - 0.15558037388836965
      - 0.1630926663056016
      - 0.15786141314310953
      - 0.15922433219384402
      - 0.16236170602496713
      - 0.16468357242411003
      - 0.1604298867750913
      - 0.15830043330788612
      - 0.15910545754013583
      - 0.15832642320310697
      - 0.15830894868122414
  task_id: 1
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_naive/task_2_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.2015733539040493
      NRMSE: 0.6626560190893012
      mean_wQuantileLoss: 0.16389382062284538
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      - 116
      - 117
      - 118
      - 119
      - 120
      - 121
      train_loss:
      - 0.46971279568970203
      - 0.3547760280780494
      - 0.3004266796633601
      - 0.273361912695691
      - 0.23956346727209166
      - 0.21609683940187097
      - 0.21413406555075198
      - 0.1995313591323793
      - 0.193224280141294
      - 0.189871835464146
      - 0.18124978587729856
      - 0.17255381756694987
      - 0.20105875935405493
      - 0.167891169548966
      - 0.1765609317808412
      - 0.17904162680497393
      - 0.16830337955616415
      - 0.1718873076606542
      - 0.16368851228617132
      - 0.1759310716879554
      - 0.16593305621063337
      - 0.1612217336660251
      - 0.16438131436007097
      - 0.162341921008192
      - 0.16441687877522781
      - 0.16421757044736296
      - 0.16230671940138564
      - 0.1801832384080626
      - 0.16057952371193096
      - 0.15953893697587773
      - 0.1573918325593695
      - 0.16490273654926568
      - 0.15796490316279233
      - 0.15641799132572487
      - 0.16028346144594252
      - 0.1609787450870499
      - 0.15603159985039383
      - 0.19822696381015703
      - 0.14838174311444163
      - 0.15955337701598182
      - 0.15380328032188118
      - 0.15204071352491155
      - 0.15106750914128497
      - 0.15060302993515506
      - 0.16402571921935305
      - 0.15460972796427086
      - 0.15101167641114444
      - 0.14860013773432001
      - 0.15258208505110815
      - 0.14737110037822276
      - 0.15558312478242442
      - 0.15329283685423434
      - 0.17381909274263307
      - 0.1503361336654052
      - 0.16300219699041918
      - 0.1484236444812268
      - 0.14729454350890592
      - 0.1515116976806894
      - 0.14740410720696673
      - 0.15116100438171998
      - 0.17849109059898183
      - 0.14569658046821132
      - 0.15116272994782776
      - 0.14672659104689956
      - 0.1515019029029645
      - 0.15381403401261196
      - 0.14943323750048876
      - 0.14903109468286857
      - 0.1503415214829147
      - 0.15735191735439003
      - 0.14605691912584007
      - 0.14499489532317966
      - 0.14332532574189827
      - 0.1540230906684883
      - 0.14834080898435786
      - 0.1435633059591055
      - 0.1460342223290354
      - 0.1492575831944123
      - 0.14507533254800364
      - 0.14490836358163506
      - 0.14814023388316855
      - 0.14642728416947648
      - 0.14601820515235886
      - 0.14421909354859963
      - 0.14609133213525638
      - 0.14474436646560207
      - 0.14352551277261227
      - 0.14498970133718103
      - 0.14327389450045303
      - 0.14628166414331645
      - 0.14787856105249375
      - 0.14331691630650312
      - 0.1395918867783621
      - 0.15271572500932962
      - 0.1433090350474231
      - 0.1405329536064528
      - 0.14360006287461147
      - 0.14157964230980724
      - 0.14432490966282785
      - 0.14108891604701057
      - 0.14347641461063176
      - 0.140820037398953
      - 0.13798238715389743
      - 0.14147006877465174
      - 0.1423879589419812
      - 0.14332879590801895
      - 0.1412041811272502
      - 0.14306300587486476
      - 0.14150583010632545
      - 0.14106339769205078
      - 0.14272843895014375
      - 0.14445923559833318
      - 0.13713180675404146
      - 0.14237331179901958
      - 0.1595158299896866
      - 0.14441792014986277
      - 0.141792525711935
      - 0.13996281108120456
      - 0.14604901860002428
      - 0.1416218534577638
      - 0.14370881975628436
      val_loss:
      - 0.42796799540519714
      - 0.3547760280780494
      - 0.3004266796633601
      - 0.273361912695691
      - 0.23956346727209166
      - 0.21609683940187097
      - 0.21413406555075198
      - 0.1995313591323793
      - 0.193224280141294
      - 0.189871835464146
      - 0.18124978587729856
      - 0.17255381756694987
      - 0.20105875935405493
      - 0.167891169548966
      - 0.1765609317808412
      - 0.17904162680497393
      - 0.16830337955616415
      - 0.1718873076606542
      - 0.16368851228617132
      - 0.1759310716879554
      - 0.16593305621063337
      - 0.1612217336660251
      - 0.16438131436007097
      - 0.162341921008192
      - 0.16441687877522781
      - 0.16421757044736296
      - 0.16230671940138564
      - 0.1801832384080626
      - 0.16057952371193096
      - 0.15953893697587773
      - 0.1573918325593695
      - 0.16490273654926568
      - 0.15796490316279233
      - 0.15641799132572487
      - 0.16028346144594252
      - 0.1609787450870499
      - 0.15603159985039383
      - 0.19822696381015703
      - 0.14838174311444163
      - 0.15955337701598182
      - 0.15380328032188118
      - 0.15204071352491155
      - 0.15106750914128497
      - 0.15060302993515506
      - 0.16402571921935305
      - 0.15460972796427086
      - 0.15101167641114444
      - 0.14860013773432001
      - 0.15258208505110815
      - 0.14737110037822276
      - 0.22235458344221115
      - 0.15329283685423434
      - 0.17381909274263307
      - 0.1503361336654052
      - 0.16300219699041918
      - 0.1484236444812268
      - 0.14729454350890592
      - 0.1515116976806894
      - 0.14740410720696673
      - 0.15116100438171998
      - 0.17849109059898183
      - 0.14569658046821132
      - 0.15116272994782776
      - 0.14672659104689956
      - 0.1515019029029645
      - 0.15381403401261196
      - 0.14943323750048876
      - 0.14903109468286857
      - 0.1503415214829147
      - 0.15735191735439003
      - 0.14605691912584007
      - 0.14499489532317966
      - 0.14332532574189827
      - 0.1540230906684883
      - 0.14834080898435786
      - 0.1435633059591055
      - 0.1460342223290354
      - 0.1492575831944123
      - 0.14507533254800364
      - 0.14490836358163506
      - 0.14814023388316855
      - 0.14642728416947648
      - 0.14601820515235886
      - 0.14421909354859963
      - 0.14609133213525638
      - 0.14474436646560207
      - 0.14352551277261227
      - 0.14498970133718103
      - 0.14327389450045303
      - 0.14628166414331645
      - 0.14787856105249375
      - 0.14331691630650312
      - 0.1395918867783621
      - 0.15271572500932962
      - 0.1433090350474231
      - 0.1405329536064528
      - 0.14360006287461147
      - 0.14157964230980724
      - 0.14432490966282785
      - 0.14108891604701057
      - 0.12021987605839968
      - 0.140820037398953
      - 0.13798238715389743
      - 0.14147006877465174
      - 0.1423879589419812
      - 0.14332879590801895
      - 0.1412041811272502
      - 0.14306300587486476
      - 0.14150583010632545
      - 0.14106339769205078
      - 0.14272843895014375
      - 0.14445923559833318
      - 0.13713180675404146
      - 0.14237331179901958
      - 0.1595158299896866
      - 0.14441792014986277
      - 0.141792525711935
      - 0.13996281108120456
      - 0.14604901860002428
      - 0.1416218534577638
      - 0.14370881975628436
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_naive/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.21351108899104643
      NRMSE: 0.5104894769455925
      mean_wQuantileLoss: 0.17887647979636442
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.48433221294544637
      - 0.33403141633607447
      - 0.3131067258073017
      - 0.2958568923640996
      - 0.297230813652277
      - 0.292547408840619
      - 0.28214989916887134
      - 0.28201693727169186
      - 0.280709893675521
      - 0.2753082885174081
      - 0.27640733669977635
      - 0.27388142910785973
      - 0.27083800605032593
      - 0.2613701153313741
      - 0.27493930305354297
      - 0.26164260879158974
      - 0.255084571428597
      - 0.25982259796001017
      - 0.2589556919410825
      - 0.255039791110903
      - 0.26054051180835813
      - 0.2613093226682395
      - 0.25550115515943617
      - 0.25824655033648014
      - 0.25316877861041576
      - 0.25194515951443464
      - 0.24848196026869118
      - 0.25488581240642816
      - 0.24755642423406243
      - 0.24521922110579908
      - 0.24983402085490525
      - 0.2547386917285621
      - 0.24694262305274606
      - 0.25198227702639997
      - 0.24989217182155699
      - 0.2532316705910489
      - 0.2511353304143995
      - 0.24903746438212693
      - 0.24791593314148486
      - 0.24318610841874033
      - 0.2396399644203484
      - 0.24346982536371797
      - 0.2490317435003817
      - 0.24108116573188454
      - 0.24457720934879035
      - 0.2428114723879844
      - 0.24722837121225893
      - 0.24342679796973243
      - 0.24545865424443036
      - 0.24533851246815175
      - 0.2417691828450188
      - 0.23898307839408517
      - 0.2439043162157759
      - 0.23776897066272795
      - 0.23836756544187665
      - 0.2395217086886987
      - 0.2371548149967566
      - 0.23953490238636732
      - 0.23277707886882126
      - 0.23932296922430396
      - 0.2423131826799363
      - 0.23970433126669377
      - 0.2419684611959383
      - 0.24296482163481414
      - 0.2365401383722201
      - 0.24094969767611474
      - 0.23270193475764245
      - 0.23421606898773462
      - 0.2356297728838399
      - 0.23277329292614013
      - 0.2365875001414679
      val_loss:
      - 0.31128708720207215
      - 0.33403141633607447
      - 0.3131067258073017
      - 0.2958568923640996
      - 0.297230813652277
      - 0.292547408840619
      - 0.28214989916887134
      - 0.28201693727169186
      - 0.280709893675521
      - 0.2753082885174081
      - 0.27640733669977635
      - 0.27388142910785973
      - 0.27083800605032593
      - 0.2613701153313741
      - 0.27493930305354297
      - 0.26164260879158974
      - 0.255084571428597
      - 0.25982259796001017
      - 0.2589556919410825
      - 0.255039791110903
      - 0.26054051180835813
      - 0.2613093226682395
      - 0.25550115515943617
      - 0.25824655033648014
      - 0.25316877861041576
      - 0.25194515951443464
      - 0.24848196026869118
      - 0.25488581240642816
      - 0.24755642423406243
      - 0.24521922110579908
      - 0.24983402085490525
      - 0.2547386917285621
      - 0.24694262305274606
      - 0.25198227702639997
      - 0.24989217182155699
      - 0.2532316705910489
      - 0.2511353304143995
      - 0.24903746438212693
      - 0.24791593314148486
      - 0.24318610841874033
      - 0.2396399644203484
      - 0.24346982536371797
      - 0.2490317435003817
      - 0.24108116573188454
      - 0.24457720934879035
      - 0.2428114723879844
      - 0.24722837121225893
      - 0.24342679796973243
      - 0.24545865424443036
      - 0.24533851246815175
      - 0.2187459796667099
      - 0.23898307839408517
      - 0.2439043162157759
      - 0.23776897066272795
      - 0.23836756544187665
      - 0.2395217086886987
      - 0.2371548149967566
      - 0.23953490238636732
      - 0.23277707886882126
      - 0.23932296922430396
      - 0.2423131826799363
      - 0.23970433126669377
      - 0.2419684611959383
      - 0.24296482163481414
      - 0.2365401383722201
      - 0.24094969767611474
      - 0.23270193475764245
      - 0.23421606898773462
      - 0.2356297728838399
      - 0.23277329292614013
      - 0.2365875001414679
  task_id: 3
