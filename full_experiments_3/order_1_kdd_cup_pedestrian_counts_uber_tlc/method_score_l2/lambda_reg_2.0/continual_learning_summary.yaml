continual_learning_setup:
  method: score_l2
  method_params:
    lambda_reg: 2.0
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l2/lambda_reg_2.0/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 2.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.31267569574267423
      NRMSE: 0.7068551756779399
      mean_wQuantileLoss: 0.25110817261884477
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4161277050152421
      - 0.27004966139793396
      - 0.2368734385818243
      - 0.21728608722332865
      - 0.20935172599274665
      - 0.20740314840804785
      - 0.19573340349597856
      - 0.19092796050244942
      - 0.19389054726343602
      - 0.19020363525487483
      - 0.18304925470147282
      - 0.18579669785685837
      - 0.1797391353175044
      - 0.18227577704237774
      - 0.18053471256280318
      - 0.17388750170357525
      - 0.17933706962503493
      - 0.17723929969361052
      - 0.18371018284233287
      - 0.17176293837837875
      - 0.17338118271436542
      - 0.17617015924770385
      - 0.17548610438825563
      - 0.17341616400517523
      - 0.16847893421072513
      - 0.17226656741695479
      - 0.17405684746336192
      - 0.17034641391364858
      - 0.1647338811890222
      - 0.16615993774030358
      - 0.1695096924668178
      - 0.16863323136931285
      - 0.16417237895075232
      - 0.16352225880837068
      - 0.16569685022113845
      - 0.1675483065773733
      - 0.16783353307982907
      - 0.1663987023057416
      - 0.1684785135439597
      - 0.1590142489876598
      - 0.16438036371255293
      - 0.16127083642641082
      - 0.16482846904546022
      - 0.16324944735970348
      - 0.1616766710067168
      - 0.1636760708061047
      - 0.1645827916217968
      - 0.157061283942312
      - 0.1629025946604088
      - 0.16520623327232897
      - 0.16221733787097037
      - 0.1581972148269415
      - 0.16081720759393647
      - 0.16333567193942145
      - 0.16025945910951123
      - 0.1619686996564269
      - 0.16039539239136502
      - 0.16150188486790285
      - 0.15892398112919182
      - 0.16061868699034676
      - 0.16255757899489254
      - 0.15756963420426473
      - 0.15772771154297516
      - 0.16077282297192141
      - 0.16077778855105862
      - 0.16296752949710935
      - 0.1617548872018233
      - 0.16232910781400278
      - 0.15704544930486009
      - 0.1594772802782245
      - 0.15894133027177304
      val_loss:
      - 0.27733382284641267
      - 0.27004966139793396
      - 0.2368734385818243
      - 0.21728608722332865
      - 0.20935172599274665
      - 0.20740314840804785
      - 0.19573340349597856
      - 0.19092796050244942
      - 0.19389054726343602
      - 0.19020363525487483
      - 0.18304925470147282
      - 0.18579669785685837
      - 0.1797391353175044
      - 0.18227577704237774
      - 0.18053471256280318
      - 0.17388750170357525
      - 0.17933706962503493
      - 0.17723929969361052
      - 0.18371018284233287
      - 0.17176293837837875
      - 0.17338118271436542
      - 0.17617015924770385
      - 0.17548610438825563
      - 0.17341616400517523
      - 0.16847893421072513
      - 0.17226656741695479
      - 0.17405684746336192
      - 0.17034641391364858
      - 0.1647338811890222
      - 0.16615993774030358
      - 0.1695096924668178
      - 0.16863323136931285
      - 0.16417237895075232
      - 0.16352225880837068
      - 0.16569685022113845
      - 0.1675483065773733
      - 0.16783353307982907
      - 0.1663987023057416
      - 0.1684785135439597
      - 0.1590142489876598
      - 0.16438036371255293
      - 0.16127083642641082
      - 0.16482846904546022
      - 0.16324944735970348
      - 0.1616766710067168
      - 0.1636760708061047
      - 0.1645827916217968
      - 0.157061283942312
      - 0.1629025946604088
      - 0.16520623327232897
      - 0.1332031548023224
      - 0.1581972148269415
      - 0.16081720759393647
      - 0.16333567193942145
      - 0.16025945910951123
      - 0.1619686996564269
      - 0.16039539239136502
      - 0.16150188486790285
      - 0.15892398112919182
      - 0.16061868699034676
      - 0.16255757899489254
      - 0.15756963420426473
      - 0.15772771154297516
      - 0.16077282297192141
      - 0.16077778855105862
      - 0.16296752949710935
      - 0.1617548872018233
      - 0.16232910781400278
      - 0.15704544930486009
      - 0.1594772802782245
      - 0.15894133027177304
  task_id: 1
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l2/lambda_reg_2.0/task_2_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 2.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.2827430814298843
      NRMSE: 0.9345601973806255
      mean_wQuantileLoss: 0.2644568657426973
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 1.181275733280927
      - 0.6902820584364235
      - 0.556256755720824
      - 0.4390013620723039
      - 0.38625984149985015
      - 0.36076184490229934
      - 0.32360276335384697
      - 0.3084407513961196
      - 0.2919658550526947
      - 0.294103323481977
      - 0.29567847249563783
      - 0.29016775637865067
      - 0.274624532321468
      - 0.26709872588980943
      - 0.2762498985975981
      - 0.2651928038103506
      - 0.2666519405320287
      - 0.2659180148039013
      - 0.2609300451586023
      - 0.25794783676974475
      - 0.265686332131736
      val_loss:
      - 0.24536889791488647
      - 0.6902820584364235
      - 0.556256755720824
      - 0.4390013620723039
      - 0.38625984149985015
      - 0.36076184490229934
      - 0.32360276335384697
      - 0.3084407513961196
      - 0.2919658550526947
      - 0.294103323481977
      - 0.29567847249563783
      - 0.29016775637865067
      - 0.274624532321468
      - 0.26709872588980943
      - 0.2762498985975981
      - 0.2651928038103506
      - 0.2666519405320287
      - 0.2659180148039013
      - 0.2609300451586023
      - 0.25794783676974475
      - 0.265686332131736
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l2/lambda_reg_2.0/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 2.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 1.5484667082294725
      NRMSE: 8.445803124828206
      mean_wQuantileLoss: 1.339160571123171
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 1.3313068533316255
      - 1.2126538725569844
      - 1.1938026444986463
      - 1.1846565324813128
      - 1.1779759619385004
      - 1.1682224003598094
      - 1.179844862781465
      - 1.1756738657131791
      - 1.1776336980983615
      - 1.1607908690348268
      - 1.1691287346184254
      - 1.1745882062241435
      - 1.1588326655328274
      - 1.1682106284424663
      - 1.1657374985516071
      - 1.167507148347795
      - 1.1644386611878872
      - 1.1577189536765218
      - 1.159117789939046
      - 1.1691815350204706
      - 1.1605238504707813
      val_loss:
      - 0.8726575255393982
      - 1.2126538725569844
      - 1.1938026444986463
      - 1.1846565324813128
      - 1.1779759619385004
      - 1.1682224003598094
      - 1.179844862781465
      - 1.1756738657131791
      - 1.1776336980983615
      - 1.1607908690348268
      - 1.1691287346184254
      - 1.1745882062241435
      - 1.1588326655328274
      - 1.1682106284424663
      - 1.1657374985516071
      - 1.167507148347795
      - 1.1644386611878872
      - 1.1577189536765218
      - 1.159117789939046
      - 1.1691815350204706
      - 1.1605238504707813
  task_id: 3
