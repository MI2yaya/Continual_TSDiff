continual_learning_setup:
  method: dropout
  method_params:
    dropout_rate: 0.5
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.5/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.5
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.355291024989417
      NRMSE: 0.7354852035425068
      mean_wQuantileLoss: 0.2863003497421912
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4581599289085716
      - 0.36007781606167555
      - 0.3223032715031877
      - 0.2947451549116522
      - 0.28325667220633477
      - 0.270922445692122
      - 0.2651558208744973
      - 0.25711141573265195
      - 0.2567546289646998
      - 0.2453160877339542
      - 0.24991518852766603
      - 0.24464830022770911
      - 0.23682469851337373
      - 0.2465636256383732
      - 0.23146346327848732
      - 0.24051144975237548
      - 0.22994748887140304
      - 0.2299838779726997
      - 0.22841000254265964
      - 0.22925040079280734
      - 0.2280088788829744
      - 0.22776182857342064
      - 0.23009055480360985
      - 0.2283089489210397
      - 0.2224342825356871
      - 0.22708970005623996
      - 0.22986576217226684
      - 0.225453847553581
      - 0.22934042196720839
      - 0.225099740549922
      - 0.22737351001705974
      - 0.22831209260039032
      - 0.22640894853975624
      - 0.22876541153527796
      - 0.22506438253913075
      - 0.22002241504378617
      - 0.22274471796117723
      - 0.22703482769429684
      - 0.22079740662593395
      - 0.22196802066173404
      - 0.22668363410048187
      - 0.23268415872007608
      - 0.22349436522927135
      - 0.2276711668819189
      - 0.21864980296231806
      - 0.2229852315504104
      - 0.22121275076642632
      - 0.22345436445903033
      - 0.21913239499554038
      - 0.22463056538254023
      - 0.2296894183382392
      - 0.21331886091502383
      - 0.22404011490289122
      - 0.22568333242088556
      - 0.22309225413482636
      - 0.22489133989438415
      - 0.21931050857529044
      - 0.220299051143229
      - 0.2221149797551334
      - 0.2214741628849879
      - 0.21917376189958304
      - 0.22666511044371873
      - 0.2190618192544207
      - 0.22504507552366704
      - 0.21376015641726553
      - 0.2199845340801403
      - 0.22058254678267986
      - 0.22331166290678084
      - 0.2208995465771295
      - 0.22075506125111133
      - 0.2209512849804014
      val_loss:
      - 0.3503122627735138
      - 0.36007781606167555
      - 0.3223032715031877
      - 0.2947451549116522
      - 0.28325667220633477
      - 0.270922445692122
      - 0.2651558208744973
      - 0.25711141573265195
      - 0.2567546289646998
      - 0.2453160877339542
      - 0.24991518852766603
      - 0.24464830022770911
      - 0.23682469851337373
      - 0.2465636256383732
      - 0.23146346327848732
      - 0.24051144975237548
      - 0.22994748887140304
      - 0.2299838779726997
      - 0.22841000254265964
      - 0.22925040079280734
      - 0.2280088788829744
      - 0.22776182857342064
      - 0.23009055480360985
      - 0.2283089489210397
      - 0.2224342825356871
      - 0.22708970005623996
      - 0.22986576217226684
      - 0.225453847553581
      - 0.22934042196720839
      - 0.225099740549922
      - 0.22737351001705974
      - 0.22831209260039032
      - 0.22640894853975624
      - 0.22876541153527796
      - 0.22506438253913075
      - 0.22002241504378617
      - 0.22274471796117723
      - 0.22703482769429684
      - 0.22079740662593395
      - 0.22196802066173404
      - 0.22668363410048187
      - 0.23268415872007608
      - 0.22349436522927135
      - 0.2276711668819189
      - 0.21864980296231806
      - 0.2229852315504104
      - 0.22121275076642632
      - 0.22345436445903033
      - 0.21913239499554038
      - 0.22463056538254023
      - 0.21245334446430206
      - 0.21331886091502383
      - 0.22404011490289122
      - 0.22568333242088556
      - 0.22309225413482636
      - 0.22489133989438415
      - 0.21931050857529044
      - 0.220299051143229
      - 0.2221149797551334
      - 0.2214741628849879
      - 0.21917376189958304
      - 0.22666511044371873
      - 0.2190618192544207
      - 0.22504507552366704
      - 0.21376015641726553
      - 0.2199845340801403
      - 0.22058254678267986
      - 0.22331166290678084
      - 0.2208995465771295
      - 0.22075506125111133
      - 0.2209512849804014
  task_id: 1
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.5/task_2_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.5
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.20145312962617767
      NRMSE: 0.6860607034486297
      mean_wQuantileLoss: 0.17535997856950655
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.585579312639311
      - 0.4282999667339027
      - 0.3875215598382056
      - 0.3366605171468109
      - 0.3198526616906747
      - 0.33883512229658663
      - 0.3230833221459761
      - 0.28337727417238057
      - 0.2825712038902566
      - 0.2776147242402658
      - 0.270323233329691
      - 0.2717597287846729
      - 0.26055160630494356
      - 0.24733539146836847
      - 0.27924619952682406
      - 0.23764351930003613
      - 0.24289326835423708
      - 0.24186789861414582
      - 0.22918081609532237
      - 0.2368980428436771
      - 0.24596390925580636
      - 0.23822687671054155
      - 0.22361900680698454
      - 0.226655084756203
      - 0.22322905785404146
      - 0.2220106675522402
      - 0.21665397833567113
      - 0.2111093860003166
      - 0.2167035680031404
      - 0.21837754402076825
      - 0.22154304129071534
      - 0.22212644707178697
      - 0.20960355387069285
      - 0.22720397834200412
      - 0.2283066826639697
      - 0.2364802323281765
      - 0.2113564774626866
      - 0.21956375904846936
      - 0.20980706252157688
      - 0.21264487481676042
      - 0.20967193064279854
      - 0.20875292021082714
      - 0.21244886587373912
      - 0.20546446775551885
      - 0.20947411144152284
      - 0.22530038352124393
      - 0.2073381709633395
      - 0.20087234524544328
      - 0.20439215010264888
      - 0.21652335324324667
      - 0.21233207324985415
      - 0.2040851585334167
      - 0.2113891476765275
      - 0.20388784422539175
      - 0.20975568529684097
      - 0.20711500494508073
      - 0.20813176757656038
      - 0.20584528689505532
      - 0.21187440201174468
      - 0.20442732225637883
      - 0.19797049241606146
      - 0.20308598247356713
      - 0.20117832999676466
      - 0.20396587543655187
      - 0.2034885900793597
      - 0.20341427985113114
      - 0.22675900429021567
      - 0.20320463203825057
      - 0.19842903304379433
      - 0.1963008667808026
      - 0.1930355915101245
      val_loss:
      - 0.28358013927936554
      - 0.4282999667339027
      - 0.3875215598382056
      - 0.3366605171468109
      - 0.3198526616906747
      - 0.33883512229658663
      - 0.3230833221459761
      - 0.28337727417238057
      - 0.2825712038902566
      - 0.2776147242402658
      - 0.270323233329691
      - 0.2717597287846729
      - 0.26055160630494356
      - 0.24733539146836847
      - 0.27924619952682406
      - 0.23764351930003613
      - 0.24289326835423708
      - 0.24186789861414582
      - 0.22918081609532237
      - 0.2368980428436771
      - 0.24596390925580636
      - 0.23822687671054155
      - 0.22361900680698454
      - 0.226655084756203
      - 0.22322905785404146
      - 0.2220106675522402
      - 0.21665397833567113
      - 0.2111093860003166
      - 0.2167035680031404
      - 0.21837754402076825
      - 0.22154304129071534
      - 0.22212644707178697
      - 0.20960355387069285
      - 0.22720397834200412
      - 0.2283066826639697
      - 0.2364802323281765
      - 0.2113564774626866
      - 0.21956375904846936
      - 0.20980706252157688
      - 0.21264487481676042
      - 0.20967193064279854
      - 0.20875292021082714
      - 0.21244886587373912
      - 0.20546446775551885
      - 0.20947411144152284
      - 0.22530038352124393
      - 0.2073381709633395
      - 0.20087234524544328
      - 0.20439215010264888
      - 0.21652335324324667
      - 0.15209368616342545
      - 0.2040851585334167
      - 0.2113891476765275
      - 0.20388784422539175
      - 0.20975568529684097
      - 0.20711500494508073
      - 0.20813176757656038
      - 0.20584528689505532
      - 0.21187440201174468
      - 0.20442732225637883
      - 0.19797049241606146
      - 0.20308598247356713
      - 0.20117832999676466
      - 0.20396587543655187
      - 0.2034885900793597
      - 0.20341427985113114
      - 0.22675900429021567
      - 0.20320463203825057
      - 0.19842903304379433
      - 0.1963008667808026
      - 0.1930355915101245
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.5/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.5
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.22048577206793257
      NRMSE: 0.517842417734588
      mean_wQuantileLoss: 0.21281126778441417
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.6191778641659766
      - 0.39986249268986285
      - 0.3645881535485387
      - 0.3485039244405925
      - 0.34340164926834404
      - 0.3277444300474599
      - 0.33306036959402263
      - 0.3273487974656746
      - 0.32292675331700593
      - 0.3242090141866356
      - 0.31707629666198045
      - 0.3317758607445285
      - 0.318921786500141
      - 0.3279491744469851
      - 0.3231860164087266
      - 0.3208898262819275
      - 0.3233683422440663
      - 0.3139306396478787
      - 0.31659511767793447
      - 0.32085021841339767
      - 0.3178973355097696
      - 0.3133719122270122
      - 0.31673505099024624
      - 0.3165527111850679
      - 0.3163114357739687
      - 0.3096075071953237
      - 0.31260336725972593
      - 0.3061991384020075
      - 0.309769784566015
      - 0.30762002407573164
      - 0.3105088750598952
      - 0.3107579151401296
      - 0.30986715212929994
      - 0.31399493641220033
      - 0.3071289841318503
      - 0.30248137121088803
      - 0.3114729153458029
      - 0.3102694775443524
      - 0.30168858787510544
      - 0.307789474260062
      - 0.3082723297411576
      - 0.3019569533644244
      - 0.3035917504457757
      - 0.30802089208737016
      - 0.30701217404566705
      - 0.3016385925002396
      - 0.30393165023997426
      - 0.30532070249319077
      - 0.3033899406436831
      - 0.3043239397229627
      - 0.30272956483531743
      - 0.29787452262826264
      - 0.301713002147153
      - 0.29964553739409894
      - 0.3024547966197133
      - 0.29658305703196675
      - 0.30208904843311757
      - 0.2969549377448857
      - 0.3032957963878289
      - 0.2994294709060341
      - 0.3025183192221448
      - 0.3007279331795871
      - 0.30110124906059355
      - 0.3046399870654568
      - 0.3010265009943396
      - 0.29939255851786584
      - 0.3035165687324479
      - 0.30076730088330805
      - 0.29680839157663286
      - 0.29668487259186804
      - 0.29620078240986913
      val_loss:
      - 0.3423885881900787
      - 0.39986249268986285
      - 0.3645881535485387
      - 0.3485039244405925
      - 0.34340164926834404
      - 0.3277444300474599
      - 0.33306036959402263
      - 0.3273487974656746
      - 0.32292675331700593
      - 0.3242090141866356
      - 0.31707629666198045
      - 0.3317758607445285
      - 0.318921786500141
      - 0.3279491744469851
      - 0.3231860164087266
      - 0.3208898262819275
      - 0.3233683422440663
      - 0.3139306396478787
      - 0.31659511767793447
      - 0.32085021841339767
      - 0.3178973355097696
      - 0.3133719122270122
      - 0.31673505099024624
      - 0.3165527111850679
      - 0.3163114357739687
      - 0.3096075071953237
      - 0.31260336725972593
      - 0.3061991384020075
      - 0.309769784566015
      - 0.30762002407573164
      - 0.3105088750598952
      - 0.3107579151401296
      - 0.30986715212929994
      - 0.31399493641220033
      - 0.3071289841318503
      - 0.30248137121088803
      - 0.3114729153458029
      - 0.3102694775443524
      - 0.30168858787510544
      - 0.307789474260062
      - 0.3082723297411576
      - 0.3019569533644244
      - 0.3035917504457757
      - 0.30802089208737016
      - 0.30701217404566705
      - 0.3016385925002396
      - 0.30393165023997426
      - 0.30532070249319077
      - 0.3033899406436831
      - 0.3043239397229627
      - 0.21284884661436082
      - 0.29787452262826264
      - 0.301713002147153
      - 0.29964553739409894
      - 0.3024547966197133
      - 0.29658305703196675
      - 0.30208904843311757
      - 0.2969549377448857
      - 0.3032957963878289
      - 0.2994294709060341
      - 0.3025183192221448
      - 0.3007279331795871
      - 0.30110124906059355
      - 0.3046399870654568
      - 0.3010265009943396
      - 0.29939255851786584
      - 0.3035165687324479
      - 0.30076730088330805
      - 0.29680839157663286
      - 0.29668487259186804
      - 0.29620078240986913
  task_id: 3
