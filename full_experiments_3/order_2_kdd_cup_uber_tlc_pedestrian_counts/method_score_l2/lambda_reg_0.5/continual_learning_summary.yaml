continual_learning_setup:
  method: score_l2
  method_params:
    lambda_reg: 0.5
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
  - train_pedestrian_counts.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l2/lambda_reg_0.5/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.5
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3912004987316446
      NRMSE: 0.7774263880021617
      mean_wQuantileLoss: 0.29754406575609565
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4280361538985744
      - 0.28391120361629874
      - 0.2423270115396008
      - 0.2083457165863365
      - 0.20927821251098067
      - 0.20176070043817163
      - 0.19165034359320998
      - 0.19180097640492022
      - 0.18660879897652194
      - 0.18124788830755278
      - 0.17885741614736617
      - 0.1782053512870334
      - 0.1768820978468284
      - 0.17677831201581284
      - 0.1734731974429451
      - 0.18349962768843397
      - 0.17673284362535924
      - 0.17956552788382396
      - 0.1739857184002176
      - 0.17672913620481268
      - 0.17351514199981466
      - 0.17144885554444045
      - 0.1713966192328371
      - 0.17057368432870135
      - 0.16891000285977498
      - 0.17109000805066898
      - 0.1704798226710409
      - 0.16597809334052727
      - 0.1652154975454323
      - 0.1684174414840527
      - 0.16811600647633895
      - 0.16863951383857056
      - 0.16474481055047363
      - 0.16823014750843868
      - 0.16804955899715424
      - 0.16430263698566705
      - 0.16851679293904454
      - 0.1677783693303354
      - 0.15855577791808173
      - 0.1614914411911741
      - 0.16650803503580391
      - 0.15917215863009915
      - 0.15893435559701174
      - 0.16108978481497616
      - 0.16624581802170724
      - 0.1636455402476713
      - 0.16277237585745752
      - 0.161650893103797
      - 0.16177321603754535
      - 0.16049164632568136
      - 0.15892936353338882
      - 0.16536471142899245
      - 0.16051639837678522
      - 0.1632599889417179
      - 0.16207558737369254
      - 0.16261932119959965
      - 0.15928595542209223
      - 0.15822122676763684
      - 0.16045501740882173
      - 0.1613001208170317
      - 0.16282638412667438
      - 0.1561575253144838
      - 0.16349917865591124
      - 0.1540385540574789
      - 0.161450844083447
      - 0.15820199978770688
      - 0.15685528301401064
      - 0.16245968703879043
      - 0.1513674622401595
      - 0.15866934921359643
      - 0.15515394124668092
      val_loss:
      - 0.30284228920936584
      - 0.28391120361629874
      - 0.2423270115396008
      - 0.2083457165863365
      - 0.20927821251098067
      - 0.20176070043817163
      - 0.19165034359320998
      - 0.19180097640492022
      - 0.18660879897652194
      - 0.18124788830755278
      - 0.17885741614736617
      - 0.1782053512870334
      - 0.1768820978468284
      - 0.17677831201581284
      - 0.1734731974429451
      - 0.18349962768843397
      - 0.17673284362535924
      - 0.17956552788382396
      - 0.1739857184002176
      - 0.17672913620481268
      - 0.17351514199981466
      - 0.17144885554444045
      - 0.1713966192328371
      - 0.17057368432870135
      - 0.16891000285977498
      - 0.17109000805066898
      - 0.1704798226710409
      - 0.16597809334052727
      - 0.1652154975454323
      - 0.1684174414840527
      - 0.16811600647633895
      - 0.16863951383857056
      - 0.16474481055047363
      - 0.16823014750843868
      - 0.16804955899715424
      - 0.16430263698566705
      - 0.16851679293904454
      - 0.1677783693303354
      - 0.15855577791808173
      - 0.1614914411911741
      - 0.16650803503580391
      - 0.15917215863009915
      - 0.15893435559701174
      - 0.16108978481497616
      - 0.16624581802170724
      - 0.1636455402476713
      - 0.16277237585745752
      - 0.161650893103797
      - 0.16177321603754535
      - 0.16049164632568136
      - 0.1370561018586159
      - 0.16536471142899245
      - 0.16051639837678522
      - 0.1632599889417179
      - 0.16207558737369254
      - 0.16261932119959965
      - 0.15928595542209223
      - 0.15822122676763684
      - 0.16045501740882173
      - 0.1613001208170317
      - 0.16282638412667438
      - 0.1561575253144838
      - 0.16349917865591124
      - 0.1540385540574789
      - 0.161450844083447
      - 0.15820199978770688
      - 0.15685528301401064
      - 0.16245968703879043
      - 0.1513674622401595
      - 0.15866934921359643
      - 0.15515394124668092
  task_id: 1
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l2/lambda_reg_0.5/task_2_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.5
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.42681086806479596
      NRMSE: 2.70972467428658
      mean_wQuantileLoss: 0.42626453675664283
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 1.3731345375999808
      - 1.1797236404381692
      - 1.1066914862021804
      - 1.0920213041827083
      - 1.0976038053631783
      - 1.0706711080856621
      - 1.07243618555367
      - 1.0704591046087444
      - 1.0649821921251714
      - 1.058528314344585
      - 1.042040047235787
      - 1.0638699452392757
      - 1.054554708302021
      - 1.042503611650318
      - 1.0564930690452456
      - 1.0392570197582245
      - 1.046735371928662
      - 1.0366239664144814
      - 1.061716241762042
      - 1.0698771900497377
      - 1.044578047003597
      val_loss:
      - 0.42711768448352816
      - 1.1797236404381692
      - 1.1066914862021804
      - 1.0920213041827083
      - 1.0976038053631783
      - 1.0706711080856621
      - 1.07243618555367
      - 1.0704591046087444
      - 1.0649821921251714
      - 1.058528314344585
      - 1.042040047235787
      - 1.0638699452392757
      - 1.054554708302021
      - 1.042503611650318
      - 1.0564930690452456
      - 1.0392570197582245
      - 1.046735371928662
      - 1.0366239664144814
      - 1.061716241762042
      - 1.0698771900497377
      - 1.044578047003597
  task_id: 2
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l2/lambda_reg_0.5/task_3_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.5
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3541065301604045
      NRMSE: 1.0753997526523165
      mean_wQuantileLoss: 0.2805287569455477
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 0.7370230876840651
      - 0.6005076162982732
      - 0.5441438241396099
      - 0.5315010347403586
      - 0.4886483761947602
      - 0.48509873100556433
      - 0.4983766933437437
      - 0.4984694488812238
      - 0.4772480851970613
      - 0.4820192533079535
      - 0.4631032778415829
      - 0.46591233904473484
      - 0.4653375323396176
      - 0.4718158240430057
      - 0.4753877962939441
      - 0.47173435031436384
      - 0.46366876480169594
      - 0.462411638116464
      - 0.4529227097518742
      - 0.4678453153464943
      - 0.4687897216062993
      val_loss:
      - 0.44456747174263
      - 0.6005076162982732
      - 0.5441438241396099
      - 0.5315010347403586
      - 0.4886483761947602
      - 0.48509873100556433
      - 0.4983766933437437
      - 0.4984694488812238
      - 0.4772480851970613
      - 0.4820192533079535
      - 0.4631032778415829
      - 0.46591233904473484
      - 0.4653375323396176
      - 0.4718158240430057
      - 0.4753877962939441
      - 0.47173435031436384
      - 0.46366876480169594
      - 0.462411638116464
      - 0.4529227097518742
      - 0.4678453153464943
      - 0.4687897216062993
  task_id: 3
