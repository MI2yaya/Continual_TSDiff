best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_dropout/dropout_rate_0.5/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
config:
  batch_size: 64
  context_length: 312
  dataset: kdd_cup_2018_without_missing
  device: cuda:1
  diffusion_config: diffusion_small_config
  dropout_rate: 0.5
  eval_every: 50
  freq: H
  gradient_clip_val: 0.5
  init_skip: true
  lambda_reg: 0.0
  lr: 0.001
  max_epochs: 1000
  model: unconditional
  normalization: mean
  num_batches_per_epoch: 128
  num_samples: 16
  prediction_length: 24
  sampler: ddpm
  sampler_params:
    guidance: quantile
    scale: 1
  score_loss_type: l2
  setup: forecasting
  use_features: false
  use_lags: true
  use_validation_set: true
metrics:
- ND: 0.3511087488598352
  NRMSE: 0.7236371376438168
  mean_wQuantileLoss: 0.2798735518677761
  missing_scenario: none
  missing_values: 0
training_history:
  epochs:
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  - 20
  - 21
  - 22
  - 23
  - 24
  - 25
  - 26
  - 27
  - 28
  - 29
  - 30
  - 31
  - 32
  - 33
  - 34
  - 35
  - 36
  - 37
  - 38
  - 39
  - 40
  - 41
  - 42
  - 43
  - 44
  - 45
  - 46
  - 47
  - 48
  - 49
  - 50
  - 51
  - 52
  - 53
  - 54
  - 55
  - 56
  - 57
  - 58
  - 59
  - 60
  - 61
  - 62
  - 63
  - 64
  - 65
  - 66
  - 67
  - 68
  - 69
  - 70
  - 71
  train_loss:
  - 0.46765551646240056
  - 0.37451054679695517
  - 0.3358415161492303
  - 0.31376684410497546
  - 0.291025843587704
  - 0.2788518989691511
  - 0.2599144874839112
  - 0.25909688032697886
  - 0.2498927570413798
  - 0.2507029474945739
  - 0.24153391807340086
  - 0.23750089504756033
  - 0.23964445502497256
  - 0.24098325765226036
  - 0.23400661605410278
  - 0.23218993213959038
  - 0.23116005526389927
  - 0.23195268295239657
  - 0.23259770881850272
  - 0.23365913634188473
  - 0.2324169701896608
  - 0.2305833693826571
  - 0.2298208427382633
  - 0.22423378913663328
  - 0.22941178211476654
  - 0.2264696933561936
  - 0.22249112755525857
  - 0.2265671791974455
  - 0.22092151292599738
  - 0.22419860365334898
  - 0.22619872400537133
  - 0.22105874214321375
  - 0.22460834006778896
  - 0.22575021581724286
  - 0.2227625889936462
  - 0.22988143097609282
  - 0.2231668446911499
  - 0.22818022745195776
  - 0.22226548253092915
  - 0.21998025884386152
  - 0.21807596168946475
  - 0.22320573136676103
  - 0.22288746235426515
  - 0.2163447820348665
  - 0.22385711432434618
  - 0.22697514458559453
  - 0.2220500900875777
  - 0.22343700734199956
  - 0.21757517638616264
  - 0.21631361730396748
  - 0.22076921758707613
  - 0.21929192263633013
  - 0.22206841909792274
  - 0.21488598873838782
  - 0.22486333712004125
  - 0.22376702108886093
  - 0.2202779621584341
  - 0.22323022445198148
  - 0.2169459768338129
  - 0.2240086761303246
  - 0.21450924035161734
  - 0.2225857072044164
  - 0.2144004323054105
  - 0.21906048036180437
  - 0.21491359692299739
  - 0.2169650422874838
  - 0.2177398760104552
  - 0.22368277504574507
  - 0.21722783241420984
  - 0.21778003044892102
  - 0.2134446551790461
  val_loss:
  - 0.29381239116191865
  - 0.37451054679695517
  - 0.3358415161492303
  - 0.31376684410497546
  - 0.291025843587704
  - 0.2788518989691511
  - 0.2599144874839112
  - 0.25909688032697886
  - 0.2498927570413798
  - 0.2507029474945739
  - 0.24153391807340086
  - 0.23750089504756033
  - 0.23964445502497256
  - 0.24098325765226036
  - 0.23400661605410278
  - 0.23218993213959038
  - 0.23116005526389927
  - 0.23195268295239657
  - 0.23259770881850272
  - 0.23365913634188473
  - 0.2324169701896608
  - 0.2305833693826571
  - 0.2298208427382633
  - 0.22423378913663328
  - 0.22941178211476654
  - 0.2264696933561936
  - 0.22249112755525857
  - 0.2265671791974455
  - 0.22092151292599738
  - 0.22419860365334898
  - 0.22619872400537133
  - 0.22105874214321375
  - 0.22460834006778896
  - 0.22575021581724286
  - 0.2227625889936462
  - 0.22988143097609282
  - 0.2231668446911499
  - 0.22818022745195776
  - 0.22226548253092915
  - 0.21998025884386152
  - 0.21807596168946475
  - 0.22320573136676103
  - 0.22288746235426515
  - 0.2163447820348665
  - 0.22385711432434618
  - 0.22697514458559453
  - 0.2220500900875777
  - 0.22343700734199956
  - 0.21757517638616264
  - 0.21631361730396748
  - 0.1952800989151001
  - 0.21929192263633013
  - 0.22206841909792274
  - 0.21488598873838782
  - 0.22486333712004125
  - 0.22376702108886093
  - 0.2202779621584341
  - 0.22323022445198148
  - 0.2169459768338129
  - 0.2240086761303246
  - 0.21450924035161734
  - 0.2225857072044164
  - 0.2144004323054105
  - 0.21906048036180437
  - 0.21491359692299739
  - 0.2169650422874838
  - 0.2177398760104552
  - 0.22368277504574507
  - 0.21722783241420984
  - 0.21778003044892102
  - 0.2134446551790461
