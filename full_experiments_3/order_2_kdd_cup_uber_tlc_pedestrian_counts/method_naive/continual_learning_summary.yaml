continual_learning_setup:
  method: naive
  method_params: {}
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
  - train_pedestrian_counts.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_naive/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.34735349165541257
      NRMSE: 0.725108842758002
      mean_wQuantileLoss: 0.27562120271295865
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      train_loss:
      - 0.42093606921844184
      - 0.28526494081597775
      - 0.24625764519441873
      - 0.20877194055356085
      - 0.2044596984051168
      - 0.19727329933084548
      - 0.19877046754118055
      - 0.1920154218096286
      - 0.18963839585194364
      - 0.18585483019705862
      - 0.18343909172108397
      - 0.19081916636787355
      - 0.1800449185539037
      - 0.1808280408149585
      - 0.18125593953300267
      - 0.18138305057073012
      - 0.18064235482597724
      - 0.1683112743194215
      - 0.17473306565079838
      - 0.1706214349833317
      - 0.17367472976911813
      - 0.17454616160830483
      - 0.1740673971362412
      - 0.171016433800105
      - 0.17101398011436686
      - 0.16695313260424882
      - 0.16808197420323268
      - 0.17265429871622473
      - 0.1656402603839524
      - 0.16621588508132845
      - 0.16819642123300582
      - 0.16733658238081262
      - 0.16426581266568974
      - 0.16670996038010344
      - 0.16237959801219404
      - 0.16575920407194644
      - 0.16393556288676336
      - 0.16975055763032287
      - 0.16650382953230292
      - 0.16410119854845107
      - 0.16301799553912133
      - 0.1671479435171932
      - 0.16397823038278148
      - 0.16329370654420927
      - 0.15892812045058236
      - 0.16294820938492194
      - 0.1667215977795422
      - 0.16278311848873273
      - 0.16264214518014342
      - 0.15814547892659903
      - 0.1671261023147963
      - 0.16217433603014797
      - 0.16228751663584262
      - 0.16424083546735346
      - 0.1579512762837112
      - 0.16005942219635472
      - 0.1611890948843211
      - 0.1586066663148813
      - 0.15784778189845383
      - 0.15947455662535504
      - 0.1596150029799901
      - 0.16033179522491992
      - 0.15903626097133383
      - 0.15531394752906635
      - 0.1600736016407609
      - 0.15754187642596662
      - 0.16288404469378293
      - 0.1568158491863869
      - 0.1567678569117561
      - 0.156391010095831
      - 0.1569324178271927
      - 0.15517887228634208
      - 0.15589077753247693
      - 0.15754290914628655
      - 0.15590502572013065
      - 0.15795905713457614
      - 0.15816344157792628
      - 0.15578129282221198
      - 0.15346889127977192
      - 0.1609652113984339
      - 0.15827657229965553
      - 0.15544517029775307
      - 0.1572858932777308
      - 0.1574836372747086
      - 0.15409711404936388
      - 0.15263106458587572
      - 0.15313024580245838
      - 0.15294532862026244
      - 0.16349699103739113
      - 0.1545727613265626
      - 0.15440047992160544
      - 0.14949765626806766
      - 0.15286413067951798
      - 0.15416871092747897
      - 0.15602274873526767
      - 0.15869531163480133
      - 0.15204764739610255
      - 0.15499227115651593
      - 0.1557064372464083
      - 0.15408052847487852
      - 0.1513094647671096
      - 0.15395832026842982
      - 0.16086000914219767
      - 0.15768263692734763
      - 0.15779418830061331
      - 0.15450401924317703
      - 0.15302342153154314
      - 0.1551475558662787
      - 0.15604453842388466
      - 0.15173970395699143
      - 0.1552212288370356
      - 0.15156703727552667
      val_loss:
      - 0.30668545961380006
      - 0.28526494081597775
      - 0.24625764519441873
      - 0.20877194055356085
      - 0.2044596984051168
      - 0.19727329933084548
      - 0.19877046754118055
      - 0.1920154218096286
      - 0.18963839585194364
      - 0.18585483019705862
      - 0.18343909172108397
      - 0.19081916636787355
      - 0.1800449185539037
      - 0.1808280408149585
      - 0.18125593953300267
      - 0.18138305057073012
      - 0.18064235482597724
      - 0.1683112743194215
      - 0.17473306565079838
      - 0.1706214349833317
      - 0.17367472976911813
      - 0.17454616160830483
      - 0.1740673971362412
      - 0.171016433800105
      - 0.17101398011436686
      - 0.16695313260424882
      - 0.16808197420323268
      - 0.17265429871622473
      - 0.1656402603839524
      - 0.16621588508132845
      - 0.16819642123300582
      - 0.16733658238081262
      - 0.16426581266568974
      - 0.16670996038010344
      - 0.16237959801219404
      - 0.16575920407194644
      - 0.16393556288676336
      - 0.16975055763032287
      - 0.16650382953230292
      - 0.16410119854845107
      - 0.16301799553912133
      - 0.1671479435171932
      - 0.16397823038278148
      - 0.16329370654420927
      - 0.15892812045058236
      - 0.16294820938492194
      - 0.1667215977795422
      - 0.16278311848873273
      - 0.16264214518014342
      - 0.15814547892659903
      - 0.15685757249593735
      - 0.16217433603014797
      - 0.16228751663584262
      - 0.16424083546735346
      - 0.1579512762837112
      - 0.16005942219635472
      - 0.1611890948843211
      - 0.1586066663148813
      - 0.15784778189845383
      - 0.15947455662535504
      - 0.1596150029799901
      - 0.16033179522491992
      - 0.15903626097133383
      - 0.15531394752906635
      - 0.1600736016407609
      - 0.15754187642596662
      - 0.16288404469378293
      - 0.1568158491863869
      - 0.1567678569117561
      - 0.156391010095831
      - 0.1569324178271927
      - 0.15517887228634208
      - 0.15589077753247693
      - 0.15754290914628655
      - 0.15590502572013065
      - 0.15795905713457614
      - 0.15816344157792628
      - 0.15578129282221198
      - 0.15346889127977192
      - 0.1609652113984339
      - 0.15827657229965553
      - 0.15544517029775307
      - 0.1572858932777308
      - 0.1574836372747086
      - 0.15409711404936388
      - 0.15263106458587572
      - 0.15313024580245838
      - 0.15294532862026244
      - 0.16349699103739113
      - 0.1545727613265626
      - 0.15440047992160544
      - 0.14949765626806766
      - 0.15286413067951798
      - 0.15416871092747897
      - 0.15602274873526767
      - 0.15869531163480133
      - 0.15204764739610255
      - 0.15499227115651593
      - 0.1557064372464083
      - 0.15408052847487852
      - 0.1622524231672287
      - 0.15395832026842982
      - 0.16086000914219767
      - 0.15768263692734763
      - 0.15779418830061331
      - 0.15450401924317703
      - 0.15302342153154314
      - 0.1551475558662787
      - 0.15604453842388466
      - 0.15173970395699143
      - 0.1552212288370356
      - 0.15156703727552667
  task_id: 1
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_naive/task_2_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.21056651605783527
      NRMSE: 0.5994125103079827
      mean_wQuantileLoss: 0.17753128919402672
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      train_loss:
      - 0.47508628480136395
      - 0.3281387658789754
      - 0.3106390633620322
      - 0.3045078609138727
      - 0.2905604240950197
      - 0.285964909940958
      - 0.27801658445969224
      - 0.2796407761052251
      - 0.2723233747528866
      - 0.2754662431543693
      - 0.2697564223781228
      - 0.27399171725846827
      - 0.26776817231439054
      - 0.2693474180996418
      - 0.2633742179023102
      - 0.2595274168998003
      - 0.2633247609483078
      - 0.2559307679766789
      - 0.26319139695260674
      - 0.25706692796666175
      - 0.25527656252961606
      - 0.25395619706250727
      - 0.2561820012051612
      - 0.2524569919332862
      - 0.2544350556563586
      - 0.252991511952132
      - 0.248164304648526
      - 0.25450776773504913
      - 0.24357763899024576
      - 0.25187535223085433
      - 0.24834055465180427
      - 0.2511564346496016
      - 0.24815184785984457
      - 0.24999531789217144
      - 0.24626728263683617
      - 0.2425526868319139
      - 0.2435977344866842
      - 0.24302770930808038
      - 0.250103497877717
      - 0.24222064181230962
      - 0.24270739266648889
      - 0.24314320599660277
      - 0.24797616561409086
      - 0.24966075632255524
      - 0.2452011837158352
      - 0.24585259170271456
      - 0.23916115367319435
      - 0.24631052440963686
      - 0.2467926872195676
      - 0.2441858850652352
      - 0.24548687576316297
      - 0.24493134417571127
      - 0.24628242070320994
      - 0.2408907252829522
      - 0.23903770942706615
      - 0.24398690811358392
      - 0.24694790376815945
      - 0.24288176791742444
      - 0.23727476666681468
      - 0.2410639488371089
      - 0.2362502096220851
      - 0.24493415793403983
      - 0.24056714191101491
      - 0.24442361353430897
      - 0.24439001292921603
      - 0.23630167078226805
      - 0.2388519225642085
      - 0.2390219442313537
      - 0.23976839741226286
      - 0.23914959467947483
      - 0.23920323979109526
      - 0.23881712253205478
      - 0.23791433696169406
      - 0.23118850169703364
      - 0.2407362883677706
      - 0.2353617443004623
      - 0.238885716535151
      - 0.23595681879669428
      - 0.23508923791814595
      - 0.24000584066379815
      - 0.23919276241213083
      - 0.23927694477606565
      - 0.23577891825698316
      - 0.23985922161955386
      - 0.23634522315114737
      - 0.2427327528130263
      - 0.23791075765620917
      - 0.24079340195748955
      - 0.22990908531937748
      - 0.23607053305022418
      - 0.23642928537447006
      - 0.2359666097909212
      - 0.23306327441241592
      - 0.23094172414857894
      - 0.24132204882334918
      - 0.23752312257420272
      - 0.2369091344298795
      - 0.23545683023985475
      - 0.23512269649654627
      - 0.23489417042583227
      - 0.23504319705534726
      - 0.23544325504917651
      - 0.23264945531263947
      - 0.23098536219913512
      - 0.23737419839017093
      - 0.23604668607003987
      - 0.24287156015634537
      - 0.23298092861659825
      - 0.23607172654010355
      val_loss:
      - 0.3485358476638794
      - 0.3281387658789754
      - 0.3106390633620322
      - 0.3045078609138727
      - 0.2905604240950197
      - 0.285964909940958
      - 0.27801658445969224
      - 0.2796407761052251
      - 0.2723233747528866
      - 0.2754662431543693
      - 0.2697564223781228
      - 0.27399171725846827
      - 0.26776817231439054
      - 0.2693474180996418
      - 0.2633742179023102
      - 0.2595274168998003
      - 0.2633247609483078
      - 0.2559307679766789
      - 0.26319139695260674
      - 0.25706692796666175
      - 0.25527656252961606
      - 0.25395619706250727
      - 0.2561820012051612
      - 0.2524569919332862
      - 0.2544350556563586
      - 0.252991511952132
      - 0.248164304648526
      - 0.25450776773504913
      - 0.24357763899024576
      - 0.25187535223085433
      - 0.24834055465180427
      - 0.2511564346496016
      - 0.24815184785984457
      - 0.24999531789217144
      - 0.24626728263683617
      - 0.2425526868319139
      - 0.2435977344866842
      - 0.24302770930808038
      - 0.250103497877717
      - 0.24222064181230962
      - 0.24270739266648889
      - 0.24314320599660277
      - 0.24797616561409086
      - 0.24966075632255524
      - 0.2452011837158352
      - 0.24585259170271456
      - 0.23916115367319435
      - 0.24631052440963686
      - 0.2467926872195676
      - 0.2441858850652352
      - 0.2374527484178543
      - 0.24493134417571127
      - 0.24628242070320994
      - 0.2408907252829522
      - 0.23903770942706615
      - 0.24398690811358392
      - 0.24694790376815945
      - 0.24288176791742444
      - 0.23727476666681468
      - 0.2410639488371089
      - 0.2362502096220851
      - 0.24493415793403983
      - 0.24056714191101491
      - 0.24442361353430897
      - 0.24439001292921603
      - 0.23630167078226805
      - 0.2388519225642085
      - 0.2390219442313537
      - 0.23976839741226286
      - 0.23914959467947483
      - 0.23920323979109526
      - 0.23881712253205478
      - 0.23791433696169406
      - 0.23118850169703364
      - 0.2407362883677706
      - 0.2353617443004623
      - 0.238885716535151
      - 0.23595681879669428
      - 0.23508923791814595
      - 0.24000584066379815
      - 0.23919276241213083
      - 0.23927694477606565
      - 0.23577891825698316
      - 0.23985922161955386
      - 0.23634522315114737
      - 0.2427327528130263
      - 0.23791075765620917
      - 0.24079340195748955
      - 0.22990908531937748
      - 0.23607053305022418
      - 0.23642928537447006
      - 0.2359666097909212
      - 0.23306327441241592
      - 0.23094172414857894
      - 0.24132204882334918
      - 0.23752312257420272
      - 0.2369091344298795
      - 0.23545683023985475
      - 0.23512269649654627
      - 0.23489417042583227
      - 0.23015740513801575
      - 0.23544325504917651
      - 0.23264945531263947
      - 0.23098536219913512
      - 0.23737419839017093
      - 0.23604668607003987
      - 0.24287156015634537
      - 0.23298092861659825
      - 0.23607172654010355
  task_id: 2
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_naive/task_3_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.20400201860547473
      NRMSE: 0.6655544869336392
      mean_wQuantileLoss: 0.16379848056911397
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5074939536862075
      - 0.3396403741789982
      - 0.29470792005304247
      - 0.26753278460819274
      - 0.24907811300363392
      - 0.2260817177593708
      - 0.20307262556161731
      - 0.19624859961913899
      - 0.18886694265529513
      - 0.19996892515337095
      - 0.18021225440315902
      - 0.17415079224156216
      - 0.22394121071556583
      - 0.17225158814108
      - 0.1743315329658799
      - 0.17345111281611025
      - 0.17143658950226381
      - 0.16524982580449432
      - 0.17251664743525907
      - 0.162702274043113
      - 0.16599681187653914
      - 0.166574913833756
      - 0.16251248656772077
      - 0.16195434122346342
      - 0.1595898493542336
      - 0.1580735730822198
      - 0.16165266843745485
      - 0.16844121285248548
      - 0.17365478846477345
      - 0.16095045930705965
      - 0.1539792992407456
      - 0.15718082612147555
      - 0.15651818312471732
      - 0.16456191142788157
      - 0.15942312660627067
      - 0.15163884789217263
      - 0.16104156849905849
      - 0.15509613166796044
      - 0.1538478980655782
      - 0.16289659164613113
      - 0.15142976626520976
      - 0.16094997437903658
      - 0.15125258348416537
      - 0.1539077250054106
      - 0.15220702916849405
      - 0.15129250206518918
      - 0.15321516076801345
      - 0.15000684815458953
      - 0.15370680746855214
      - 0.15070432447828352
      - 0.16618188563734293
      - 0.15036014886572957
      - 0.14942717767553404
      - 0.15002775186439976
      - 0.14849434973439202
      - 0.14800299535272643
      - 0.1479898532270454
      - 0.15257144503993914
      - 0.15381432848516852
      - 0.15306511701783165
      - 0.14863616827642545
      - 0.1466707331710495
      - 0.15147979249013588
      - 0.14890170010039583
      - 0.14909758849535137
      - 0.1457913761259988
      - 0.148631876334548
      - 0.14573927357560024
      - 0.14150542876450345
      - 0.1450603639241308
      - 0.15669737657299265
      val_loss:
      - 0.5092014521360397
      - 0.3396403741789982
      - 0.29470792005304247
      - 0.26753278460819274
      - 0.24907811300363392
      - 0.2260817177593708
      - 0.20307262556161731
      - 0.19624859961913899
      - 0.18886694265529513
      - 0.19996892515337095
      - 0.18021225440315902
      - 0.17415079224156216
      - 0.22394121071556583
      - 0.17225158814108
      - 0.1743315329658799
      - 0.17345111281611025
      - 0.17143658950226381
      - 0.16524982580449432
      - 0.17251664743525907
      - 0.162702274043113
      - 0.16599681187653914
      - 0.166574913833756
      - 0.16251248656772077
      - 0.16195434122346342
      - 0.1595898493542336
      - 0.1580735730822198
      - 0.16165266843745485
      - 0.16844121285248548
      - 0.17365478846477345
      - 0.16095045930705965
      - 0.1539792992407456
      - 0.15718082612147555
      - 0.15651818312471732
      - 0.16456191142788157
      - 0.15942312660627067
      - 0.15163884789217263
      - 0.16104156849905849
      - 0.15509613166796044
      - 0.1538478980655782
      - 0.16289659164613113
      - 0.15142976626520976
      - 0.16094997437903658
      - 0.15125258348416537
      - 0.1539077250054106
      - 0.15220702916849405
      - 0.15129250206518918
      - 0.15321516076801345
      - 0.15000684815458953
      - 0.15370680746855214
      - 0.15070432447828352
      - 0.11165149603039026
      - 0.15036014886572957
      - 0.14942717767553404
      - 0.15002775186439976
      - 0.14849434973439202
      - 0.14800299535272643
      - 0.1479898532270454
      - 0.15257144503993914
      - 0.15381432848516852
      - 0.15306511701783165
      - 0.14863616827642545
      - 0.1466707331710495
      - 0.15147979249013588
      - 0.14890170010039583
      - 0.14909758849535137
      - 0.1457913761259988
      - 0.148631876334548
      - 0.14573927357560024
      - 0.14150542876450345
      - 0.1450603639241308
      - 0.15669737657299265
  task_id: 3
