continual_learning_setup:
  method: score_l2
  method_params:
    lambda_reg: 2.0
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
  - train_pedestrian_counts.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l2/lambda_reg_2.0/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 2.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.4218654327491814
      NRMSE: 0.8060268785488566
      mean_wQuantileLoss: 0.3368337801920422
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      - 116
      - 117
      - 118
      - 119
      - 120
      - 121
      train_loss:
      - 0.4219566723331809
      - 0.2721457143779844
      - 0.23579119378700852
      - 0.20868336461717263
      - 0.2011741567403078
      - 0.1957592303515412
      - 0.18877977796364576
      - 0.1871063161524944
      - 0.1960459330584854
      - 0.17816710722399876
      - 0.18247350788442418
      - 0.183220183767844
      - 0.18286195222754031
      - 0.17653188429540023
      - 0.17694045434473082
      - 0.17960422701435164
      - 0.1781397510203533
      - 0.18176777462940663
      - 0.17327970988117158
      - 0.16726141178514808
      - 0.17097108816960827
      - 0.17000411119079217
      - 0.1690554943634197
      - 0.1696962196729146
      - 0.1694357244996354
      - 0.1662734585115686
      - 0.16904997458914295
      - 0.16742923541460186
      - 0.16882688435725868
      - 0.16396364342654124
      - 0.16560586809646338
      - 0.16482401156099513
      - 0.16814434877596796
      - 0.16981264983769506
      - 0.1718024230794981
      - 0.16166850476292893
      - 0.16474404971813783
      - 0.16265880322316661
      - 0.15768892335472628
      - 0.16506300953915343
      - 0.16347937611863017
      - 0.1696963453432545
      - 0.1642834577942267
      - 0.16264511668123305
      - 0.1637780141318217
      - 0.1659315205179155
      - 0.16209909442113712
      - 0.16220801713643596
      - 0.1594813210540451
      - 0.1617392498301342
      - 0.15888740069931373
      - 0.15797988645499572
      - 0.1616965034045279
      - 0.15865380637114868
      - 0.1595691037364304
      - 0.1631482900120318
      - 0.1583205230999738
      - 0.15479625470470637
      - 0.1589837707579136
      - 0.16194221016485244
      - 0.16014585935045034
      - 0.1563876330619678
      - 0.15703744540223852
      - 0.15853676234837621
      - 0.15701375424396247
      - 0.15669455844908953
      - 0.16356887959409505
      - 0.15956609352724627
      - 0.15355016459943727
      - 0.15750816703075543
      - 0.15419799112714827
      - 0.15711801761062816
      - 0.16142668412066996
      - 0.1553745458368212
      - 0.15437671524705365
      - 0.15724376431899145
      - 0.1529556616442278
      - 0.15873613831354305
      - 0.15551571635296568
      - 0.15772055560955778
      - 0.15778161800699309
      - 0.15526404982665554
      - 0.15561133035225794
      - 0.15910988114774227
      - 0.1607152033247985
      - 0.1560849397792481
      - 0.15579536510631442
      - 0.15331907774088904
      - 0.14833293680567294
      - 0.1527921090601012
      - 0.16197910957271233
      - 0.15258250484475866
      - 0.15991576464148238
      - 0.15177861950360239
      - 0.15525296982377768
      - 0.15159156278241426
      - 0.1510879019042477
      - 0.15537555958144367
      - 0.1510983004118316
      - 0.14976972964359447
      - 0.15081923053367063
      - 0.15413288184208795
      - 0.15051701513584703
      - 0.15554471366340294
      - 0.15609696559840813
      - 0.15006496489513665
      - 0.1503379560308531
      - 0.1557484208024107
      - 0.15109799959464
      - 0.1492231689626351
      - 0.15166331361979246
      - 0.14959885692223907
      - 0.14887965453090146
      - 0.15440579818096012
      - 0.15191111579770222
      - 0.15221801196457818
      - 0.15548593958374113
      - 0.1535410881624557
      - 0.15598584059625864
      - 0.1496221013367176
      - 0.15203251619823277
      val_loss:
      - 0.25951909422874453
      - 0.2721457143779844
      - 0.23579119378700852
      - 0.20868336461717263
      - 0.2011741567403078
      - 0.1957592303515412
      - 0.18877977796364576
      - 0.1871063161524944
      - 0.1960459330584854
      - 0.17816710722399876
      - 0.18247350788442418
      - 0.183220183767844
      - 0.18286195222754031
      - 0.17653188429540023
      - 0.17694045434473082
      - 0.17960422701435164
      - 0.1781397510203533
      - 0.18176777462940663
      - 0.17327970988117158
      - 0.16726141178514808
      - 0.17097108816960827
      - 0.17000411119079217
      - 0.1690554943634197
      - 0.1696962196729146
      - 0.1694357244996354
      - 0.1662734585115686
      - 0.16904997458914295
      - 0.16742923541460186
      - 0.16882688435725868
      - 0.16396364342654124
      - 0.16560586809646338
      - 0.16482401156099513
      - 0.16814434877596796
      - 0.16981264983769506
      - 0.1718024230794981
      - 0.16166850476292893
      - 0.16474404971813783
      - 0.16265880322316661
      - 0.15768892335472628
      - 0.16506300953915343
      - 0.16347937611863017
      - 0.1696963453432545
      - 0.1642834577942267
      - 0.16264511668123305
      - 0.1637780141318217
      - 0.1659315205179155
      - 0.16209909442113712
      - 0.16220801713643596
      - 0.1594813210540451
      - 0.1617392498301342
      - 0.16706150770187378
      - 0.15797988645499572
      - 0.1616965034045279
      - 0.15865380637114868
      - 0.1595691037364304
      - 0.1631482900120318
      - 0.1583205230999738
      - 0.15479625470470637
      - 0.1589837707579136
      - 0.16194221016485244
      - 0.16014585935045034
      - 0.1563876330619678
      - 0.15703744540223852
      - 0.15853676234837621
      - 0.15701375424396247
      - 0.15669455844908953
      - 0.16356887959409505
      - 0.15956609352724627
      - 0.15355016459943727
      - 0.15750816703075543
      - 0.15419799112714827
      - 0.15711801761062816
      - 0.16142668412066996
      - 0.1553745458368212
      - 0.15437671524705365
      - 0.15724376431899145
      - 0.1529556616442278
      - 0.15873613831354305
      - 0.15551571635296568
      - 0.15772055560955778
      - 0.15778161800699309
      - 0.15526404982665554
      - 0.15561133035225794
      - 0.15910988114774227
      - 0.1607152033247985
      - 0.1560849397792481
      - 0.15579536510631442
      - 0.15331907774088904
      - 0.14833293680567294
      - 0.1527921090601012
      - 0.16197910957271233
      - 0.15258250484475866
      - 0.15991576464148238
      - 0.15177861950360239
      - 0.15525296982377768
      - 0.15159156278241426
      - 0.1510879019042477
      - 0.15537555958144367
      - 0.1510983004118316
      - 0.14976972964359447
      - 0.1319549486041069
      - 0.15413288184208795
      - 0.15051701513584703
      - 0.15554471366340294
      - 0.15609696559840813
      - 0.15006496489513665
      - 0.1503379560308531
      - 0.1557484208024107
      - 0.15109799959464
      - 0.1492231689626351
      - 0.15166331361979246
      - 0.14959885692223907
      - 0.14887965453090146
      - 0.15440579818096012
      - 0.15191111579770222
      - 0.15221801196457818
      - 0.15548593958374113
      - 0.1535410881624557
      - 0.15598584059625864
      - 0.1496221013367176
      - 0.15203251619823277
  task_id: 1
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l2/lambda_reg_2.0/task_2_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 2.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 2.766199014019751
      NRMSE: 15.1663284992399
      mean_wQuantileLoss: 2.680782989169032
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 3.269678223878145
      - 2.2519113644957542
      - 2.078647424466908
      - 1.9976084884256124
      - 2.036268006078899
      - 1.9874304104596376
      - 1.9447657614946365
      - 1.9250944182276726
      - 1.9968675822019577
      - 1.9397051073610783
      - 1.9019136326387525
      - 1.9869134733453393
      - 1.9144789949059486
      - 1.8698022784665227
      - 1.9437544886022806
      - 1.8785063596442342
      - 1.815290441736579
      - 1.8903595684096217
      - 1.885662422515452
      - 1.8768320605158806
      - 1.8793490137904882
      val_loss:
      - 1.0903980016708374
      - 2.2519113644957542
      - 2.078647424466908
      - 1.9976084884256124
      - 2.036268006078899
      - 1.9874304104596376
      - 1.9447657614946365
      - 1.9250944182276726
      - 1.9968675822019577
      - 1.9397051073610783
      - 1.9019136326387525
      - 1.9869134733453393
      - 1.9144789949059486
      - 1.8698022784665227
      - 1.9437544886022806
      - 1.8785063596442342
      - 1.815290441736579
      - 1.8903595684096217
      - 1.885662422515452
      - 1.8768320605158806
      - 1.8793490137904882
  task_id: 2
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l2/lambda_reg_2.0/task_3_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 2.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.4084336098115052
      NRMSE: 1.1509000168277637
      mean_wQuantileLoss: 0.30663238141392196
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 0.8629704769700766
      - 0.5641291963402182
      - 0.517592835938558
      - 0.4621145064011216
      - 0.4393000470008701
      - 0.41407190007157624
      - 0.42238019593060017
      - 0.40086869802325964
      - 0.4065616950392723
      - 0.39975083782337606
      - 0.4116644626483321
      - 0.41179809579625726
      - 0.42148653184995055
      - 0.39071887102909386
      - 0.4112068014219403
      - 0.4548971604090184
      - 0.41457332065328956
      - 0.39087697584182024
      - 0.3956324066966772
      - 0.4001216636970639
      - 0.39757585199549794
      val_loss:
      - 0.26932116970419884
      - 0.5641291963402182
      - 0.517592835938558
      - 0.4621145064011216
      - 0.4393000470008701
      - 0.41407190007157624
      - 0.42238019593060017
      - 0.40086869802325964
      - 0.4065616950392723
      - 0.39975083782337606
      - 0.4116644626483321
      - 0.41179809579625726
      - 0.42148653184995055
      - 0.39071887102909386
      - 0.4112068014219403
      - 0.4548971604090184
      - 0.41457332065328956
      - 0.39087697584182024
      - 0.3956324066966772
      - 0.4001216636970639
      - 0.39757585199549794
  task_id: 3
