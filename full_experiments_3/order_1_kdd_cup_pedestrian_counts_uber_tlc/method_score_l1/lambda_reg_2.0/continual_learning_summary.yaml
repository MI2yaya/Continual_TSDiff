continual_learning_setup:
  method: score_l1
  method_params:
    lambda_reg: 2.0
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l1/lambda_reg_2.0/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 2.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.38370934749519453
      NRMSE: 0.7694999580209883
      mean_wQuantileLoss: 0.2972018646895623
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      - 116
      - 117
      - 118
      - 119
      - 120
      - 121
      train_loss:
      - 0.432921011117287
      - 0.2871641298988834
      - 0.2502007291186601
      - 0.23004897660575807
      - 0.20309470273787156
      - 0.20822471723658964
      - 0.1997951994999312
      - 0.19660889881197363
      - 0.19100105704274029
      - 0.19519785337615758
      - 0.18272431916557252
      - 0.18153838068246841
      - 0.18743656034348533
      - 0.18319869932020083
      - 0.1833377602742985
      - 0.1766056446940638
      - 0.17736436612904072
      - 0.1830282662413083
      - 0.1737664377433248
      - 0.17501880566123873
      - 0.17704947787569836
      - 0.1699823047965765
      - 0.1765680884127505
      - 0.17616711131995544
      - 0.16958252870244905
      - 0.17192270752275363
      - 0.1742355425376445
      - 0.17096152645535767
      - 0.1684463751153089
      - 0.16759624640690163
      - 0.17840971396071836
      - 0.16671389812836424
      - 0.16749332362087443
      - 0.1660404948052019
      - 0.16931475466117263
      - 0.16649363457690924
      - 0.16960812045726925
      - 0.16711807873798534
      - 0.16051974869333208
      - 0.16026302392128855
      - 0.16222751553868875
      - 0.1698383396724239
      - 0.15967407322023064
      - 0.16534210951067507
      - 0.15987543942173943
      - 0.1628242357983254
      - 0.16302122437627986
      - 0.1616459974902682
      - 0.16385616577463225
      - 0.164265590836294
      - 0.16097876551793888
      - 0.16184044413967058
      - 0.1688736299984157
      - 0.15945511596510187
      - 0.1589036614750512
      - 0.1690184511244297
      - 0.1663386367727071
      - 0.16006887558614835
      - 0.15950647735735402
      - 0.16019923152634874
      - 0.1630017826682888
      - 0.16085309983463958
      - 0.16244704375276342
      - 0.15720305766444653
      - 0.16134630335727707
      - 0.15912013774504885
      - 0.156984162167646
      - 0.15919970074901357
      - 0.15800424484768882
      - 0.16055420669727027
      - 0.15557997609721497
      - 0.15986234752926975
      - 0.16293390287319198
      - 0.16015722695738077
      - 0.15574249520432204
      - 0.1576693025417626
      - 0.1559744641999714
      - 0.17084837378934026
      - 0.1577445766888559
      - 0.15924380312208086
      - 0.158711644471623
      - 0.15605970553588122
      - 0.15789449325529858
      - 0.15485952445305884
      - 0.15566338057396933
      - 0.15690467722015455
      - 0.15652138536097482
      - 0.15763903758488595
      - 0.15512435080017895
      - 0.15482644585426897
      - 0.15667985851177946
      - 0.15147749049356207
      - 0.1546543184085749
      - 0.15055307938018814
      - 0.15564538462786004
      - 0.15757961990311742
      - 0.15862837061285973
      - 0.15626520937075838
      - 0.15750678384210914
      - 0.15383811009814963
      - 0.15644541196525097
      - 0.1551021781633608
      - 0.15069370158016682
      - 0.1512642787420191
      - 0.15112712199334055
      - 0.15472943900385872
      - 0.1546470926841721
      - 0.16005246399436146
      - 0.15688657882856205
      - 0.15790627436945215
      - 0.14920139906462282
      - 0.1513586599030532
      - 0.15034392732195556
      - 0.15399859548779204
      - 0.156089746451471
      - 0.15424709295621142
      - 0.15384336520219222
      - 0.15210443822434172
      - 0.1530733800609596
      - 0.1532937801675871
      - 0.15078397176694125
      val_loss:
      - 0.2729530334472656
      - 0.2871641298988834
      - 0.2502007291186601
      - 0.23004897660575807
      - 0.20309470273787156
      - 0.20822471723658964
      - 0.1997951994999312
      - 0.19660889881197363
      - 0.19100105704274029
      - 0.19519785337615758
      - 0.18272431916557252
      - 0.18153838068246841
      - 0.18743656034348533
      - 0.18319869932020083
      - 0.1833377602742985
      - 0.1766056446940638
      - 0.17736436612904072
      - 0.1830282662413083
      - 0.1737664377433248
      - 0.17501880566123873
      - 0.17704947787569836
      - 0.1699823047965765
      - 0.1765680884127505
      - 0.17616711131995544
      - 0.16958252870244905
      - 0.17192270752275363
      - 0.1742355425376445
      - 0.17096152645535767
      - 0.1684463751153089
      - 0.16759624640690163
      - 0.17840971396071836
      - 0.16671389812836424
      - 0.16749332362087443
      - 0.1660404948052019
      - 0.16931475466117263
      - 0.16649363457690924
      - 0.16960812045726925
      - 0.16711807873798534
      - 0.16051974869333208
      - 0.16026302392128855
      - 0.16222751553868875
      - 0.1698383396724239
      - 0.15967407322023064
      - 0.16534210951067507
      - 0.15987543942173943
      - 0.1628242357983254
      - 0.16302122437627986
      - 0.1616459974902682
      - 0.16385616577463225
      - 0.164265590836294
      - 0.1641259089112282
      - 0.16184044413967058
      - 0.1688736299984157
      - 0.15945511596510187
      - 0.1589036614750512
      - 0.1690184511244297
      - 0.1663386367727071
      - 0.16006887558614835
      - 0.15950647735735402
      - 0.16019923152634874
      - 0.1630017826682888
      - 0.16085309983463958
      - 0.16244704375276342
      - 0.15720305766444653
      - 0.16134630335727707
      - 0.15912013774504885
      - 0.156984162167646
      - 0.15919970074901357
      - 0.15800424484768882
      - 0.16055420669727027
      - 0.15557997609721497
      - 0.15986234752926975
      - 0.16293390287319198
      - 0.16015722695738077
      - 0.15574249520432204
      - 0.1576693025417626
      - 0.1559744641999714
      - 0.17084837378934026
      - 0.1577445766888559
      - 0.15924380312208086
      - 0.158711644471623
      - 0.15605970553588122
      - 0.15789449325529858
      - 0.15485952445305884
      - 0.15566338057396933
      - 0.15690467722015455
      - 0.15652138536097482
      - 0.15763903758488595
      - 0.15512435080017895
      - 0.15482644585426897
      - 0.15667985851177946
      - 0.15147749049356207
      - 0.1546543184085749
      - 0.15055307938018814
      - 0.15564538462786004
      - 0.15757961990311742
      - 0.15862837061285973
      - 0.15626520937075838
      - 0.15750678384210914
      - 0.15383811009814963
      - 0.14433057755231857
      - 0.1551021781633608
      - 0.15069370158016682
      - 0.1512642787420191
      - 0.15112712199334055
      - 0.15472943900385872
      - 0.1546470926841721
      - 0.16005246399436146
      - 0.15688657882856205
      - 0.15790627436945215
      - 0.14920139906462282
      - 0.1513586599030532
      - 0.15034392732195556
      - 0.15399859548779204
      - 0.156089746451471
      - 0.15424709295621142
      - 0.15384336520219222
      - 0.15210443822434172
      - 0.1530733800609596
      - 0.1532937801675871
      - 0.15078397176694125
  task_id: 1
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l1/lambda_reg_2.0/task_2_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 2.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.5117758852973638
      NRMSE: 1.4437039514473253
      mean_wQuantileLoss: 0.3975578628935427
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 1.042092127725482
      - 0.7827307991683483
      - 0.6736666061915457
      - 0.6156419164035469
      - 0.564612805377692
      - 0.5615231730043888
      - 0.5104264100082219
      - 0.5163062703795731
      - 0.5021124943159521
      - 0.46563553786836565
      - 0.4571329480968416
      - 0.45664027356542647
      - 0.4501453526318073
      - 0.4655430258717388
      - 0.43768994486890733
      - 0.4282783467788249
      - 0.42607153905555606
      - 0.4220966196153313
      - 0.43575176037847996
      - 0.41840908536687493
      - 0.41584028862416744
      - 0.43481600447557867
      - 0.4062990590464324
      - 0.4207463529892266
      - 0.39738308545202017
      - 0.3979378326330334
      - 0.3994944009464234
      - 0.3949784084688872
      - 0.3883876879699528
      - 0.3929271271917969
      - 0.3908194631803781
      - 0.390236638719216
      - 0.4010697677731514
      - 0.38751489482820034
      - 0.3891907627694309
      - 0.38545591314323246
      - 0.4083675106521696
      - 0.38682756875641644
      - 0.38791200378909707
      - 0.3756634769961238
      - 0.3831291545648128
      - 0.37744549941271544
      - 0.3795494334772229
      - 0.3799183394294232
      - 0.3784371395595372
      - 0.3814432283397764
      - 0.37414626288227737
      - 0.37972663273103535
      - 0.37633509701117873
      - 0.370639089262113
      - 0.3659745045006275
      - 0.3752907298039645
      - 0.37123463745228946
      - 0.37257279362529516
      - 0.36313337623141706
      - 0.37259148387238383
      - 0.3729198870714754
      - 0.3675658628344536
      - 0.36633120314218104
      - 0.3673505268525332
      - 0.36630234867334366
      - 0.3633181357290596
      - 0.3666789617855102
      - 0.3634543165098876
      - 0.3634777436964214
      - 0.36454468313604593
      - 0.3521232893690467
      - 0.36202764534391463
      - 0.3633464372251183
      - 0.37945868680253625
      - 0.35762058943510056
      val_loss:
      - 0.5562355071306229
      - 0.7827307991683483
      - 0.6736666061915457
      - 0.6156419164035469
      - 0.564612805377692
      - 0.5615231730043888
      - 0.5104264100082219
      - 0.5163062703795731
      - 0.5021124943159521
      - 0.46563553786836565
      - 0.4571329480968416
      - 0.45664027356542647
      - 0.4501453526318073
      - 0.4655430258717388
      - 0.43768994486890733
      - 0.4282783467788249
      - 0.42607153905555606
      - 0.4220966196153313
      - 0.43575176037847996
      - 0.41840908536687493
      - 0.41584028862416744
      - 0.43481600447557867
      - 0.4062990590464324
      - 0.4207463529892266
      - 0.39738308545202017
      - 0.3979378326330334
      - 0.3994944009464234
      - 0.3949784084688872
      - 0.3883876879699528
      - 0.3929271271917969
      - 0.3908194631803781
      - 0.390236638719216
      - 0.4010697677731514
      - 0.38751489482820034
      - 0.3891907627694309
      - 0.38545591314323246
      - 0.4083675106521696
      - 0.38682756875641644
      - 0.38791200378909707
      - 0.3756634769961238
      - 0.3831291545648128
      - 0.37744549941271544
      - 0.3795494334772229
      - 0.3799183394294232
      - 0.3784371395595372
      - 0.3814432283397764
      - 0.37414626288227737
      - 0.37972663273103535
      - 0.37633509701117873
      - 0.370639089262113
      - 0.1371806003153324
      - 0.3752907298039645
      - 0.37123463745228946
      - 0.37257279362529516
      - 0.36313337623141706
      - 0.37259148387238383
      - 0.3729198870714754
      - 0.3675658628344536
      - 0.36633120314218104
      - 0.3673505268525332
      - 0.36630234867334366
      - 0.3633181357290596
      - 0.3666789617855102
      - 0.3634543165098876
      - 0.3634777436964214
      - 0.36454468313604593
      - 0.3521232893690467
      - 0.36202764534391463
      - 0.3633464372251183
      - 0.37945868680253625
      - 0.35762058943510056
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l1/lambda_reg_2.0/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 2.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 3.026880700307907
      NRMSE: 8.97778937302055
      mean_wQuantileLoss: 2.875056599248259
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 1.616097909398377
      - 1.500277565792203
      - 1.4698192542418838
      - 1.4428309062495828
      - 1.4311571875587106
      - 1.4334427677094936
      - 1.4207797357812524
      - 1.4129661675542593
      - 1.4098379593342543
      - 1.4082908313721418
      - 1.3940582424402237
      - 1.3931511733680964
      - 1.393974651582539
      - 1.3934827437624335
      - 1.3789459019899368
      - 1.3792079538106918
      - 1.3791921492666006
      - 1.379261864349246
      - 1.3727940581738949
      - 1.379775010049343
      - 1.372142625041306
      val_loss:
      - 0.830124044418335
      - 1.500277565792203
      - 1.4698192542418838
      - 1.4428309062495828
      - 1.4311571875587106
      - 1.4334427677094936
      - 1.4207797357812524
      - 1.4129661675542593
      - 1.4098379593342543
      - 1.4082908313721418
      - 1.3940582424402237
      - 1.3931511733680964
      - 1.393974651582539
      - 1.3934827437624335
      - 1.3789459019899368
      - 1.3792079538106918
      - 1.3791921492666006
      - 1.379261864349246
      - 1.3727940581738949
      - 1.379775010049343
      - 1.372142625041306
  task_id: 3
