continual_learning_setup:
  method: dropout
  method_params:
    dropout_rate: 0.1
  num_tasks: 3
  task_sequence:
  - train_pedestrian_counts.yaml
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_dropout/dropout_rate_0.1/task_1_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.1
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.20439136848679088
      NRMSE: 0.6687820874104172
      mean_wQuantileLoss: 0.16451858017798385
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.49391974438913167
      - 0.35019519715569913
      - 0.3260528070386499
      - 0.27524107845965773
      - 0.2565256549278274
      - 0.26024373539257795
      - 0.23581857373937964
      - 0.22010065254289657
      - 0.20795210369396955
      - 0.21417261706665158
      - 0.196783663763199
      - 0.19180206494638696
      - 0.19949382432969287
      - 0.18576114694587886
      - 0.1891420110478066
      - 0.18187780468724668
      - 0.2056260229437612
      - 0.1819893575157039
      - 0.18501260172342882
      - 0.18155202502384782
      - 0.18296333483885974
      - 0.17308000347111374
      - 0.1767983400495723
      - 0.17812492395751178
      - 0.16983949876157567
      - 0.16748077096417546
      - 0.1723679208662361
      - 0.16787697235122323
      - 0.1897108490811661
      - 0.16747472033603117
      - 0.17074864549795166
      - 0.17072731570806354
      - 0.17087201273534447
      - 0.16729039343772456
      - 0.16586104279849678
      - 0.16506130091147497
      - 0.16950508998706937
      - 0.16639802284771577
      - 0.16927928634686396
      - 0.1619339347234927
      - 0.1650345329544507
      - 0.1634669004706666
      - 0.16327970003476366
      - 0.16396863554837182
      - 0.16278200026135892
      - 0.1622827667160891
      - 0.16884531790856272
      - 0.16136775159975514
      - 0.1582996579236351
      - 0.16302877967245877
      - 0.15881198219722137
      - 0.16004332387819886
      - 0.16022921347757801
      - 0.15885553800035268
      - 0.1560244740685448
      - 0.1589571163058281
      - 0.1574588160146959
      - 0.16313372051808983
      - 0.16210257005877793
      - 0.1585295787663199
      - 0.15558116894681007
      - 0.15819479245692492
      - 0.15905770484823734
      - 0.1592259740573354
      - 0.1583056169329211
      - 0.1568495794199407
      - 0.17642150510801002
      - 0.16066381568089128
      - 0.1544764153077267
      - 0.15730975079350173
      - 0.17683703446527943
      val_loss:
      - 0.3289857730269432
      - 0.35019519715569913
      - 0.3260528070386499
      - 0.27524107845965773
      - 0.2565256549278274
      - 0.26024373539257795
      - 0.23581857373937964
      - 0.22010065254289657
      - 0.20795210369396955
      - 0.21417261706665158
      - 0.196783663763199
      - 0.19180206494638696
      - 0.19949382432969287
      - 0.18576114694587886
      - 0.1891420110478066
      - 0.18187780468724668
      - 0.2056260229437612
      - 0.1819893575157039
      - 0.18501260172342882
      - 0.18155202502384782
      - 0.18296333483885974
      - 0.17308000347111374
      - 0.1767983400495723
      - 0.17812492395751178
      - 0.16983949876157567
      - 0.16748077096417546
      - 0.1723679208662361
      - 0.16787697235122323
      - 0.1897108490811661
      - 0.16747472033603117
      - 0.17074864549795166
      - 0.17072731570806354
      - 0.17087201273534447
      - 0.16729039343772456
      - 0.16586104279849678
      - 0.16506130091147497
      - 0.16950508998706937
      - 0.16639802284771577
      - 0.16927928634686396
      - 0.1619339347234927
      - 0.1650345329544507
      - 0.1634669004706666
      - 0.16327970003476366
      - 0.16396863554837182
      - 0.16278200026135892
      - 0.1622827667160891
      - 0.16884531790856272
      - 0.16136775159975514
      - 0.1582996579236351
      - 0.16302877967245877
      - 0.139081172645092
      - 0.16004332387819886
      - 0.16022921347757801
      - 0.15885553800035268
      - 0.1560244740685448
      - 0.1589571163058281
      - 0.1574588160146959
      - 0.16313372051808983
      - 0.16210257005877793
      - 0.1585295787663199
      - 0.15558116894681007
      - 0.15819479245692492
      - 0.15905770484823734
      - 0.1592259740573354
      - 0.1583056169329211
      - 0.1568495794199407
      - 0.17642150510801002
      - 0.16066381568089128
      - 0.1544764153077267
      - 0.15730975079350173
      - 0.17683703446527943
  task_id: 1
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_dropout/dropout_rate_0.1/task_2_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.1
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.33561946561348593
      NRMSE: 0.6987159103450981
      mean_wQuantileLoss: 0.2608401902891822
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.45924912171904
      - 0.31599461915902793
      - 0.262446001986973
      - 0.23661644058302045
      - 0.21656667557545006
      - 0.22119605354964733
      - 0.20934998360462487
      - 0.21103560749907047
      - 0.1990261118626222
      - 0.19549995940178633
      - 0.1945109730004333
      - 0.19251449464354664
      - 0.19984363170806319
      - 0.18870330165373161
      - 0.1918026374769397
      - 0.1887249741703272
      - 0.18941515928599983
      - 0.19463124126195908
      - 0.18599642999470234
      - 0.18440262973308563
      - 0.18583076319191605
      - 0.18257634423207492
      - 0.1886990291532129
      - 0.18061288294848055
      - 0.18757391307735816
      - 0.18185375211760402
      - 0.1853052094229497
      - 0.18158149009104818
      - 0.1749042478040792
      - 0.179231142741628
      - 0.17298404418397695
      - 0.18057429481996223
      - 0.17420760862296447
      - 0.17501563753467053
      - 0.17729748506098986
      - 0.17965279961936176
      - 0.17733457445865497
      - 0.17488461104221642
      - 0.1760042256792076
      - 0.17929124855436385
      - 0.175868249789346
      - 0.17009075649548322
      - 0.1716200489900075
      - 0.1712926507461816
      - 0.1744107230915688
      - 0.17244282946921885
      - 0.17704691784456372
      - 0.16968244733288884
      - 0.17162694659782574
      - 0.17006897210376337
      - 0.16936561034526676
      - 0.16956761345500126
      - 0.17074535437859595
      - 0.16696768027031794
      - 0.17588724719826132
      - 0.1727857130463235
      - 0.17087952105794102
      - 0.1716616310295649
      - 0.16827015578746796
      - 0.17341093125287443
      - 0.17116438271477818
      - 0.1663088243221864
      - 0.16846791532589123
      - 0.1713107935502194
      - 0.16327183617977425
      - 0.16460651502711698
      - 0.16903163789538667
      - 0.16660179133759812
      - 0.16847679391503334
      - 0.16616149008041248
      - 0.16843628871720284
      val_loss:
      - 0.27908477783203123
      - 0.31599461915902793
      - 0.262446001986973
      - 0.23661644058302045
      - 0.21656667557545006
      - 0.22119605354964733
      - 0.20934998360462487
      - 0.21103560749907047
      - 0.1990261118626222
      - 0.19549995940178633
      - 0.1945109730004333
      - 0.19251449464354664
      - 0.19984363170806319
      - 0.18870330165373161
      - 0.1918026374769397
      - 0.1887249741703272
      - 0.18941515928599983
      - 0.19463124126195908
      - 0.18599642999470234
      - 0.18440262973308563
      - 0.18583076319191605
      - 0.18257634423207492
      - 0.1886990291532129
      - 0.18061288294848055
      - 0.18757391307735816
      - 0.18185375211760402
      - 0.1853052094229497
      - 0.18158149009104818
      - 0.1749042478040792
      - 0.179231142741628
      - 0.17298404418397695
      - 0.18057429481996223
      - 0.17420760862296447
      - 0.17501563753467053
      - 0.17729748506098986
      - 0.17965279961936176
      - 0.17733457445865497
      - 0.17488461104221642
      - 0.1760042256792076
      - 0.17929124855436385
      - 0.175868249789346
      - 0.17009075649548322
      - 0.1716200489900075
      - 0.1712926507461816
      - 0.1744107230915688
      - 0.17244282946921885
      - 0.17704691784456372
      - 0.16968244733288884
      - 0.17162694659782574
      - 0.17006897210376337
      - 0.11879940181970597
      - 0.16956761345500126
      - 0.17074535437859595
      - 0.16696768027031794
      - 0.17588724719826132
      - 0.1727857130463235
      - 0.17087952105794102
      - 0.1716616310295649
      - 0.16827015578746796
      - 0.17341093125287443
      - 0.17116438271477818
      - 0.1663088243221864
      - 0.16846791532589123
      - 0.1713107935502194
      - 0.16327183617977425
      - 0.16460651502711698
      - 0.16903163789538667
      - 0.16660179133759812
      - 0.16847679391503334
      - 0.16616149008041248
      - 0.16843628871720284
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_dropout/dropout_rate_0.1/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.1
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.21534487915576297
      NRMSE: 0.5291224674516588
      mean_wQuantileLoss: 0.1798682941472685
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5224256124347448
      - 0.35477628209628165
      - 0.32494200847577304
      - 0.3154499945230782
      - 0.30337434250395745
      - 0.2925163736799732
      - 0.29094751458615065
      - 0.2906480362871662
      - 0.28858944785315543
      - 0.2866782897617668
      - 0.27733140462078154
      - 0.27922319201752543
      - 0.27982871572021395
      - 0.2766509058419615
      - 0.2734086290001869
      - 0.27712015353608876
      - 0.2744672578992322
      - 0.27621917403303087
      - 0.27718450978863984
      - 0.26929045119322836
      - 0.2648720156867057
      - 0.2670396730536595
      - 0.2663567657582462
      - 0.26364810997620225
      - 0.2677584484918043
      - 0.2704287408851087
      - 0.2598813573131338
      - 0.2622940839501098
      - 0.25985515816137195
      - 0.2631302020745352
      - 0.26396693382412195
      - 0.2562590945744887
      - 0.26543333556037396
      - 0.2595292606856674
      - 0.2613012940855697
      - 0.26815534045454115
      - 0.26034168677870184
      - 0.2591944334562868
      - 0.2575529516907409
      - 0.25466739863622934
      - 0.2566490429453552
      - 0.25274147652089596
      - 0.25842415110673755
      - 0.2565091891447082
      - 0.25275077600963414
      - 0.2560752003919333
      - 0.2541793832788244
      - 0.25470246945042163
      - 0.25003683287650347
      - 0.2465123098809272
      - 0.24878251610789448
      - 0.24892563081812114
      - 0.2521868735784665
      - 0.24832389247603714
      - 0.25078109349124134
      - 0.2477468807483092
      - 0.2565833783010021
      - 0.25610739877447486
      - 0.25355568970553577
      - 0.25604886829387397
      - 0.2508663040352985
      - 0.25335298548452556
      - 0.25194356276188046
      - 0.2578744230559096
      - 0.25194663915317506
      - 0.24868033616803586
      - 0.2511731554986909
      - 0.2493832737673074
      - 0.24796713574323803
      - 0.25269435974769294
      - 0.25010892970021814
      val_loss:
      - 0.31990182101726533
      - 0.35477628209628165
      - 0.32494200847577304
      - 0.3154499945230782
      - 0.30337434250395745
      - 0.2925163736799732
      - 0.29094751458615065
      - 0.2906480362871662
      - 0.28858944785315543
      - 0.2866782897617668
      - 0.27733140462078154
      - 0.27922319201752543
      - 0.27982871572021395
      - 0.2766509058419615
      - 0.2734086290001869
      - 0.27712015353608876
      - 0.2744672578992322
      - 0.27621917403303087
      - 0.27718450978863984
      - 0.26929045119322836
      - 0.2648720156867057
      - 0.2670396730536595
      - 0.2663567657582462
      - 0.26364810997620225
      - 0.2677584484918043
      - 0.2704287408851087
      - 0.2598813573131338
      - 0.2622940839501098
      - 0.25985515816137195
      - 0.2631302020745352
      - 0.26396693382412195
      - 0.2562590945744887
      - 0.26543333556037396
      - 0.2595292606856674
      - 0.2613012940855697
      - 0.26815534045454115
      - 0.26034168677870184
      - 0.2591944334562868
      - 0.2575529516907409
      - 0.25466739863622934
      - 0.2566490429453552
      - 0.25274147652089596
      - 0.25842415110673755
      - 0.2565091891447082
      - 0.25275077600963414
      - 0.2560752003919333
      - 0.2541793832788244
      - 0.25470246945042163
      - 0.25003683287650347
      - 0.2465123098809272
      - 0.2114549458026886
      - 0.24892563081812114
      - 0.2521868735784665
      - 0.24832389247603714
      - 0.25078109349124134
      - 0.2477468807483092
      - 0.2565833783010021
      - 0.25610739877447486
      - 0.25355568970553577
      - 0.25604886829387397
      - 0.2508663040352985
      - 0.25335298548452556
      - 0.25194356276188046
      - 0.2578744230559096
      - 0.25194663915317506
      - 0.24868033616803586
      - 0.2511731554986909
      - 0.2493832737673074
      - 0.24796713574323803
      - 0.25269435974769294
      - 0.25010892970021814
  task_id: 3
