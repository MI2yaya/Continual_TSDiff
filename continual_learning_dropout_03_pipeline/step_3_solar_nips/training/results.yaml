best_checkpoint: null
config:
  batch_size: 64
  context_length: 336
  dataset: solar_nips
  device: cuda:0
  diffusion_config: diffusion_small_config
  dropout_rate: 0.3
  eval_every: 50
  freq: H
  gradient_clip_val: 0.5
  init_skip: false
  lr: 0.001
  max_epochs: 1000
  model: unconditional
  normalization: mean
  num_batches_per_epoch: 128
  num_samples: 16
  prediction_length: 24
  resume_from_checkpoint: ./continual_learning_dropout_03_pipeline/step_2_pedestrian_counts/training/pedestrian_counts_checkpoint_best.pth
  sampler: ddpm
  sampler_params:
    guidance: quantile
    scale: 8
  setup: forecasting
  use_features: false
  use_lags: true
  use_validation_set: true
metrics:
- ND: 0.500061382759069
  NRMSE: 1.1060571058866204
  mean_wQuantileLoss: 0.3835029026257548
  missing_scenario: none
  missing_values: 0
training_history:
  epochs:
  - 52
  - 53
  - 54
  - 55
  - 56
  - 57
  - 58
  - 59
  - 60
  - 61
  - 62
  - 63
  - 64
  - 65
  - 66
  - 67
  - 68
  - 69
  - 70
  - 71
  - 72
  - 73
  - 74
  - 75
  - 76
  - 77
  - 78
  - 79
  - 80
  - 81
  - 82
  - 83
  - 84
  - 85
  - 86
  - 87
  - 88
  - 89
  - 90
  - 91
  - 92
  - 93
  - 94
  - 95
  - 96
  - 97
  - 98
  - 99
  - 100
  - 101
  - 102
  - 103
  - 104
  - 105
  - 106
  - 107
  - 108
  - 109
  - 110
  - 111
  - 112
  - 113
  - 114
  - 115
  - 116
  - 117
  - 118
  - 119
  - 120
  - 121
  train_loss:
  - 0.32646155497059226
  - 0.2072432228596881
  - 0.19521313882432878
  - 0.18507844721898437
  - 0.1805871929973364
  - 0.17740062810480595
  - 0.17440024367533624
  - 0.1726854102453217
  - 0.17005973332561553
  - 0.16914920933777466
  - 0.16943249269388616
  - 0.16658606455894187
  - 0.16605028515914455
  - 0.16285640967544168
  - 0.16005940514151007
  - 0.16250965592917055
  - 0.16156387026421726
  - 0.16152416710974649
  - 0.16064574819756672
  - 0.15814076527021825
  - 0.15902068425202742
  - 0.15706780145410448
  - 0.15405359730357304
  - 0.15583881887141615
  - 0.15671094227582216
  - 0.15463910368271172
  - 0.15215343434829265
  - 0.15361537592252716
  - 0.15383324079448357
  - 0.15388554107630625
  - 0.15207170374924317
  - 0.1553044298198074
  - 0.15269633563002571
  - 0.14993989013601094
  - 0.15226552763488144
  - 0.15116941480664536
  - 0.14694325166055933
  - 0.15106285898946226
  - 0.14706404937896878
  - 0.15127866435796022
  - 0.14825650403508916
  - 0.14905194274615496
  - 0.1449044369510375
  - 0.1493115602643229
  - 0.1459128598216921
  - 0.1453952532610856
  - 0.14729959436226636
  - 0.14952206920133904
  - 0.14679378655273467
  - 0.14678136142902076
  - 0.1472342586494051
  - 0.14522900310112163
  - 0.1453984443214722
  - 0.14591095928335562
  - 0.14828861184651032
  - 0.1475298402365297
  - 0.14424558484461159
  - 0.14544131234288216
  - 0.14835865958593786
  - 0.1454991289647296
  - 0.14325932605424896
  - 0.14480193791678175
  - 0.14255123725160956
  - 0.14430629828711972
  - 0.14353130420204252
  - 0.14393526344792917
  - 0.14162651932565495
  - 0.143096343788784
  - 0.14365921664284542
  - 0.14450671337544918
  val_loss:
  - 0.32646155497059226
  - 0.2072432228596881
  - 0.19521313882432878
  - 0.18507844721898437
  - 0.1805871929973364
  - 0.17740062810480595
  - 0.17440024367533624
  - 0.1726854102453217
  - 0.17005973332561553
  - 0.16914920933777466
  - 0.16943249269388616
  - 0.16658606455894187
  - 0.16605028515914455
  - 0.16285640967544168
  - 0.16005940514151007
  - 0.16250965592917055
  - 0.16156387026421726
  - 0.16152416710974649
  - 0.16064574819756672
  - 0.15814076527021825
  - 0.15902068425202742
  - 0.15706780145410448
  - 0.15405359730357304
  - 0.15583881887141615
  - 0.15671094227582216
  - 0.15463910368271172
  - 0.15215343434829265
  - 0.15361537592252716
  - 0.15383324079448357
  - 0.15388554107630625
  - 0.15207170374924317
  - 0.1553044298198074
  - 0.15269633563002571
  - 0.14993989013601094
  - 0.15226552763488144
  - 0.15116941480664536
  - 0.14694325166055933
  - 0.15106285898946226
  - 0.14706404937896878
  - 0.15127866435796022
  - 0.14825650403508916
  - 0.14905194274615496
  - 0.1449044369510375
  - 0.1493115602643229
  - 0.1459128598216921
  - 0.1453952532610856
  - 0.14729959436226636
  - 0.14952206920133904
  - 0.14679378655273467
  - 0.11725585410992305
  - 0.1472342586494051
  - 0.14522900310112163
  - 0.1453984443214722
  - 0.14591095928335562
  - 0.14828861184651032
  - 0.1475298402365297
  - 0.14424558484461159
  - 0.14544131234288216
  - 0.14835865958593786
  - 0.1454991289647296
  - 0.14325932605424896
  - 0.14480193791678175
  - 0.14255123725160956
  - 0.14430629828711972
  - 0.14353130420204252
  - 0.14393526344792917
  - 0.14162651932565495
  - 0.143096343788784
  - 0.14365921664284542
  - 0.14450671337544918
