continual_learning_setup:
  method: score_l2
  method_params:
    lambda_reg: 0.5
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l2/lambda_reg_0.5/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.5
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3253105403874573
      NRMSE: 0.6847896519891136
      mean_wQuantileLoss: 0.25511926109765565
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.41548542748205364
      - 0.2853026163065806
      - 0.23184308607596904
      - 0.22006177413277328
      - 0.20342857955256477
      - 0.20129437022842467
      - 0.18961070146178827
      - 0.1946948990225792
      - 0.1970863503520377
      - 0.19865133840357885
      - 0.19408059335546568
      - 0.18988457508385181
      - 0.1797778132604435
      - 0.17792674590600654
      - 0.17494741617701948
      - 0.17731583071872592
      - 0.17345176840899512
      - 0.1726633661892265
      - 0.174494112317916
      - 0.17096736055100337
      - 0.17384908528765664
      - 0.17190141696482897
      - 0.17345057765487581
      - 0.16750871611293405
      - 0.16591099399374798
      - 0.17484866408631206
      - 0.16814131365390494
      - 0.15989403001731262
      - 0.17233195510925725
      - 0.1656514651258476
      - 0.17264030495425686
      - 0.1672860700637102
      - 0.16394703049445525
      - 0.1678760663489811
      - 0.1636802128632553
      - 0.1597268140176311
      - 0.16401002276688814
      - 0.1666059775161557
      - 0.15951681468868628
      - 0.16352188220480457
      - 0.16891326737822965
      - 0.16303391620749608
      - 0.16234754008473828
      - 0.16079114767489955
      - 0.1599808331229724
      - 0.16493451973656192
      - 0.16121655370807275
      - 0.1617592130205594
      - 0.16060512396506965
      - 0.16164744622074068
      - 0.16061525762779638
      - 0.16003216459648684
      - 0.1642206089454703
      - 0.16092577355448157
      - 0.15940842946292832
      - 0.16141200810670853
      - 0.15795637795235962
      - 0.16046605707379058
      - 0.16312538977945223
      - 0.15896741027245298
      - 0.1583987814374268
      - 0.15991124894935638
      - 0.1609706089948304
      - 0.16045205848058686
      - 0.15559266065247357
      - 0.1529405309120193
      - 0.15541654877597466
      - 0.16045478417072445
      - 0.16102985269390047
      - 0.15834674978395924
      - 0.15316521102795377
      val_loss:
      - 0.29734891951084136
      - 0.2853026163065806
      - 0.23184308607596904
      - 0.22006177413277328
      - 0.20342857955256477
      - 0.20129437022842467
      - 0.18961070146178827
      - 0.1946948990225792
      - 0.1970863503520377
      - 0.19865133840357885
      - 0.19408059335546568
      - 0.18988457508385181
      - 0.1797778132604435
      - 0.17792674590600654
      - 0.17494741617701948
      - 0.17731583071872592
      - 0.17345176840899512
      - 0.1726633661892265
      - 0.174494112317916
      - 0.17096736055100337
      - 0.17384908528765664
      - 0.17190141696482897
      - 0.17345057765487581
      - 0.16750871611293405
      - 0.16591099399374798
      - 0.17484866408631206
      - 0.16814131365390494
      - 0.15989403001731262
      - 0.17233195510925725
      - 0.1656514651258476
      - 0.17264030495425686
      - 0.1672860700637102
      - 0.16394703049445525
      - 0.1678760663489811
      - 0.1636802128632553
      - 0.1597268140176311
      - 0.16401002276688814
      - 0.1666059775161557
      - 0.15951681468868628
      - 0.16352188220480457
      - 0.16891326737822965
      - 0.16303391620749608
      - 0.16234754008473828
      - 0.16079114767489955
      - 0.1599808331229724
      - 0.16493451973656192
      - 0.16121655370807275
      - 0.1617592130205594
      - 0.16060512396506965
      - 0.16164744622074068
      - 0.1374366447329521
      - 0.16003216459648684
      - 0.1642206089454703
      - 0.16092577355448157
      - 0.15940842946292832
      - 0.16141200810670853
      - 0.15795637795235962
      - 0.16046605707379058
      - 0.16312538977945223
      - 0.15896741027245298
      - 0.1583987814374268
      - 0.15991124894935638
      - 0.1609706089948304
      - 0.16045205848058686
      - 0.15559266065247357
      - 0.1529405309120193
      - 0.15541654877597466
      - 0.16045478417072445
      - 0.16102985269390047
      - 0.15834674978395924
      - 0.15316521102795377
  task_id: 1
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l2/lambda_reg_0.5/task_2_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.5
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.2121807710016832
      NRMSE: 0.7236511867348526
      mean_wQuantileLoss: 0.17470959742573378
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      train_loss:
      - 0.6076703381258994
      - 0.4186525106197223
      - 0.3368857541354373
      - 0.29421267902944237
      - 0.26381676527671516
      - 0.24641140759922564
      - 0.2510193655034527
      - 0.22165204014163464
      - 0.21625761163886636
      - 0.21462287404574454
      - 0.21315014956053346
      - 0.21063105401117355
      - 0.20773816225118935
      - 0.20370840886607766
      - 0.20839536329731345
      - 0.2019565935479477
      - 0.1923184598563239
      - 0.20711001998279244
      - 0.19133040658198297
      - 0.1866938362363726
      - 0.20717273617628962
      - 0.22893736080732197
      - 0.19461026915814728
      - 0.19084686669521034
      - 0.18790930416435003
      - 0.18886890483554453
      - 0.1868872430641204
      - 0.19252124254126102
      - 0.1833894091541879
      - 0.1857641926035285
      - 0.18438250536564738
      - 0.18866650067502633
      - 0.19656244688667357
      - 0.19282833684701473
      - 0.19662152556702495
      - 0.18650195957161486
      - 0.18089074059389532
      - 0.18413104442879558
      - 0.18582662660628557
      - 0.18291716964449733
      - 0.18339809373719618
      - 0.2084959409548901
      - 0.18137717311037704
      - 0.1821662134025246
      - 0.18233629985479638
      - 0.179005968850106
      - 0.18146142771001905
      - 0.18045778351370245
      - 0.17851558496477082
      - 0.20831883209757507
      - 0.18255072901956737
      - 0.17857604019809514
      - 0.18166593479691073
      - 0.20618573913816363
      - 0.17529246053891256
      - 0.17903049720916897
      - 0.193718648632057
      - 0.17810793546959758
      - 0.1793613653862849
      - 0.18917900719679892
      - 0.18153728893958032
      - 0.18175126978894696
      - 0.17592769564362243
      - 0.179694595746696
      - 0.1805284817237407
      - 0.17906254436820745
      - 0.17691426014062017
      - 0.18014474801020697
      - 0.17715616791974753
      - 0.18987719557480887
      - 0.17646654462441802
      - 0.17454147833632305
      - 0.17635574890300632
      - 0.17811499553499743
      - 0.17396053613629192
      - 0.17880415834952146
      - 0.17874799232231453
      - 0.18034765037009493
      - 0.17645503400126472
      - 0.185737372841686
      - 0.18047457648208365
      - 0.17694587190635502
      - 0.1742843915708363
      - 0.17833004682324827
      - 0.1791585116297938
      - 0.17950803268468007
      - 0.1781613347120583
      - 0.18124282557982951
      - 0.17484766867710277
      - 0.17435066180769354
      - 0.17845304415095598
      - 0.18409718410111964
      - 0.194124978617765
      - 0.174930325942114
      - 0.17427889059763402
      val_loss:
      - 0.40719926357269287
      - 0.4186525106197223
      - 0.3368857541354373
      - 0.29421267902944237
      - 0.26381676527671516
      - 0.24641140759922564
      - 0.2510193655034527
      - 0.22165204014163464
      - 0.21625761163886636
      - 0.21462287404574454
      - 0.21315014956053346
      - 0.21063105401117355
      - 0.20773816225118935
      - 0.20370840886607766
      - 0.20839536329731345
      - 0.2019565935479477
      - 0.1923184598563239
      - 0.20711001998279244
      - 0.19133040658198297
      - 0.1866938362363726
      - 0.20717273617628962
      - 0.22893736080732197
      - 0.19461026915814728
      - 0.19084686669521034
      - 0.18790930416435003
      - 0.18886890483554453
      - 0.1868872430641204
      - 0.19252124254126102
      - 0.1833894091541879
      - 0.1857641926035285
      - 0.18438250536564738
      - 0.18866650067502633
      - 0.19656244688667357
      - 0.19282833684701473
      - 0.19662152556702495
      - 0.18650195957161486
      - 0.18089074059389532
      - 0.18413104442879558
      - 0.18582662660628557
      - 0.18291716964449733
      - 0.18339809373719618
      - 0.2084959409548901
      - 0.18137717311037704
      - 0.1821662134025246
      - 0.18233629985479638
      - 0.179005968850106
      - 0.18146142771001905
      - 0.18045778351370245
      - 0.17851558496477082
      - 0.20831883209757507
      - 0.3972999304533005
      - 0.17857604019809514
      - 0.18166593479691073
      - 0.20618573913816363
      - 0.17529246053891256
      - 0.17903049720916897
      - 0.193718648632057
      - 0.17810793546959758
      - 0.1793613653862849
      - 0.18917900719679892
      - 0.18153728893958032
      - 0.18175126978894696
      - 0.17592769564362243
      - 0.179694595746696
      - 0.1805284817237407
      - 0.17906254436820745
      - 0.17691426014062017
      - 0.18014474801020697
      - 0.17715616791974753
      - 0.18987719557480887
      - 0.17646654462441802
      - 0.17454147833632305
      - 0.17635574890300632
      - 0.17811499553499743
      - 0.17396053613629192
      - 0.17880415834952146
      - 0.17874799232231453
      - 0.18034765037009493
      - 0.17645503400126472
      - 0.185737372841686
      - 0.18047457648208365
      - 0.17694587190635502
      - 0.1742843915708363
      - 0.17833004682324827
      - 0.1791585116297938
      - 0.17950803268468007
      - 0.1781613347120583
      - 0.18124282557982951
      - 0.17484766867710277
      - 0.17435066180769354
      - 0.17845304415095598
      - 0.18409718410111964
      - 0.194124978617765
      - 0.174930325942114
      - 0.17427889059763402
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l2/lambda_reg_0.5/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.5
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.5076058062744516
      NRMSE: 3.053604078118703
      mean_wQuantileLoss: 0.48974176553429505
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 1.303373749833554
      - 1.1253580288030207
      - 1.0434474828653038
      - 1.019396270159632
      - 1.0319701177068055
      - 0.9939252166077495
      - 1.009421600960195
      - 0.9945374694652855
      - 0.9966614837758243
      - 1.0062242881394923
      - 1.0205019167624414
      - 0.9991387114860117
      - 0.9691495033912361
      - 0.9727281760424376
      - 0.9945574947632849
      - 0.9962297887541354
      - 0.989617567975074
      - 0.9717170740477741
      - 0.9774681124836206
      - 0.9747550655156374
      - 0.988135751336813
      val_loss:
      - 0.4625467896461487
      - 1.1253580288030207
      - 1.0434474828653038
      - 1.019396270159632
      - 1.0319701177068055
      - 0.9939252166077495
      - 1.009421600960195
      - 0.9945374694652855
      - 0.9966614837758243
      - 1.0062242881394923
      - 1.0205019167624414
      - 0.9991387114860117
      - 0.9691495033912361
      - 0.9727281760424376
      - 0.9945574947632849
      - 0.9962297887541354
      - 0.989617567975074
      - 0.9717170740477741
      - 0.9774681124836206
      - 0.9747550655156374
      - 0.988135751336813
  task_id: 3
