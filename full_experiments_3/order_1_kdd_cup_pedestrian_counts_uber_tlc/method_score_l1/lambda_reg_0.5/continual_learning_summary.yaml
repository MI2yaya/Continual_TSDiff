continual_learning_setup:
  method: score_l1
  method_params:
    lambda_reg: 0.5
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l1/lambda_reg_0.5/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.5
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.34158121458838225
      NRMSE: 0.7018418435233651
      mean_wQuantileLoss: 0.2657313020181489
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4200037420960143
      - 0.28795298794284463
      - 0.24562214163597673
      - 0.22171654272824526
      - 0.20887656533159316
      - 0.1991853779181838
      - 0.19985514937434345
      - 0.19500984088517725
      - 0.19285836728522554
      - 0.19244430761318654
      - 0.18330481159500778
      - 0.18434730783337727
      - 0.18094180058687925
      - 0.17980965977767482
      - 0.18204956460976973
      - 0.1741398741141893
      - 0.18151548790046945
      - 0.17619600292528048
      - 0.17288360896054655
      - 0.1736079174443148
      - 0.16836274549132213
      - 0.1682271240861155
      - 0.16770309786079451
      - 0.17193541617598385
      - 0.17193917487747967
      - 0.16863169241696596
      - 0.1657911878428422
      - 0.16666434181388468
      - 0.1728245567646809
      - 0.1664469361421652
      - 0.1658320538699627
      - 0.1677061269292608
      - 0.16967297246446833
      - 0.16595431650057435
      - 0.1627439102740027
      - 0.1692577861249447
      - 0.16374363243812695
      - 0.16488345758989453
      - 0.15856955386698246
      - 0.1658095241873525
      - 0.16446448309579864
      - 0.1651585383224301
      - 0.16079743875889108
      - 0.16205290710786358
      - 0.16280499001732096
      - 0.15909863804699853
      - 0.1593201202340424
      - 0.1576033306773752
      - 0.16668170224875212
      - 0.16252999065909535
      - 0.16603715770179406
      - 0.1605850484338589
      - 0.1558725266950205
      - 0.158216493553482
      - 0.1567106029833667
      - 0.1600917082396336
      - 0.16228875878732651
      - 0.1590202257502824
      - 0.162610107450746
      - 0.15923900378402323
      - 0.15683131758123636
      - 0.160530952969566
      - 0.16205062298104167
      - 0.15855365403695032
      - 0.15951661625877023
      - 0.1628221290302463
      - 0.15476589248282835
      - 0.15393467497779056
      - 0.15641423902707174
      - 0.1575795192620717
      - 0.1540320788626559
      val_loss:
      - 0.2887607216835022
      - 0.28795298794284463
      - 0.24562214163597673
      - 0.22171654272824526
      - 0.20887656533159316
      - 0.1991853779181838
      - 0.19985514937434345
      - 0.19500984088517725
      - 0.19285836728522554
      - 0.19244430761318654
      - 0.18330481159500778
      - 0.18434730783337727
      - 0.18094180058687925
      - 0.17980965977767482
      - 0.18204956460976973
      - 0.1741398741141893
      - 0.18151548790046945
      - 0.17619600292528048
      - 0.17288360896054655
      - 0.1736079174443148
      - 0.16836274549132213
      - 0.1682271240861155
      - 0.16770309786079451
      - 0.17193541617598385
      - 0.17193917487747967
      - 0.16863169241696596
      - 0.1657911878428422
      - 0.16666434181388468
      - 0.1728245567646809
      - 0.1664469361421652
      - 0.1658320538699627
      - 0.1677061269292608
      - 0.16967297246446833
      - 0.16595431650057435
      - 0.1627439102740027
      - 0.1692577861249447
      - 0.16374363243812695
      - 0.16488345758989453
      - 0.15856955386698246
      - 0.1658095241873525
      - 0.16446448309579864
      - 0.1651585383224301
      - 0.16079743875889108
      - 0.16205290710786358
      - 0.16280499001732096
      - 0.15909863804699853
      - 0.1593201202340424
      - 0.1576033306773752
      - 0.16668170224875212
      - 0.16252999065909535
      - 0.1427044153213501
      - 0.1605850484338589
      - 0.1558725266950205
      - 0.158216493553482
      - 0.1567106029833667
      - 0.1600917082396336
      - 0.16228875878732651
      - 0.1590202257502824
      - 0.162610107450746
      - 0.15923900378402323
      - 0.15683131758123636
      - 0.160530952969566
      - 0.16205062298104167
      - 0.15855365403695032
      - 0.15951661625877023
      - 0.1628221290302463
      - 0.15476589248282835
      - 0.15393467497779056
      - 0.15641423902707174
      - 0.1575795192620717
      - 0.1540320788626559
  task_id: 1
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l1/lambda_reg_0.5/task_2_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.5
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.2494180419832296
      NRMSE: 0.8038100446871976
      mean_wQuantileLoss: 0.20423596460437463
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5958299345802516
      - 0.4603050153236836
      - 0.4139141249470413
      - 0.3670299636432901
      - 0.35160997381899506
      - 0.3234803735977039
      - 0.3114960065577179
      - 0.2914769717026502
      - 0.27986671472899616
      - 0.2704209942603484
      - 0.27080817078240216
      - 0.2651591975009069
      - 0.26267557532992214
      - 0.2638644417747855
      - 0.2552721375832334
      - 0.25386134907603264
      - 0.25299181730952114
      - 0.25048160075675696
      - 0.2540076116565615
      - 0.2645990418968722
      - 0.24470279342494905
      - 0.24469409743323922
      - 0.24586061690934002
      - 0.2563440517988056
      - 0.2469104533083737
      - 0.25913748424500227
      - 0.23730872583109885
      - 0.24239790649153292
      - 0.23845656344201416
      - 0.2358198076253757
      - 0.2370613666716963
      - 0.24059200915507972
      - 0.2427485443186015
      - 0.23419168032705784
      - 0.24012343678623438
      - 0.231022939668037
      - 0.23422293993644416
      - 0.239205363788642
      - 0.2466013670200482
      - 0.23364579665940255
      - 0.23808065184857696
      - 0.23127336066681892
      - 0.2416603093734011
      - 0.23778029100503772
      - 0.23546469525899738
      - 0.2351385576184839
      - 0.23187881708145142
      - 0.23541938874404877
      - 0.23654179775621742
      - 0.23182174400426447
      - 0.23343714664224535
      - 0.24402275204192847
      - 0.23263208707794547
      - 0.2315406345296651
      - 0.2376416950719431
      - 0.2327391585567966
      - 0.2307619343046099
      - 0.2279377644881606
      - 0.22825033427216113
      - 0.22805772349238396
      - 0.2338760900311172
      - 0.23166147014126182
      - 0.2328189826803282
      - 0.23097285081166774
      - 0.23217588278930634
      - 0.2352773412130773
      - 0.23401366721373051
      - 0.2275797709589824
      - 0.22941895690746605
      - 0.22975224198307842
      - 0.22521330369636416
      val_loss:
      - 0.4481893628835678
      - 0.4603050153236836
      - 0.4139141249470413
      - 0.3670299636432901
      - 0.35160997381899506
      - 0.3234803735977039
      - 0.3114960065577179
      - 0.2914769717026502
      - 0.27986671472899616
      - 0.2704209942603484
      - 0.27080817078240216
      - 0.2651591975009069
      - 0.26267557532992214
      - 0.2638644417747855
      - 0.2552721375832334
      - 0.25386134907603264
      - 0.25299181730952114
      - 0.25048160075675696
      - 0.2540076116565615
      - 0.2645990418968722
      - 0.24470279342494905
      - 0.24469409743323922
      - 0.24586061690934002
      - 0.2563440517988056
      - 0.2469104533083737
      - 0.25913748424500227
      - 0.23730872583109885
      - 0.24239790649153292
      - 0.23845656344201416
      - 0.2358198076253757
      - 0.2370613666716963
      - 0.24059200915507972
      - 0.2427485443186015
      - 0.23419168032705784
      - 0.24012343678623438
      - 0.231022939668037
      - 0.23422293993644416
      - 0.239205363788642
      - 0.2466013670200482
      - 0.23364579665940255
      - 0.23808065184857696
      - 0.23127336066681892
      - 0.2416603093734011
      - 0.23778029100503772
      - 0.23546469525899738
      - 0.2351385576184839
      - 0.23187881708145142
      - 0.23541938874404877
      - 0.23654179775621742
      - 0.23182174400426447
      - 0.14852404594421387
      - 0.24402275204192847
      - 0.23263208707794547
      - 0.2315406345296651
      - 0.2376416950719431
      - 0.2327391585567966
      - 0.2307619343046099
      - 0.2279377644881606
      - 0.22825033427216113
      - 0.22805772349238396
      - 0.2338760900311172
      - 0.23166147014126182
      - 0.2328189826803282
      - 0.23097285081166774
      - 0.23217588278930634
      - 0.2352773412130773
      - 0.23401366721373051
      - 0.2275797709589824
      - 0.22941895690746605
      - 0.22975224198307842
      - 0.22521330369636416
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l1/lambda_reg_0.5/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.5
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3134548851822934
      NRMSE: 1.419172886620554
      mean_wQuantileLoss: 0.3093505506697432
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 0.8722202968783677
      - 0.7384933582507074
      - 0.7182589643634856
      - 0.706435477361083
      - 0.7008805093355477
      - 0.6910896487534046
      - 0.6945898723788559
      - 0.6853810348547995
      - 0.6802925853990018
      - 0.6752794440835714
      - 0.6752123171463609
      - 0.6754245688207448
      - 0.6765570882707834
      - 0.6779205417260528
      - 0.6761912419460714
      - 0.6663762172684073
      - 0.6671968959271908
      - 0.6705665490590036
      - 0.6673725824803114
      - 0.6653524367138743
      - 0.6611966635100543
      val_loss:
      - 0.42554472088813783
      - 0.7384933582507074
      - 0.7182589643634856
      - 0.706435477361083
      - 0.7008805093355477
      - 0.6910896487534046
      - 0.6945898723788559
      - 0.6853810348547995
      - 0.6802925853990018
      - 0.6752794440835714
      - 0.6752123171463609
      - 0.6754245688207448
      - 0.6765570882707834
      - 0.6779205417260528
      - 0.6761912419460714
      - 0.6663762172684073
      - 0.6671968959271908
      - 0.6705665490590036
      - 0.6673725824803114
      - 0.6653524367138743
      - 0.6611966635100543
  task_id: 3
