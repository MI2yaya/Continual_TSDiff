continual_learning_setup:
  method: score_l1
  method_params:
    lambda_reg: 0.5
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
  - train_pedestrian_counts.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l1/lambda_reg_0.5/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.5
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.36164181646832716
      NRMSE: 0.7196341412195817
      mean_wQuantileLoss: 0.2819087582922268
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4161682145204395
      - 0.27313398476690054
      - 0.23289524350548163
      - 0.21121063036844134
      - 0.20016632042825222
      - 0.20016730902716517
      - 0.1965726909111254
      - 0.19147678662557155
      - 0.18959926709067076
      - 0.18790529924444854
      - 0.18709359428612515
      - 0.18472058657789603
      - 0.17150043410947546
      - 0.17627061356324703
      - 0.1777810085332021
      - 0.17642516683554277
      - 0.17691425705561414
      - 0.17509552877163514
      - 0.18277237744769081
      - 0.171078669547569
      - 0.1704982099472545
      - 0.1716840957524255
      - 0.17159985873149708
      - 0.17418573948089033
      - 0.16777092317352071
      - 0.1677237765979953
      - 0.16932396858464926
      - 0.16875717538641766
      - 0.1685716729843989
      - 0.17082892754115164
      - 0.16282012197189033
      - 0.16929573734523728
      - 0.16566458670422435
      - 0.16723556391661987
      - 0.16120033647166565
      - 0.1626094586099498
      - 0.16891016229055822
      - 0.16349148290464655
      - 0.16382209217408672
      - 0.1663863385329023
      - 0.16397455020342022
      - 0.16266450547846034
      - 0.161895937519148
      - 0.16203347739065066
      - 0.1683131626341492
      - 0.15932195913046598
      - 0.16548438288737088
      - 0.17292457452276722
      - 0.16555664979387075
      - 0.16725235630292445
      - 0.1604241299792193
      - 0.157308756664861
      - 0.1586842485703528
      - 0.1632517905673012
      - 0.156947661715094
      - 0.16389556595822796
      - 0.15832183399470523
      - 0.1634954062756151
      - 0.15926823939662427
      - 0.1600243506836705
      - 0.16473877761745825
      - 0.15756616462022066
      - 0.15601429261732846
      - 0.16535283642588183
      - 0.15564235288184136
      - 0.16184288286603987
      - 0.15452720067696646
      - 0.1594976412015967
      - 0.15923641540575773
      - 0.16124559339368716
      - 0.16034853871678934
      val_loss:
      - 0.2871508449316025
      - 0.27313398476690054
      - 0.23289524350548163
      - 0.21121063036844134
      - 0.20016632042825222
      - 0.20016730902716517
      - 0.1965726909111254
      - 0.19147678662557155
      - 0.18959926709067076
      - 0.18790529924444854
      - 0.18709359428612515
      - 0.18472058657789603
      - 0.17150043410947546
      - 0.17627061356324703
      - 0.1777810085332021
      - 0.17642516683554277
      - 0.17691425705561414
      - 0.17509552877163514
      - 0.18277237744769081
      - 0.171078669547569
      - 0.1704982099472545
      - 0.1716840957524255
      - 0.17159985873149708
      - 0.17418573948089033
      - 0.16777092317352071
      - 0.1677237765979953
      - 0.16932396858464926
      - 0.16875717538641766
      - 0.1685716729843989
      - 0.17082892754115164
      - 0.16282012197189033
      - 0.16929573734523728
      - 0.16566458670422435
      - 0.16723556391661987
      - 0.16120033647166565
      - 0.1626094586099498
      - 0.16891016229055822
      - 0.16349148290464655
      - 0.16382209217408672
      - 0.1663863385329023
      - 0.16397455020342022
      - 0.16266450547846034
      - 0.161895937519148
      - 0.16203347739065066
      - 0.1683131626341492
      - 0.15932195913046598
      - 0.16548438288737088
      - 0.17292457452276722
      - 0.16555664979387075
      - 0.16725235630292445
      - 0.14618245959281922
      - 0.157308756664861
      - 0.1586842485703528
      - 0.1632517905673012
      - 0.156947661715094
      - 0.16389556595822796
      - 0.15832183399470523
      - 0.1634954062756151
      - 0.15926823939662427
      - 0.1600243506836705
      - 0.16473877761745825
      - 0.15756616462022066
      - 0.15601429261732846
      - 0.16535283642588183
      - 0.15564235288184136
      - 0.16184288286603987
      - 0.15452720067696646
      - 0.1594976412015967
      - 0.15923641540575773
      - 0.16124559339368716
      - 0.16034853871678934
  task_id: 1
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l1/lambda_reg_0.5/task_2_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.5
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.26336834180868385
      NRMSE: 1.2878652764635965
      mean_wQuantileLoss: 0.2941306219269253
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 0.8737577004358172
      - 0.7397432834841311
      - 0.7208386952988803
      - 0.7091385908424854
      - 0.7052805656567216
      - 0.7042663958854973
      - 0.695218229200691
      - 0.6932649840600789
      - 0.685587243642658
      - 0.6837653517723083
      - 0.6890413612127304
      - 0.6783408396877348
      - 0.680806924123317
      - 0.6798844924196601
      - 0.6779426950961351
      - 0.6846409165300429
      - 0.6738032731227577
      - 0.672806631308049
      - 0.6687574074603617
      - 0.672063966281712
      - 0.6647044159471989
      val_loss:
      - 0.360625696182251
      - 0.7397432834841311
      - 0.7208386952988803
      - 0.7091385908424854
      - 0.7052805656567216
      - 0.7042663958854973
      - 0.695218229200691
      - 0.6932649840600789
      - 0.685587243642658
      - 0.6837653517723083
      - 0.6890413612127304
      - 0.6783408396877348
      - 0.680806924123317
      - 0.6798844924196601
      - 0.6779426950961351
      - 0.6846409165300429
      - 0.6738032731227577
      - 0.672806631308049
      - 0.6687574074603617
      - 0.672063966281712
      - 0.6647044159471989
  task_id: 2
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l1/lambda_reg_0.5/task_3_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.5
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.2581516352589892
      NRMSE: 0.8546341569495118
      mean_wQuantileLoss: 0.24161162588713214
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 0.8527929824776947
      - 0.6882381588220596
      - 0.6319642555899918
      - 0.6134765860624611
      - 0.572639940539375
      - 0.5600550891831517
      - 0.5531176165677607
      - 0.540045642759651
      - 0.5472633724566549
      - 0.5366015916224569
      - 0.5346447480842471
      - 0.5321394049096853
      - 0.5230379230342805
      - 0.5173990875482559
      - 0.5167872968595475
      - 0.5184444014448673
      - 0.515180452959612
      - 0.5108791068196297
      - 0.5094953193329275
      - 0.5078173759393394
      - 0.5046544105280191
      val_loss:
      - 0.29793098010122776
      - 0.6882381588220596
      - 0.6319642555899918
      - 0.6134765860624611
      - 0.572639940539375
      - 0.5600550891831517
      - 0.5531176165677607
      - 0.540045642759651
      - 0.5472633724566549
      - 0.5366015916224569
      - 0.5346447480842471
      - 0.5321394049096853
      - 0.5230379230342805
      - 0.5173990875482559
      - 0.5167872968595475
      - 0.5184444014448673
      - 0.515180452959612
      - 0.5108791068196297
      - 0.5094953193329275
      - 0.5078173759393394
      - 0.5046544105280191
  task_id: 3
