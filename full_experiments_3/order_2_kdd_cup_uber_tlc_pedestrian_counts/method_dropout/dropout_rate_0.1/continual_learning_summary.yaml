continual_learning_setup:
  method: dropout
  method_params:
    dropout_rate: 0.1
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
  - train_pedestrian_counts.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_dropout/dropout_rate_0.1/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.1
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.31461858378854396
      NRMSE: 0.6773000495789372
      mean_wQuantileLoss: 0.2476465435642704
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4224566253833473
      - 0.30169927526731044
      - 0.26332530623767525
      - 0.22663589660078287
      - 0.2254441698314622
      - 0.21177197730867192
      - 0.21518889477010816
      - 0.2023684051237069
      - 0.2071301059331745
      - 0.20383900083834305
      - 0.195698375988286
      - 0.1954002776183188
      - 0.19557925657136366
      - 0.20351510908221826
      - 0.1859025892917998
      - 0.18509687489131466
      - 0.18418186926282942
      - 0.1820585989044048
      - 0.17867340077646077
      - 0.1819216218427755
      - 0.18329010513843969
      - 0.18183509446680546
      - 0.18968986731488258
      - 0.17937944125151262
      - 0.17793393618194386
      - 0.1795158457243815
      - 0.17968183860648423
      - 0.17555097670992836
      - 0.17343049024930224
      - 0.17447671969421208
      - 0.17312591237714514
      - 0.16961902857292444
      - 0.17253422481007874
      - 0.17481977312127128
      - 0.17214404453989118
      - 0.1794366764370352
      - 0.17403474648017436
      - 0.17089916940312833
      - 0.17471405596006662
      - 0.1694425162859261
      - 0.17435749672586098
      - 0.1685371159692295
      - 0.16980799322482198
      - 0.16797332209534943
      - 0.17066528904251754
      - 0.16923339548520744
      - 0.16631585609866306
      - 0.17174573871307075
      - 0.17156933911610395
      - 0.1662947327713482
      - 0.17271019867621362
      - 0.16795160761103034
      - 0.16515928850276396
      - 0.1660488797351718
      - 0.16441882628714666
      - 0.16912329551996663
      - 0.17222920042695478
      - 0.16348179226042703
      - 0.1665455810725689
      - 0.16774354153312743
      - 0.16579728783108294
      - 0.1649298542761244
      - 0.164742348191794
      - 0.16704350023064762
      - 0.16642481839517131
      - 0.16312411229591817
      - 0.16230918397195637
      - 0.16351425612811
      - 0.16301654325798154
      - 0.16794789087725803
      - 0.1655441870680079
      val_loss:
      - 0.28566068708896636
      - 0.30169927526731044
      - 0.26332530623767525
      - 0.22663589660078287
      - 0.2254441698314622
      - 0.21177197730867192
      - 0.21518889477010816
      - 0.2023684051237069
      - 0.2071301059331745
      - 0.20383900083834305
      - 0.195698375988286
      - 0.1954002776183188
      - 0.19557925657136366
      - 0.20351510908221826
      - 0.1859025892917998
      - 0.18509687489131466
      - 0.18418186926282942
      - 0.1820585989044048
      - 0.17867340077646077
      - 0.1819216218427755
      - 0.18329010513843969
      - 0.18183509446680546
      - 0.18968986731488258
      - 0.17937944125151262
      - 0.17793393618194386
      - 0.1795158457243815
      - 0.17968183860648423
      - 0.17555097670992836
      - 0.17343049024930224
      - 0.17447671969421208
      - 0.17312591237714514
      - 0.16961902857292444
      - 0.17253422481007874
      - 0.17481977312127128
      - 0.17214404453989118
      - 0.1794366764370352
      - 0.17403474648017436
      - 0.17089916940312833
      - 0.17471405596006662
      - 0.1694425162859261
      - 0.17435749672586098
      - 0.1685371159692295
      - 0.16980799322482198
      - 0.16797332209534943
      - 0.17066528904251754
      - 0.16923339548520744
      - 0.16631585609866306
      - 0.17174573871307075
      - 0.17156933911610395
      - 0.1662947327713482
      - 0.1410253256559372
      - 0.16795160761103034
      - 0.16515928850276396
      - 0.1660488797351718
      - 0.16441882628714666
      - 0.16912329551996663
      - 0.17222920042695478
      - 0.16348179226042703
      - 0.1665455810725689
      - 0.16774354153312743
      - 0.16579728783108294
      - 0.1649298542761244
      - 0.164742348191794
      - 0.16704350023064762
      - 0.16642481839517131
      - 0.16312411229591817
      - 0.16230918397195637
      - 0.16351425612811
      - 0.16301654325798154
      - 0.16794789087725803
      - 0.1655441870680079
  task_id: 1
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_dropout/dropout_rate_0.1/task_2_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.1
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.2148786797240124
      NRMSE: 0.5411342362313878
      mean_wQuantileLoss: 0.17724112128514755
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5208027048502117
      - 0.3610179042443633
      - 0.3302437881939113
      - 0.32137192436493933
      - 0.3064607344567776
      - 0.30360106751322746
      - 0.29731813119724393
      - 0.2924860371276736
      - 0.2962922330480069
      - 0.2845412571914494
      - 0.2896549686556682
      - 0.28665179188828915
      - 0.2861033354420215
      - 0.27822967269457877
      - 0.277176889590919
      - 0.27893781976308674
      - 0.2766373831545934
      - 0.2737395182484761
      - 0.2671891226200387
      - 0.2733756913803518
      - 0.26942308235447854
      - 0.2683682640781626
      - 0.2660117733757943
      - 0.26614480512216687
      - 0.2668338764924556
      - 0.2624316858127713
      - 0.2629370803479105
      - 0.2621707405196503
      - 0.2680167001672089
      - 0.26388238929212093
      - 0.26063547970261425
      - 0.2601058821892366
      - 0.2616089879302308
      - 0.26203729095868766
      - 0.2598088118247688
      - 0.26558201503939927
      - 0.2609857296338305
      - 0.25722030422184616
      - 0.2584334189305082
      - 0.2634722065413371
      - 0.25488870416302234
      - 0.25662142236251384
      - 0.2498305890476331
      - 0.2583505855873227
      - 0.2564851645147428
      - 0.25458548322785646
      - 0.2519788626814261
      - 0.256482099997811
      - 0.2541288605425507
      - 0.25874456483870745
      - 0.2501561534591019
      - 0.2526922805700451
      - 0.2572598025435582
      - 0.25352187326643616
      - 0.2518232971196994
      - 0.2607615820597857
      - 0.24651079869363457
      - 0.25252595148049295
      - 0.24629519949667156
      - 0.2552590031409636
      - 0.2537649112055078
      - 0.247533789020963
      - 0.2525136972544715
      - 0.25146434060297906
      - 0.2515037776902318
      - 0.2513695827219635
      - 0.24569881183560938
      - 0.2493347761919722
      - 0.24986716802231967
      - 0.24433916038833559
      - 0.24813302094116807
      val_loss:
      - 0.3615604221820831
      - 0.3610179042443633
      - 0.3302437881939113
      - 0.32137192436493933
      - 0.3064607344567776
      - 0.30360106751322746
      - 0.29731813119724393
      - 0.2924860371276736
      - 0.2962922330480069
      - 0.2845412571914494
      - 0.2896549686556682
      - 0.28665179188828915
      - 0.2861033354420215
      - 0.27822967269457877
      - 0.277176889590919
      - 0.27893781976308674
      - 0.2766373831545934
      - 0.2737395182484761
      - 0.2671891226200387
      - 0.2733756913803518
      - 0.26942308235447854
      - 0.2683682640781626
      - 0.2660117733757943
      - 0.26614480512216687
      - 0.2668338764924556
      - 0.2624316858127713
      - 0.2629370803479105
      - 0.2621707405196503
      - 0.2680167001672089
      - 0.26388238929212093
      - 0.26063547970261425
      - 0.2601058821892366
      - 0.2616089879302308
      - 0.26203729095868766
      - 0.2598088118247688
      - 0.26558201503939927
      - 0.2609857296338305
      - 0.25722030422184616
      - 0.2584334189305082
      - 0.2634722065413371
      - 0.25488870416302234
      - 0.25662142236251384
      - 0.2498305890476331
      - 0.2583505855873227
      - 0.2564851645147428
      - 0.25458548322785646
      - 0.2519788626814261
      - 0.256482099997811
      - 0.2541288605425507
      - 0.25874456483870745
      - 0.18712340593338012
      - 0.2526922805700451
      - 0.2572598025435582
      - 0.25352187326643616
      - 0.2518232971196994
      - 0.2607615820597857
      - 0.24651079869363457
      - 0.25252595148049295
      - 0.24629519949667156
      - 0.2552590031409636
      - 0.2537649112055078
      - 0.247533789020963
      - 0.2525136972544715
      - 0.25146434060297906
      - 0.2515037776902318
      - 0.2513695827219635
      - 0.24569881183560938
      - 0.2493347761919722
      - 0.24986716802231967
      - 0.24433916038833559
      - 0.24813302094116807
  task_id: 2
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_dropout/dropout_rate_0.1/task_3_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.1
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.20246514711756736
      NRMSE: 0.6776241076006109
      mean_wQuantileLoss: 0.1654773401872672
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      train_loss:
      - 0.507135541876778
      - 0.3710340795805678
      - 0.3258548645535484
      - 0.2795054301386699
      - 0.27755977935157716
      - 0.2525639820378274
      - 0.24982012098189443
      - 0.21960938605479896
      - 0.21416093793231994
      - 0.2029745854670182
      - 0.20558782655280083
      - 0.20345678535522893
      - 0.19591724721249193
      - 0.19812804943649098
      - 0.1867515510530211
      - 0.18838897143723443
      - 0.1827999416855164
      - 0.1790594084886834
      - 0.18354667600942776
      - 0.18607054941821843
      - 0.1904849997954443
      - 0.17751060222508386
      - 0.18968722189310938
      - 0.17299196030944586
      - 0.17849165899679065
      - 0.20984535437310115
      - 0.17063734208932146
      - 0.17592300690012053
      - 0.18740724684903398
      - 0.16866984113585204
      - 0.1685094863642007
      - 0.16574250772828236
      - 0.17139333812519908
      - 0.16781798854935914
      - 0.16460336174350232
      - 0.16418216284364462
      - 0.16086611384525895
      - 0.16538717166986316
      - 0.16547818551771343
      - 0.16744981048395857
      - 0.17007346294121817
      - 0.16859665233641863
      - 0.1636294576455839
      - 0.16609697218518704
      - 0.1630899906740524
      - 0.1591328689828515
      - 0.16442797862691805
      - 0.160890203085728
      - 0.15923322323942557
      - 0.16452483518514782
      - 0.16260621964465827
      - 0.157261555257719
      - 0.16050874534994364
      - 0.16300028818659484
      - 0.1561143298749812
      - 0.15521425782935694
      - 0.16760713123949245
      - 0.15690366260241717
      - 0.16829804895678535
      - 0.15856570441974327
      - 0.18242837209254503
      - 0.15856122528202832
      - 0.15905530512100086
      - 0.15712955535855144
      - 0.1606626469292678
      - 0.1564352583955042
      - 0.15833210188429803
      - 0.1600497614708729
      - 0.159591025672853
      - 0.15787485428154469
      - 0.1565731663722545
      - 0.16344524029409513
      - 0.15570522577036172
      - 0.18228424922563136
      - 0.1559011359931901
      - 0.1584304984426126
      val_loss:
      - 0.6020396649837494
      - 0.3710340795805678
      - 0.3258548645535484
      - 0.2795054301386699
      - 0.27755977935157716
      - 0.2525639820378274
      - 0.24982012098189443
      - 0.21960938605479896
      - 0.21416093793231994
      - 0.2029745854670182
      - 0.20558782655280083
      - 0.20345678535522893
      - 0.19591724721249193
      - 0.19812804943649098
      - 0.1867515510530211
      - 0.18838897143723443
      - 0.1827999416855164
      - 0.1790594084886834
      - 0.18354667600942776
      - 0.18607054941821843
      - 0.1904849997954443
      - 0.17751060222508386
      - 0.18968722189310938
      - 0.17299196030944586
      - 0.17849165899679065
      - 0.20984535437310115
      - 0.17063734208932146
      - 0.17592300690012053
      - 0.18740724684903398
      - 0.16866984113585204
      - 0.1685094863642007
      - 0.16574250772828236
      - 0.17139333812519908
      - 0.16781798854935914
      - 0.16460336174350232
      - 0.16418216284364462
      - 0.16086611384525895
      - 0.16538717166986316
      - 0.16547818551771343
      - 0.16744981048395857
      - 0.17007346294121817
      - 0.16859665233641863
      - 0.1636294576455839
      - 0.16609697218518704
      - 0.1630899906740524
      - 0.1591328689828515
      - 0.16442797862691805
      - 0.160890203085728
      - 0.15923322323942557
      - 0.16452483518514782
      - 0.35071560740470886
      - 0.157261555257719
      - 0.16050874534994364
      - 0.16300028818659484
      - 0.1561143298749812
      - 0.15521425782935694
      - 0.16760713123949245
      - 0.15690366260241717
      - 0.16829804895678535
      - 0.15856570441974327
      - 0.18242837209254503
      - 0.15856122528202832
      - 0.15905530512100086
      - 0.15712955535855144
      - 0.1606626469292678
      - 0.1564352583955042
      - 0.15833210188429803
      - 0.1600497614708729
      - 0.159591025672853
      - 0.15787485428154469
      - 0.1565731663722545
      - 0.16344524029409513
      - 0.15570522577036172
      - 0.18228424922563136
      - 0.1559011359931901
      - 0.1584304984426126
  task_id: 3
