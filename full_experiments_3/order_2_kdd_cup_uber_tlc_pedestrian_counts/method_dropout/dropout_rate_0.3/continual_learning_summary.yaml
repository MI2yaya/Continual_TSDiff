continual_learning_setup:
  method: dropout
  method_params:
    dropout_rate: 0.3
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
  - train_pedestrian_counts.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_dropout/dropout_rate_0.3/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.3
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.38152481635721913
      NRMSE: 0.7650180567078321
      mean_wQuantileLoss: 0.3003772393309582
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.43198456708341837
      - 0.33044153719674796
      - 0.2836189860245213
      - 0.2543922901386395
      - 0.24259497446473688
      - 0.23125352780334651
      - 0.23045414790976793
      - 0.2231091932626441
      - 0.21787647914607078
      - 0.2174913571216166
      - 0.21443589695263654
      - 0.20915738411713392
      - 0.2139063649228774
      - 0.20806229033041745
      - 0.20276476460276172
      - 0.21912224765401334
      - 0.20203057990875095
      - 0.20059280830901116
      - 0.20021726214326918
      - 0.2061835000058636
      - 0.20370783359976485
      - 0.20094689133111387
      - 0.21092023904202506
      - 0.20236999256303534
      - 0.20024652231950313
      - 0.19856169336708263
      - 0.20179508277215064
      - 0.1999015600886196
      - 0.19695733522530645
      - 0.20377028972143307
      - 0.20037739863619208
      - 0.1942150570685044
      - 0.1976462044985965
      - 0.19685091509018093
      - 0.19405310123693198
      - 0.19605764362495393
      - 0.19228011486120522
      - 0.1975211481912993
      - 0.20278043416328728
      - 0.19408297433983535
      - 0.19189692847430706
      - 0.1959165369044058
      - 0.19553060573525727
      - 0.19736304570687935
      - 0.19250801572343335
      - 0.19643333472777158
      - 0.18886302661849186
      - 0.19152327085612342
      - 0.1926550327334553
      - 0.19467344938311726
      - 0.1949261671397835
      - 0.18682298948988318
      - 0.19615157932275906
      - 0.1899618218303658
      - 0.19110604678280652
      - 0.1958637658972293
      - 0.1866535234148614
      - 0.1932233552215621
      - 0.18481347139459103
      - 0.1870493545429781
      - 0.1892646040651016
      - 0.19065125798806548
      - 0.18790177337359637
      - 0.18680726294405758
      - 0.19260639359708875
      - 0.19097285368479788
      - 0.19119843596126884
      - 0.19258354173507541
      - 0.1891633405466564
      - 0.1895069342572242
      - 0.1859036354580894
      val_loss:
      - 0.34193079471588134
      - 0.33044153719674796
      - 0.2836189860245213
      - 0.2543922901386395
      - 0.24259497446473688
      - 0.23125352780334651
      - 0.23045414790976793
      - 0.2231091932626441
      - 0.21787647914607078
      - 0.2174913571216166
      - 0.21443589695263654
      - 0.20915738411713392
      - 0.2139063649228774
      - 0.20806229033041745
      - 0.20276476460276172
      - 0.21912224765401334
      - 0.20203057990875095
      - 0.20059280830901116
      - 0.20021726214326918
      - 0.2061835000058636
      - 0.20370783359976485
      - 0.20094689133111387
      - 0.21092023904202506
      - 0.20236999256303534
      - 0.20024652231950313
      - 0.19856169336708263
      - 0.20179508277215064
      - 0.1999015600886196
      - 0.19695733522530645
      - 0.20377028972143307
      - 0.20037739863619208
      - 0.1942150570685044
      - 0.1976462044985965
      - 0.19685091509018093
      - 0.19405310123693198
      - 0.19605764362495393
      - 0.19228011486120522
      - 0.1975211481912993
      - 0.20278043416328728
      - 0.19408297433983535
      - 0.19189692847430706
      - 0.1959165369044058
      - 0.19553060573525727
      - 0.19736304570687935
      - 0.19250801572343335
      - 0.19643333472777158
      - 0.18886302661849186
      - 0.19152327085612342
      - 0.1926550327334553
      - 0.19467344938311726
      - 0.16239439249038695
      - 0.18682298948988318
      - 0.19615157932275906
      - 0.1899618218303658
      - 0.19110604678280652
      - 0.1958637658972293
      - 0.1866535234148614
      - 0.1932233552215621
      - 0.18481347139459103
      - 0.1870493545429781
      - 0.1892646040651016
      - 0.19065125798806548
      - 0.18790177337359637
      - 0.18680726294405758
      - 0.19260639359708875
      - 0.19097285368479788
      - 0.19119843596126884
      - 0.19258354173507541
      - 0.1891633405466564
      - 0.1895069342572242
      - 0.1859036354580894
  task_id: 1
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_dropout/dropout_rate_0.3/task_2_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.3
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.2215947628537148
      NRMSE: 0.6217378712863999
      mean_wQuantileLoss: 0.1976915047640098
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5607886894140393
      - 0.36687116068787873
      - 0.34489938826300204
      - 0.32760006829630584
      - 0.3149988183286041
      - 0.3156054791761562
      - 0.31045125622767955
      - 0.3064036901341751
      - 0.3040561332600191
      - 0.30335647461470217
      - 0.3064756748499349
      - 0.2967974179191515
      - 0.2962600134778768
      - 0.3007056163623929
      - 0.29753158357925713
      - 0.2989765026140958
      - 0.3024600070202723
      - 0.29100900143384933
      - 0.2955014178296551
      - 0.2925785252591595
      - 0.2985642912099138
      - 0.29815919895190746
      - 0.2921956793870777
      - 0.29213367926422507
      - 0.2878856855677441
      - 0.2855528307845816
      - 0.28770046366844326
      - 0.2881312577519566
      - 0.2906649583019316
      - 0.29000123275909573
      - 0.2869846108369529
      - 0.2930887790862471
      - 0.28910345945041627
      - 0.2857048463774845
      - 0.28766093985177577
      - 0.2847962357336655
      - 0.2877223379909992
      - 0.28167136933188885
      - 0.2839355064788833
      - 0.2895814359653741
      - 0.27926317777018994
      - 0.2847722191363573
      - 0.27966620249208063
      - 0.27910144918132573
      - 0.27528085734229535
      - 0.27836963103618473
      - 0.28284781507682055
      - 0.27870018035173416
      - 0.2816830512601882
      - 0.2823767738882452
      - 0.2788504820782691
      - 0.27814487810246646
      - 0.2788334871875122
      - 0.27853141934610903
      - 0.27368632599245757
      - 0.27611218637321144
      - 0.27628192922566086
      - 0.2771374562289566
      - 0.27834458358120173
      - 0.27391064853873104
      - 0.2744176951237023
      - 0.2796527051832527
      - 0.27425825118552893
      - 0.27557057584635913
      - 0.2794242219533771
      - 0.2791364999720827
      - 0.2775835422798991
      - 0.27550728560891
      - 0.27728373312857
      - 0.27800739742815495
      - 0.2775523934978992
      val_loss:
      - 0.3071520239114761
      - 0.36687116068787873
      - 0.34489938826300204
      - 0.32760006829630584
      - 0.3149988183286041
      - 0.3156054791761562
      - 0.31045125622767955
      - 0.3064036901341751
      - 0.3040561332600191
      - 0.30335647461470217
      - 0.3064756748499349
      - 0.2967974179191515
      - 0.2962600134778768
      - 0.3007056163623929
      - 0.29753158357925713
      - 0.2989765026140958
      - 0.3024600070202723
      - 0.29100900143384933
      - 0.2955014178296551
      - 0.2925785252591595
      - 0.2985642912099138
      - 0.29815919895190746
      - 0.2921956793870777
      - 0.29213367926422507
      - 0.2878856855677441
      - 0.2855528307845816
      - 0.28770046366844326
      - 0.2881312577519566
      - 0.2906649583019316
      - 0.29000123275909573
      - 0.2869846108369529
      - 0.2930887790862471
      - 0.28910345945041627
      - 0.2857048463774845
      - 0.28766093985177577
      - 0.2847962357336655
      - 0.2877223379909992
      - 0.28167136933188885
      - 0.2839355064788833
      - 0.2895814359653741
      - 0.27926317777018994
      - 0.2847722191363573
      - 0.27966620249208063
      - 0.27910144918132573
      - 0.27528085734229535
      - 0.27836963103618473
      - 0.28284781507682055
      - 0.27870018035173416
      - 0.2816830512601882
      - 0.2823767738882452
      - 0.25426051020622253
      - 0.27814487810246646
      - 0.2788334871875122
      - 0.27853141934610903
      - 0.27368632599245757
      - 0.27611218637321144
      - 0.27628192922566086
      - 0.2771374562289566
      - 0.27834458358120173
      - 0.27391064853873104
      - 0.2744176951237023
      - 0.2796527051832527
      - 0.27425825118552893
      - 0.27557057584635913
      - 0.2794242219533771
      - 0.2791364999720827
      - 0.2775835422798991
      - 0.27550728560891
      - 0.27728373312857
      - 0.27800739742815495
      - 0.2775523934978992
  task_id: 2
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_dropout/dropout_rate_0.3/task_3_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.3
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.20055328872220202
      NRMSE: 0.7167850112375377
      mean_wQuantileLoss: 0.1682897086289476
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.46847899025306106
      - 0.3627343410626054
      - 0.3302113809622824
      - 0.3579360353760421
      - 0.31617988739162683
      - 0.29548598639667034
      - 0.2641165240202099
      - 0.2654009935213253
      - 0.2445691868197173
      - 0.24197636602912098
      - 0.23032816301565617
      - 0.22479035786818713
      - 0.21840625174809247
      - 0.2360103753162548
      - 0.2173773772083223
      - 0.2106668477645144
      - 0.20784563105553389
      - 0.2068986568483524
      - 0.20372878888156265
      - 0.20098409208003432
      - 0.2034506315831095
      - 0.1944832494482398
      - 0.19724283023970202
      - 0.19939242448890582
      - 0.19888247130438685
      - 0.21344414178747684
      - 0.19737951620481908
      - 0.20011306105880067
      - 0.20033094543032348
      - 0.19301783514674753
      - 0.20615206891670823
      - 0.19068759633228183
      - 0.19447672634851187
      - 0.18922440498135984
      - 0.19231399009004235
      - 0.19390254188328981
      - 0.20285123639041558
      - 0.1919486903352663
      - 0.21898162353318185
      - 0.187221885076724
      - 0.19012046960415319
      - 0.18960984982550144
      - 0.20641099812928587
      - 0.1872820210410282
      - 0.18528769083786756
      - 0.185387080127839
      - 0.18522647442296147
      - 0.18797122762771323
      - 0.18412850995082408
      - 0.18637834466062486
      - 0.18122211372246966
      - 0.1872161549399607
      - 0.18434931087540463
      - 0.20852566964458674
      - 0.18351630738470703
      - 0.18451257410924882
      - 0.19034182641189545
      - 0.18453563685761765
      - 0.18266682722605765
      - 0.18894294468918815
      - 0.18979440460680053
      - 0.1794802169315517
      - 0.19005988677963614
      - 0.18337520607747138
      - 0.18454234721139073
      - 0.18277238588780165
      - 0.18355506850639358
      - 0.17670474626356736
      - 0.1818736889399588
      - 0.17931963072624058
      - 0.17795625777216628
      val_loss:
      - 0.5945555716753006
      - 0.3627343410626054
      - 0.3302113809622824
      - 0.3579360353760421
      - 0.31617988739162683
      - 0.29548598639667034
      - 0.2641165240202099
      - 0.2654009935213253
      - 0.2445691868197173
      - 0.24197636602912098
      - 0.23032816301565617
      - 0.22479035786818713
      - 0.21840625174809247
      - 0.2360103753162548
      - 0.2173773772083223
      - 0.2106668477645144
      - 0.20784563105553389
      - 0.2068986568483524
      - 0.20372878888156265
      - 0.20098409208003432
      - 0.2034506315831095
      - 0.1944832494482398
      - 0.19724283023970202
      - 0.19939242448890582
      - 0.19888247130438685
      - 0.21344414178747684
      - 0.19737951620481908
      - 0.20011306105880067
      - 0.20033094543032348
      - 0.19301783514674753
      - 0.20615206891670823
      - 0.19068759633228183
      - 0.19447672634851187
      - 0.18922440498135984
      - 0.19231399009004235
      - 0.19390254188328981
      - 0.20285123639041558
      - 0.1919486903352663
      - 0.21898162353318185
      - 0.187221885076724
      - 0.19012046960415319
      - 0.18960984982550144
      - 0.20641099812928587
      - 0.1872820210410282
      - 0.18528769083786756
      - 0.185387080127839
      - 0.18522647442296147
      - 0.18797122762771323
      - 0.18412850995082408
      - 0.18637834466062486
      - 0.11557467095553875
      - 0.1872161549399607
      - 0.18434931087540463
      - 0.20852566964458674
      - 0.18351630738470703
      - 0.18451257410924882
      - 0.19034182641189545
      - 0.18453563685761765
      - 0.18266682722605765
      - 0.18894294468918815
      - 0.18979440460680053
      - 0.1794802169315517
      - 0.19005988677963614
      - 0.18337520607747138
      - 0.18454234721139073
      - 0.18277238588780165
      - 0.18355506850639358
      - 0.17670474626356736
      - 0.1818736889399588
      - 0.17931963072624058
      - 0.17795625777216628
  task_id: 3
