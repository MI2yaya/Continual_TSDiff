continual_learning_setup:
  method: score_l1
  method_params:
    lambda_reg: 2.0
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
  - train_pedestrian_counts.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l1/lambda_reg_2.0/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 2.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3531538034544173
      NRMSE: 0.7452945370537251
      mean_wQuantileLoss: 0.27903500005975285
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4182397050317377
      - 0.28309527167584747
      - 0.23795222083572298
      - 0.20846298441756517
      - 0.20178669306915253
      - 0.19308178400387987
      - 0.1914889964973554
      - 0.18680189060978591
      - 0.18125393678201362
      - 0.18106728902785107
      - 0.18590960965957493
      - 0.18728005030425265
      - 0.17931135062826797
      - 0.1771527740638703
      - 0.1786145424703136
      - 0.17780921241501346
      - 0.17777937918435782
      - 0.17600502603454515
      - 0.16686021315399557
      - 0.1710944032529369
      - 0.1672446290613152
      - 0.17200306965969503
      - 0.17015108175110072
      - 0.17026499297935516
      - 0.16730727517278865
      - 0.1709670937852934
      - 0.16947281028842553
      - 0.17266979557462037
      - 0.17139769432833418
      - 0.1627497150329873
      - 0.16430992778623477
      - 0.16263420408358797
      - 0.1691038372227922
      - 0.16355695336824283
      - 0.16503674071282148
      - 0.16349088895367458
      - 0.16085343714803457
      - 0.16207469225628302
      - 0.1672058113035746
      - 0.1626715767197311
      - 0.16074550413759425
      - 0.15624013205524534
      - 0.15985292167169973
      - 0.16538891720119864
      - 0.16087173420237377
      - 0.16225191263947636
      - 0.16263913491275162
      - 0.15936438483186066
      - 0.16410120617365465
      - 0.16451395209878683
      - 0.1569622009410523
      - 0.15775132464477792
      - 0.15944075788138434
      - 0.15655733464518562
      - 0.16055188904283568
      - 0.15958131611114368
      - 0.16442233964335173
      - 0.15718290186487138
      - 0.15840443433262408
      - 0.15405244391877204
      - 0.15986635320587084
      - 0.16027483571087942
      - 0.15349018690176308
      - 0.15830589196411893
      - 0.16071893268963322
      - 0.1541602833312936
      - 0.16102989541832358
      - 0.15632311830995604
      - 0.15613231132738292
      - 0.15227197436615825
      - 0.15988049475708976
      val_loss:
      - 0.24965980648994446
      - 0.28309527167584747
      - 0.23795222083572298
      - 0.20846298441756517
      - 0.20178669306915253
      - 0.19308178400387987
      - 0.1914889964973554
      - 0.18680189060978591
      - 0.18125393678201362
      - 0.18106728902785107
      - 0.18590960965957493
      - 0.18728005030425265
      - 0.17931135062826797
      - 0.1771527740638703
      - 0.1786145424703136
      - 0.17780921241501346
      - 0.17777937918435782
      - 0.17600502603454515
      - 0.16686021315399557
      - 0.1710944032529369
      - 0.1672446290613152
      - 0.17200306965969503
      - 0.17015108175110072
      - 0.17026499297935516
      - 0.16730727517278865
      - 0.1709670937852934
      - 0.16947281028842553
      - 0.17266979557462037
      - 0.17139769432833418
      - 0.1627497150329873
      - 0.16430992778623477
      - 0.16263420408358797
      - 0.1691038372227922
      - 0.16355695336824283
      - 0.16503674071282148
      - 0.16349088895367458
      - 0.16085343714803457
      - 0.16207469225628302
      - 0.1672058113035746
      - 0.1626715767197311
      - 0.16074550413759425
      - 0.15624013205524534
      - 0.15985292167169973
      - 0.16538891720119864
      - 0.16087173420237377
      - 0.16225191263947636
      - 0.16263913491275162
      - 0.15936438483186066
      - 0.16410120617365465
      - 0.16451395209878683
      - 0.13233027160167693
      - 0.15775132464477792
      - 0.15944075788138434
      - 0.15655733464518562
      - 0.16055188904283568
      - 0.15958131611114368
      - 0.16442233964335173
      - 0.15718290186487138
      - 0.15840443433262408
      - 0.15405244391877204
      - 0.15986635320587084
      - 0.16027483571087942
      - 0.15349018690176308
      - 0.15830589196411893
      - 0.16071893268963322
      - 0.1541602833312936
      - 0.16102989541832358
      - 0.15632311830995604
      - 0.15613231132738292
      - 0.15227197436615825
      - 0.15988049475708976
  task_id: 1
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l1/lambda_reg_2.0/task_2_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 2.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 1.6230701063473096
      NRMSE: 5.7088676492161134
      mean_wQuantileLoss: 1.9656370104925902
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 1.6607700223103166
      - 1.5612909542396665
      - 1.530383661389351
      - 1.5016418937593699
      - 1.4898304818198085
      - 1.489637286402285
      - 1.4840776547789574
      - 1.4786960082128644
      - 1.4654492549598217
      - 1.4680844834074378
      - 1.4547846028581262
      - 1.457679665647447
      - 1.4428596934303641
      - 1.4440040588378906
      - 1.4481371212750673
      - 1.4483030205592513
      - 1.438231754116714
      - 1.4433871395885944
      - 1.4329055324196815
      - 1.4315236052498221
      - 1.423952148295939
      val_loss:
      - 0.7965518832206726
      - 1.5612909542396665
      - 1.530383661389351
      - 1.5016418937593699
      - 1.4898304818198085
      - 1.489637286402285
      - 1.4840776547789574
      - 1.4786960082128644
      - 1.4654492549598217
      - 1.4680844834074378
      - 1.4547846028581262
      - 1.457679665647447
      - 1.4428596934303641
      - 1.4440040588378906
      - 1.4481371212750673
      - 1.4483030205592513
      - 1.438231754116714
      - 1.4433871395885944
      - 1.4329055324196815
      - 1.4315236052498221
      - 1.423952148295939
  task_id: 2
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l1/lambda_reg_2.0/task_3_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 2.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.6711616800772789
      NRMSE: 1.7782322142529268
      mean_wQuantileLoss: 0.5248977046821164
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 0.9711448913440108
      - 0.773851444479078
      - 0.7624981668777764
      - 0.698893173597753
      - 0.7047970192506909
      - 0.6999724111519754
      - 0.701557710301131
      - 0.6730107716284692
      - 0.6716845307964832
      - 0.7039940075483173
      - 0.6566351787187159
      - 0.6515886352863163
      - 0.6622890373691916
      - 0.6494158026762307
      - 0.6455559795722365
      - 0.6465808164793998
      - 0.6493560839444399
      - 0.6409551682882011
      - 0.6278293011710048
      - 0.675651295343414
      - 0.6217874137219042
      val_loss:
      - 0.6036378443241119
      - 0.773851444479078
      - 0.7624981668777764
      - 0.698893173597753
      - 0.7047970192506909
      - 0.6999724111519754
      - 0.701557710301131
      - 0.6730107716284692
      - 0.6716845307964832
      - 0.7039940075483173
      - 0.6566351787187159
      - 0.6515886352863163
      - 0.6622890373691916
      - 0.6494158026762307
      - 0.6455559795722365
      - 0.6465808164793998
      - 0.6493560839444399
      - 0.6409551682882011
      - 0.6278293011710048
      - 0.675651295343414
      - 0.6217874137219042
  task_id: 3
