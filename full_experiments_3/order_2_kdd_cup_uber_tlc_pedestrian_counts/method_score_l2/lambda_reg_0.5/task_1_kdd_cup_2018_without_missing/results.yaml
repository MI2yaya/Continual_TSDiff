best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l2/lambda_reg_0.5/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
config:
  batch_size: 64
  context_length: 312
  dataset: kdd_cup_2018_without_missing
  device: cuda:1
  diffusion_config: diffusion_small_config
  dropout_rate: 0.0
  eval_every: 50
  freq: H
  gradient_clip_val: 0.5
  init_skip: true
  lambda_reg: 0.5
  lr: 0.001
  max_epochs: 1000
  model: unconditional
  normalization: mean
  num_batches_per_epoch: 128
  num_samples: 16
  prediction_length: 24
  sampler: ddpm
  sampler_params:
    guidance: quantile
    scale: 1
  score_loss_type: l2
  setup: forecasting
  use_features: false
  use_lags: true
  use_validation_set: true
metrics:
- ND: 0.3912004987316446
  NRMSE: 0.7774263880021617
  mean_wQuantileLoss: 0.29754406575609565
  missing_scenario: none
  missing_values: 0
training_history:
  epochs:
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  - 20
  - 21
  - 22
  - 23
  - 24
  - 25
  - 26
  - 27
  - 28
  - 29
  - 30
  - 31
  - 32
  - 33
  - 34
  - 35
  - 36
  - 37
  - 38
  - 39
  - 40
  - 41
  - 42
  - 43
  - 44
  - 45
  - 46
  - 47
  - 48
  - 49
  - 50
  - 51
  - 52
  - 53
  - 54
  - 55
  - 56
  - 57
  - 58
  - 59
  - 60
  - 61
  - 62
  - 63
  - 64
  - 65
  - 66
  - 67
  - 68
  - 69
  - 70
  - 71
  train_loss:
  - 0.4280361538985744
  - 0.28391120361629874
  - 0.2423270115396008
  - 0.2083457165863365
  - 0.20927821251098067
  - 0.20176070043817163
  - 0.19165034359320998
  - 0.19180097640492022
  - 0.18660879897652194
  - 0.18124788830755278
  - 0.17885741614736617
  - 0.1782053512870334
  - 0.1768820978468284
  - 0.17677831201581284
  - 0.1734731974429451
  - 0.18349962768843397
  - 0.17673284362535924
  - 0.17956552788382396
  - 0.1739857184002176
  - 0.17672913620481268
  - 0.17351514199981466
  - 0.17144885554444045
  - 0.1713966192328371
  - 0.17057368432870135
  - 0.16891000285977498
  - 0.17109000805066898
  - 0.1704798226710409
  - 0.16597809334052727
  - 0.1652154975454323
  - 0.1684174414840527
  - 0.16811600647633895
  - 0.16863951383857056
  - 0.16474481055047363
  - 0.16823014750843868
  - 0.16804955899715424
  - 0.16430263698566705
  - 0.16851679293904454
  - 0.1677783693303354
  - 0.15855577791808173
  - 0.1614914411911741
  - 0.16650803503580391
  - 0.15917215863009915
  - 0.15893435559701174
  - 0.16108978481497616
  - 0.16624581802170724
  - 0.1636455402476713
  - 0.16277237585745752
  - 0.161650893103797
  - 0.16177321603754535
  - 0.16049164632568136
  - 0.15892936353338882
  - 0.16536471142899245
  - 0.16051639837678522
  - 0.1632599889417179
  - 0.16207558737369254
  - 0.16261932119959965
  - 0.15928595542209223
  - 0.15822122676763684
  - 0.16045501740882173
  - 0.1613001208170317
  - 0.16282638412667438
  - 0.1561575253144838
  - 0.16349917865591124
  - 0.1540385540574789
  - 0.161450844083447
  - 0.15820199978770688
  - 0.15685528301401064
  - 0.16245968703879043
  - 0.1513674622401595
  - 0.15866934921359643
  - 0.15515394124668092
  val_loss:
  - 0.30284228920936584
  - 0.28391120361629874
  - 0.2423270115396008
  - 0.2083457165863365
  - 0.20927821251098067
  - 0.20176070043817163
  - 0.19165034359320998
  - 0.19180097640492022
  - 0.18660879897652194
  - 0.18124788830755278
  - 0.17885741614736617
  - 0.1782053512870334
  - 0.1768820978468284
  - 0.17677831201581284
  - 0.1734731974429451
  - 0.18349962768843397
  - 0.17673284362535924
  - 0.17956552788382396
  - 0.1739857184002176
  - 0.17672913620481268
  - 0.17351514199981466
  - 0.17144885554444045
  - 0.1713966192328371
  - 0.17057368432870135
  - 0.16891000285977498
  - 0.17109000805066898
  - 0.1704798226710409
  - 0.16597809334052727
  - 0.1652154975454323
  - 0.1684174414840527
  - 0.16811600647633895
  - 0.16863951383857056
  - 0.16474481055047363
  - 0.16823014750843868
  - 0.16804955899715424
  - 0.16430263698566705
  - 0.16851679293904454
  - 0.1677783693303354
  - 0.15855577791808173
  - 0.1614914411911741
  - 0.16650803503580391
  - 0.15917215863009915
  - 0.15893435559701174
  - 0.16108978481497616
  - 0.16624581802170724
  - 0.1636455402476713
  - 0.16277237585745752
  - 0.161650893103797
  - 0.16177321603754535
  - 0.16049164632568136
  - 0.1370561018586159
  - 0.16536471142899245
  - 0.16051639837678522
  - 0.1632599889417179
  - 0.16207558737369254
  - 0.16261932119959965
  - 0.15928595542209223
  - 0.15822122676763684
  - 0.16045501740882173
  - 0.1613001208170317
  - 0.16282638412667438
  - 0.1561575253144838
  - 0.16349917865591124
  - 0.1540385540574789
  - 0.161450844083447
  - 0.15820199978770688
  - 0.15685528301401064
  - 0.16245968703879043
  - 0.1513674622401595
  - 0.15866934921359643
  - 0.15515394124668092
