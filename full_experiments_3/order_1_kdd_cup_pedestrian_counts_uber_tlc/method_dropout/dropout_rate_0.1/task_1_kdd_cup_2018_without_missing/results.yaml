best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.1/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
config:
  batch_size: 64
  context_length: 312
  dataset: kdd_cup_2018_without_missing
  device: cuda:1
  diffusion_config: diffusion_small_config
  dropout_rate: 0.1
  eval_every: 50
  freq: H
  gradient_clip_val: 0.5
  init_skip: true
  lambda_reg: 0.0
  lr: 0.001
  max_epochs: 1000
  model: unconditional
  normalization: mean
  num_batches_per_epoch: 128
  num_samples: 16
  prediction_length: 24
  sampler: ddpm
  sampler_params:
    guidance: quantile
    scale: 1
  score_loss_type: l2
  setup: forecasting
  use_features: false
  use_lags: true
  use_validation_set: true
metrics:
- ND: 0.3300841182779551
  NRMSE: 0.6852521557099097
  mean_wQuantileLoss: 0.25745787782801965
  missing_scenario: none
  missing_values: 0
training_history:
  epochs:
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  - 20
  - 21
  - 22
  - 23
  - 24
  - 25
  - 26
  - 27
  - 28
  - 29
  - 30
  - 31
  - 32
  - 33
  - 34
  - 35
  - 36
  - 37
  - 38
  - 39
  - 40
  - 41
  - 42
  - 43
  - 44
  - 45
  - 46
  - 47
  - 48
  - 49
  - 50
  - 51
  - 52
  - 53
  - 54
  - 55
  - 56
  - 57
  - 58
  - 59
  - 60
  - 61
  - 62
  - 63
  - 64
  - 65
  - 66
  - 67
  - 68
  - 69
  - 70
  - 71
  train_loss:
  - 0.4287857364397496
  - 0.2981514282291755
  - 0.2678352314978838
  - 0.24824874789919704
  - 0.22523928235750645
  - 0.21725663030520082
  - 0.2130302080186084
  - 0.2084110361756757
  - 0.20217535831034184
  - 0.20251816883683205
  - 0.19349398801568896
  - 0.19347799447132275
  - 0.19485575472936034
  - 0.1895739254541695
  - 0.18746161059243605
  - 0.1856470945640467
  - 0.18392612907337025
  - 0.18925318971741945
  - 0.18704644491663203
  - 0.18301175633678213
  - 0.17829437518958002
  - 0.18421930586919188
  - 0.17711809714091942
  - 0.18073517398443073
  - 0.17974191845860332
  - 0.18009464582428336
  - 0.17764558078488335
  - 0.17419759882614017
  - 0.180448051658459
  - 0.17491315730148926
  - 0.1711892919265665
  - 0.1796481286874041
  - 0.17847180110402405
  - 0.18083220365224406
  - 0.17017615516670048
  - 0.1750664901919663
  - 0.17726597748696804
  - 0.17429109086515382
  - 0.175874165201094
  - 0.17439826996997
  - 0.16953203931916505
  - 0.17577694484498352
  - 0.1715458445250988
  - 0.1717302558827214
  - 0.16978566983016208
  - 0.1751915678032674
  - 0.17499049252364784
  - 0.1676499039749615
  - 0.16971689398633316
  - 0.17228919675108045
  - 0.17136646033031866
  - 0.17199417646043003
  - 0.17286613903706893
  - 0.16818194510415196
  - 0.16902324015973136
  - 0.1721236122539267
  - 0.16935040953103453
  - 0.16814629541477188
  - 0.16810284677194431
  - 0.1695089390850626
  - 0.16976536303991452
  - 0.15984802861930802
  - 0.1643809505039826
  - 0.16710058209719136
  - 0.17291110922815278
  - 0.16605716513004154
  - 0.1671190759516321
  - 0.16462977731134742
  - 0.16866428719367832
  - 0.16716125066159293
  - 0.16955155553296208
  val_loss:
  - 0.29132863879203796
  - 0.2981514282291755
  - 0.2678352314978838
  - 0.24824874789919704
  - 0.22523928235750645
  - 0.21725663030520082
  - 0.2130302080186084
  - 0.2084110361756757
  - 0.20217535831034184
  - 0.20251816883683205
  - 0.19349398801568896
  - 0.19347799447132275
  - 0.19485575472936034
  - 0.1895739254541695
  - 0.18746161059243605
  - 0.1856470945640467
  - 0.18392612907337025
  - 0.18925318971741945
  - 0.18704644491663203
  - 0.18301175633678213
  - 0.17829437518958002
  - 0.18421930586919188
  - 0.17711809714091942
  - 0.18073517398443073
  - 0.17974191845860332
  - 0.18009464582428336
  - 0.17764558078488335
  - 0.17419759882614017
  - 0.180448051658459
  - 0.17491315730148926
  - 0.1711892919265665
  - 0.1796481286874041
  - 0.17847180110402405
  - 0.18083220365224406
  - 0.17017615516670048
  - 0.1750664901919663
  - 0.17726597748696804
  - 0.17429109086515382
  - 0.175874165201094
  - 0.17439826996997
  - 0.16953203931916505
  - 0.17577694484498352
  - 0.1715458445250988
  - 0.1717302558827214
  - 0.16978566983016208
  - 0.1751915678032674
  - 0.17499049252364784
  - 0.1676499039749615
  - 0.16971689398633316
  - 0.17228919675108045
  - 0.14441848993301393
  - 0.17199417646043003
  - 0.17286613903706893
  - 0.16818194510415196
  - 0.16902324015973136
  - 0.1721236122539267
  - 0.16935040953103453
  - 0.16814629541477188
  - 0.16810284677194431
  - 0.1695089390850626
  - 0.16976536303991452
  - 0.15984802861930802
  - 0.1643809505039826
  - 0.16710058209719136
  - 0.17291110922815278
  - 0.16605716513004154
  - 0.1671190759516321
  - 0.16462977731134742
  - 0.16866428719367832
  - 0.16716125066159293
  - 0.16955155553296208
