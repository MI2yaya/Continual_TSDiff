best_checkpoint: full_experiments/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_naive/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
config:
  batch_size: 64
  context_length: 312
  dataset: kdd_cup_2018_without_missing
  device: cuda:1
  diffusion_config: diffusion_small_config
  dropout_rate: 0.0
  eval_every: 50
  freq: H
  gradient_clip_val: 0.5
  init_skip: true
  l1_weight: 0.0
  l2_weight: 0.0
  lambda_reg: 0.0
  lr: 0.001
  max_epochs: 1000
  model: unconditional
  normalization: mean
  num_batches_per_epoch: 128
  num_samples: 16
  prediction_length: 24
  sampler: ddpm
  sampler_params:
    guidance: quantile
    scale: 1
  setup: forecasting
  use_features: false
  use_lags: true
  use_validation_set: true
metrics:
- ND: 0.40438318020818503
  NRMSE: 0.7789858433380953
  mean_wQuantileLoss: 0.3017952158786089
  missing_scenario: none
  missing_values: 0
training_history:
  epochs:
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  - 20
  - 21
  - 22
  - 23
  - 24
  - 25
  - 26
  - 27
  - 28
  - 29
  - 30
  - 31
  - 32
  - 33
  - 34
  - 35
  - 36
  - 37
  - 38
  - 39
  - 40
  - 41
  - 42
  - 43
  - 44
  - 45
  - 46
  - 47
  - 48
  - 49
  - 50
  - 51
  - 52
  - 53
  - 54
  - 55
  - 56
  - 57
  - 58
  - 59
  - 60
  - 61
  - 62
  - 63
  - 64
  - 65
  - 66
  - 67
  - 68
  - 69
  - 70
  - 71
  train_loss:
  - 0.4055209808284417
  - 0.2905929525149986
  - 0.23444822640158236
  - 0.2113470986369066
  - 0.21472568321041763
  - 0.2026538415811956
  - 0.19847334502264857
  - 0.19859629857819527
  - 0.1828829767764546
  - 0.18473711662227288
  - 0.1866632552118972
  - 0.18382324476260692
  - 0.17806040064897388
  - 0.18283973832149059
  - 0.1754070426686667
  - 0.1792277317144908
  - 0.17877733299974352
  - 0.1752140667522326
  - 0.17479409306542948
  - 0.1729930571746081
  - 0.1799824681947939
  - 0.16725335089722648
  - 0.17227727925637737
  - 0.16785551101202145
  - 0.16713320795679465
  - 0.1709399688988924
  - 0.17090637038927525
  - 0.1688230392173864
  - 0.17412847181549296
  - 0.1666945035685785
  - 0.16291990166064352
  - 0.16778292320668697
  - 0.16779793397290632
  - 0.16384866466978565
  - 0.16171300265705213
  - 0.1672949017956853
  - 0.1636042066384107
  - 0.16115285229170695
  - 0.16701201582327485
  - 0.1618104765075259
  - 0.16375944222090766
  - 0.16393597627757117
  - 0.1598336910828948
  - 0.1571905515738763
  - 0.1676472135586664
  - 0.1624753555515781
  - 0.16929998103296384
  - 0.15987327112816274
  - 0.15606488421326503
  - 0.16625816462328658
  - 0.1616289001540281
  - 0.1605634976294823
  - 0.1581922861514613
  - 0.15898578613996506
  - 0.1590671991580166
  - 0.15707087493501604
  - 0.15734463522676378
  - 0.16009680146817118
  - 0.16167807078454643
  - 0.15583656926173717
  - 0.16084595723077655
  - 0.1554811975802295
  - 0.15909448300953954
  - 0.15760965587105602
  - 0.1624135267920792
  - 0.15808132343227044
  - 0.15741054096724838
  - 0.16157232347177342
  - 0.16066562302876264
  - 0.1595865394338034
  - 0.15824150596745312
  val_loss:
  - 0.27316126227378845
  - 0.2905929525149986
  - 0.23444822640158236
  - 0.2113470986369066
  - 0.21472568321041763
  - 0.2026538415811956
  - 0.19847334502264857
  - 0.19859629857819527
  - 0.1828829767764546
  - 0.18473711662227288
  - 0.1866632552118972
  - 0.18382324476260692
  - 0.17806040064897388
  - 0.18283973832149059
  - 0.1754070426686667
  - 0.1792277317144908
  - 0.17877733299974352
  - 0.1752140667522326
  - 0.17479409306542948
  - 0.1729930571746081
  - 0.1799824681947939
  - 0.16725335089722648
  - 0.17227727925637737
  - 0.16785551101202145
  - 0.16713320795679465
  - 0.1709399688988924
  - 0.17090637038927525
  - 0.1688230392173864
  - 0.17412847181549296
  - 0.1666945035685785
  - 0.16291990166064352
  - 0.16778292320668697
  - 0.16779793397290632
  - 0.16384866466978565
  - 0.16171300265705213
  - 0.1672949017956853
  - 0.1636042066384107
  - 0.16115285229170695
  - 0.16701201582327485
  - 0.1618104765075259
  - 0.16375944222090766
  - 0.16393597627757117
  - 0.1598336910828948
  - 0.1571905515738763
  - 0.1676472135586664
  - 0.1624753555515781
  - 0.16929998103296384
  - 0.15987327112816274
  - 0.15606488421326503
  - 0.16625816462328658
  - 0.14140390008687972
  - 0.1605634976294823
  - 0.1581922861514613
  - 0.15898578613996506
  - 0.1590671991580166
  - 0.15707087493501604
  - 0.15734463522676378
  - 0.16009680146817118
  - 0.16167807078454643
  - 0.15583656926173717
  - 0.16084595723077655
  - 0.1554811975802295
  - 0.15909448300953954
  - 0.15760965587105602
  - 0.1624135267920792
  - 0.15808132343227044
  - 0.15741054096724838
  - 0.16157232347177342
  - 0.16066562302876264
  - 0.1595865394338034
  - 0.15824150596745312
