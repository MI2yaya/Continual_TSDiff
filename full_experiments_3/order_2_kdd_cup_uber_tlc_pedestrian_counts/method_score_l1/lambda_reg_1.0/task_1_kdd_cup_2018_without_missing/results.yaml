best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l1/lambda_reg_1.0/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
config:
  batch_size: 64
  context_length: 312
  dataset: kdd_cup_2018_without_missing
  device: cuda:1
  diffusion_config: diffusion_small_config
  dropout_rate: 0.0
  eval_every: 50
  freq: H
  gradient_clip_val: 0.5
  init_skip: true
  lambda_reg: 1.0
  lr: 0.001
  max_epochs: 1000
  model: unconditional
  normalization: mean
  num_batches_per_epoch: 128
  num_samples: 16
  prediction_length: 24
  sampler: ddpm
  sampler_params:
    guidance: quantile
    scale: 1
  score_loss_type: l1
  setup: forecasting
  use_features: false
  use_lags: true
  use_validation_set: true
metrics:
- ND: 0.43922404928925185
  NRMSE: 0.8134246231181262
  mean_wQuantileLoss: 0.33521663772045407
  missing_scenario: none
  missing_values: 0
training_history:
  epochs:
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  - 20
  - 21
  - 22
  - 23
  - 24
  - 25
  - 26
  - 27
  - 28
  - 29
  - 30
  - 31
  - 32
  - 33
  - 34
  - 35
  - 36
  - 37
  - 38
  - 39
  - 40
  - 41
  - 42
  - 43
  - 44
  - 45
  - 46
  - 47
  - 48
  - 49
  - 50
  - 51
  - 52
  - 53
  - 54
  - 55
  - 56
  - 57
  - 58
  - 59
  - 60
  - 61
  - 62
  - 63
  - 64
  - 65
  - 66
  - 67
  - 68
  - 69
  - 70
  - 71
  - 72
  - 73
  - 74
  - 75
  - 76
  train_loss:
  - 0.3974357070401311
  - 0.27631965884938836
  - 0.24999526469036937
  - 0.21503499860409647
  - 0.2099426886998117
  - 0.20524597563780844
  - 0.19825596327427775
  - 0.1992253724602051
  - 0.19064683449687436
  - 0.18223202356602997
  - 0.1888882332132198
  - 0.1782034649513662
  - 0.18062626186292619
  - 0.18092247640015557
  - 0.1805898552411236
  - 0.1772132752230391
  - 0.1764809694723226
  - 0.1696835614857264
  - 0.1699307746021077
  - 0.1763406494865194
  - 0.16792575764702633
  - 0.1733401147648692
  - 0.17234791204100475
  - 0.16474802256561816
  - 0.17140093119814992
  - 0.17217191914096475
  - 0.16228854085784405
  - 0.1724791785236448
  - 0.17325516761047766
  - 0.16659810964483768
  - 0.16557579330401495
  - 0.16059386916458607
  - 0.16956443479284644
  - 0.16978487104643136
  - 0.1624477962614037
  - 0.16529753821669146
  - 0.16965108504518867
  - 0.1656893262406811
  - 0.16609047580277547
  - 0.1681170371011831
  - 0.1627673048642464
  - 0.1633234239416197
  - 0.16333241132088006
  - 0.16290557850152254
  - 0.160858616232872
  - 0.15849346679169685
  - 0.1622501329984516
  - 0.15983117563882843
  - 0.1628072462626733
  - 0.16292837302898988
  - 0.15739041042979807
  - 0.15929577982751653
  - 0.16883599886205047
  - 0.16192853421671316
  - 0.1613093065097928
  - 0.1528754752362147
  - 0.16138754592975602
  - 0.15770165860885754
  - 0.15430774859851226
  - 0.15722594951512292
  - 0.15495234675472602
  - 0.16114773438312113
  - 0.1541282896650955
  - 0.16063767654122785
  - 0.1560482537606731
  - 0.1604362775105983
  - 0.15624335198663175
  - 0.16161451791413128
  - 0.15790895686950535
  - 0.15570061834296212
  - 0.16010837681824341
  - 0.15979021054226905
  - 0.1607857079943642
  - 0.15668989066034555
  - 0.15806074830470607
  - 0.1567972864722833
  val_loss:
  - 0.2623425334692001
  - 0.27631965884938836
  - 0.24999526469036937
  - 0.21503499860409647
  - 0.2099426886998117
  - 0.20524597563780844
  - 0.19825596327427775
  - 0.1992253724602051
  - 0.19064683449687436
  - 0.18223202356602997
  - 0.1888882332132198
  - 0.1782034649513662
  - 0.18062626186292619
  - 0.18092247640015557
  - 0.1805898552411236
  - 0.1772132752230391
  - 0.1764809694723226
  - 0.1696835614857264
  - 0.1699307746021077
  - 0.1763406494865194
  - 0.16792575764702633
  - 0.1733401147648692
  - 0.17234791204100475
  - 0.16474802256561816
  - 0.17140093119814992
  - 0.17217191914096475
  - 0.16228854085784405
  - 0.1724791785236448
  - 0.17325516761047766
  - 0.16659810964483768
  - 0.16557579330401495
  - 0.16059386916458607
  - 0.16956443479284644
  - 0.16978487104643136
  - 0.1624477962614037
  - 0.16529753821669146
  - 0.16965108504518867
  - 0.1656893262406811
  - 0.16609047580277547
  - 0.1681170371011831
  - 0.1627673048642464
  - 0.1633234239416197
  - 0.16333241132088006
  - 0.16290557850152254
  - 0.160858616232872
  - 0.15849346679169685
  - 0.1622501329984516
  - 0.15983117563882843
  - 0.1628072462626733
  - 0.16292837302898988
  - 0.15675411224365235
  - 0.15929577982751653
  - 0.16883599886205047
  - 0.16192853421671316
  - 0.1613093065097928
  - 0.1528754752362147
  - 0.16138754592975602
  - 0.15770165860885754
  - 0.15430774859851226
  - 0.15722594951512292
  - 0.15495234675472602
  - 0.16114773438312113
  - 0.1541282896650955
  - 0.16063767654122785
  - 0.1560482537606731
  - 0.1604362775105983
  - 0.15624335198663175
  - 0.16161451791413128
  - 0.15790895686950535
  - 0.15570061834296212
  - 0.16010837681824341
  - 0.15979021054226905
  - 0.1607857079943642
  - 0.15668989066034555
  - 0.15806074830470607
  - 0.1567972864722833
