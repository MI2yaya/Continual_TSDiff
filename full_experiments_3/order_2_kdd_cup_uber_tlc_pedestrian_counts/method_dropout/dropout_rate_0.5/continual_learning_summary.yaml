continual_learning_setup:
  method: dropout
  method_params:
    dropout_rate: 0.5
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
  - train_pedestrian_counts.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_dropout/dropout_rate_0.5/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.5
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3511087488598352
      NRMSE: 0.7236371376438168
      mean_wQuantileLoss: 0.2798735518677761
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.46765551646240056
      - 0.37451054679695517
      - 0.3358415161492303
      - 0.31376684410497546
      - 0.291025843587704
      - 0.2788518989691511
      - 0.2599144874839112
      - 0.25909688032697886
      - 0.2498927570413798
      - 0.2507029474945739
      - 0.24153391807340086
      - 0.23750089504756033
      - 0.23964445502497256
      - 0.24098325765226036
      - 0.23400661605410278
      - 0.23218993213959038
      - 0.23116005526389927
      - 0.23195268295239657
      - 0.23259770881850272
      - 0.23365913634188473
      - 0.2324169701896608
      - 0.2305833693826571
      - 0.2298208427382633
      - 0.22423378913663328
      - 0.22941178211476654
      - 0.2264696933561936
      - 0.22249112755525857
      - 0.2265671791974455
      - 0.22092151292599738
      - 0.22419860365334898
      - 0.22619872400537133
      - 0.22105874214321375
      - 0.22460834006778896
      - 0.22575021581724286
      - 0.2227625889936462
      - 0.22988143097609282
      - 0.2231668446911499
      - 0.22818022745195776
      - 0.22226548253092915
      - 0.21998025884386152
      - 0.21807596168946475
      - 0.22320573136676103
      - 0.22288746235426515
      - 0.2163447820348665
      - 0.22385711432434618
      - 0.22697514458559453
      - 0.2220500900875777
      - 0.22343700734199956
      - 0.21757517638616264
      - 0.21631361730396748
      - 0.22076921758707613
      - 0.21929192263633013
      - 0.22206841909792274
      - 0.21488598873838782
      - 0.22486333712004125
      - 0.22376702108886093
      - 0.2202779621584341
      - 0.22323022445198148
      - 0.2169459768338129
      - 0.2240086761303246
      - 0.21450924035161734
      - 0.2225857072044164
      - 0.2144004323054105
      - 0.21906048036180437
      - 0.21491359692299739
      - 0.2169650422874838
      - 0.2177398760104552
      - 0.22368277504574507
      - 0.21722783241420984
      - 0.21778003044892102
      - 0.2134446551790461
      val_loss:
      - 0.29381239116191865
      - 0.37451054679695517
      - 0.3358415161492303
      - 0.31376684410497546
      - 0.291025843587704
      - 0.2788518989691511
      - 0.2599144874839112
      - 0.25909688032697886
      - 0.2498927570413798
      - 0.2507029474945739
      - 0.24153391807340086
      - 0.23750089504756033
      - 0.23964445502497256
      - 0.24098325765226036
      - 0.23400661605410278
      - 0.23218993213959038
      - 0.23116005526389927
      - 0.23195268295239657
      - 0.23259770881850272
      - 0.23365913634188473
      - 0.2324169701896608
      - 0.2305833693826571
      - 0.2298208427382633
      - 0.22423378913663328
      - 0.22941178211476654
      - 0.2264696933561936
      - 0.22249112755525857
      - 0.2265671791974455
      - 0.22092151292599738
      - 0.22419860365334898
      - 0.22619872400537133
      - 0.22105874214321375
      - 0.22460834006778896
      - 0.22575021581724286
      - 0.2227625889936462
      - 0.22988143097609282
      - 0.2231668446911499
      - 0.22818022745195776
      - 0.22226548253092915
      - 0.21998025884386152
      - 0.21807596168946475
      - 0.22320573136676103
      - 0.22288746235426515
      - 0.2163447820348665
      - 0.22385711432434618
      - 0.22697514458559453
      - 0.2220500900875777
      - 0.22343700734199956
      - 0.21757517638616264
      - 0.21631361730396748
      - 0.1952800989151001
      - 0.21929192263633013
      - 0.22206841909792274
      - 0.21488598873838782
      - 0.22486333712004125
      - 0.22376702108886093
      - 0.2202779621584341
      - 0.22323022445198148
      - 0.2169459768338129
      - 0.2240086761303246
      - 0.21450924035161734
      - 0.2225857072044164
      - 0.2144004323054105
      - 0.21906048036180437
      - 0.21491359692299739
      - 0.2169650422874838
      - 0.2177398760104552
      - 0.22368277504574507
      - 0.21722783241420984
      - 0.21778003044892102
      - 0.2134446551790461
  task_id: 1
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_dropout/dropout_rate_0.5/task_2_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.5
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.2216734080820053
      NRMSE: 0.5631759291815069
      mean_wQuantileLoss: 0.20704212975174754
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.6117069940082729
      - 0.4029660988599062
      - 0.3619381347671151
      - 0.3539333564694971
      - 0.3388452026993036
      - 0.3336816031951457
      - 0.3323780653299764
      - 0.32603003119584173
      - 0.3249148044269532
      - 0.32524115568958223
      - 0.32671048410702497
      - 0.3222571949008852
      - 0.32149864244274795
      - 0.3239201324759051
      - 0.32475216477178037
      - 0.3145386619726196
      - 0.3197302930057049
      - 0.3171671889722347
      - 0.3140193751314655
      - 0.3175362842157483
      - 0.315422423183918
      - 0.31789653399027884
      - 0.3139513311907649
      - 0.312501389416866
      - 0.3166559733217582
      - 0.3153059969190508
      - 0.30617946630809456
      - 0.3113156132167205
      - 0.30719143175520003
      - 0.3117469773860648
      - 0.31305183249060065
      - 0.3066339362412691
      - 0.31349010253325105
      - 0.3129806115757674
      - 0.306416338076815
      - 0.31063810852356255
      - 0.30737500463146716
      - 0.3078796975314617
      - 0.30531064490787685
      - 0.3073962359922007
      - 0.3064304947620258
      - 0.3088022532174364
      - 0.30686764197889715
      - 0.3038492964114994
      - 0.305950308800675
      - 0.29819565545767546
      - 0.3006737797986716
      - 0.3020297043258324
      - 0.29976165073458105
      - 0.3066680256742984
      - 0.3052109138807282
      - 0.3013372545829043
      - 0.3087838280480355
      - 0.3005917383125052
      - 0.2993934932164848
      - 0.29756314982660115
      - 0.30143356090411544
      - 0.3000721810385585
      - 0.30160176882054657
      - 0.306589744403027
      - 0.3008325124392286
      - 0.3002929715439677
      - 0.302933044382371
      - 0.2988468747353181
      - 0.2998110535554588
      - 0.2997535051545128
      - 0.30009186605457217
      - 0.2952046957798302
      - 0.29931993898935616
      - 0.2972579913912341
      - 0.2964616366662085
      val_loss:
      - 0.3587422788143158
      - 0.4029660988599062
      - 0.3619381347671151
      - 0.3539333564694971
      - 0.3388452026993036
      - 0.3336816031951457
      - 0.3323780653299764
      - 0.32603003119584173
      - 0.3249148044269532
      - 0.32524115568958223
      - 0.32671048410702497
      - 0.3222571949008852
      - 0.32149864244274795
      - 0.3239201324759051
      - 0.32475216477178037
      - 0.3145386619726196
      - 0.3197302930057049
      - 0.3171671889722347
      - 0.3140193751314655
      - 0.3175362842157483
      - 0.315422423183918
      - 0.31789653399027884
      - 0.3139513311907649
      - 0.312501389416866
      - 0.3166559733217582
      - 0.3153059969190508
      - 0.30617946630809456
      - 0.3113156132167205
      - 0.30719143175520003
      - 0.3117469773860648
      - 0.31305183249060065
      - 0.3066339362412691
      - 0.31349010253325105
      - 0.3129806115757674
      - 0.306416338076815
      - 0.31063810852356255
      - 0.30737500463146716
      - 0.3078796975314617
      - 0.30531064490787685
      - 0.3073962359922007
      - 0.3064304947620258
      - 0.3088022532174364
      - 0.30686764197889715
      - 0.3038492964114994
      - 0.305950308800675
      - 0.29819565545767546
      - 0.3006737797986716
      - 0.3020297043258324
      - 0.29976165073458105
      - 0.3066680256742984
      - 0.2729238510131836
      - 0.3013372545829043
      - 0.3087838280480355
      - 0.3005917383125052
      - 0.2993934932164848
      - 0.29756314982660115
      - 0.30143356090411544
      - 0.3000721810385585
      - 0.30160176882054657
      - 0.306589744403027
      - 0.3008325124392286
      - 0.3002929715439677
      - 0.302933044382371
      - 0.2988468747353181
      - 0.2998110535554588
      - 0.2997535051545128
      - 0.30009186605457217
      - 0.2952046957798302
      - 0.29931993898935616
      - 0.2972579913912341
      - 0.2964616366662085
  task_id: 2
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_dropout/dropout_rate_0.5/task_3_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.5
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.21293788705358857
      NRMSE: 0.7420208064064493
      mean_wQuantileLoss: 0.17799777371069264
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5473735935520381
      - 0.3782324402127415
      - 0.3723128723213449
      - 0.32878794614225626
      - 0.34177165548317134
      - 0.3167979421559721
      - 0.3062973904889077
      - 0.2754349261522293
      - 0.2791125151561573
      - 0.2590220497222617
      - 0.2762749028624967
      - 0.2521140605676919
      - 0.24542238004505634
      - 0.23742754792328924
      - 0.2412685095332563
      - 0.24092631274834275
      - 0.22773677273653448
      - 0.25030510360375047
      - 0.22908968350384384
      - 0.2265718198614195
      - 0.22349441261030734
      - 0.22415679157711565
      - 0.2242262288928032
      - 0.22250401636119932
      - 0.21369672776199877
      - 0.220237847417593
      - 0.21932814293541014
      - 0.22602354013361037
      - 0.21209324442315847
      - 0.21852783125359565
      - 0.21624899213202298
      - 0.2125648088986054
      - 0.20933117729146034
      - 0.21307461592368782
      - 0.21170391968917102
      - 0.2107160387095064
      - 0.20729916321579367
      - 0.20646883395966142
      - 0.21393151639495045
      - 0.21789333410561085
      - 0.20711109699914232
      - 0.20536984503269196
      - 0.2090337445260957
      - 0.2059389044297859
      - 0.21424760692752898
      - 0.2052775271004066
      - 0.20563906367169693
      - 0.2039011570159346
      - 0.20563049567863345
      - 0.2020519640063867
      - 0.22051281412132084
      - 0.20854448236059397
      - 0.21637594979256392
      - 0.2042998515535146
      - 0.20462059904821217
      - 0.2049634851864539
      - 0.21639365778537467
      - 0.20313313300721347
      - 0.20028522470965981
      - 0.20321329100988805
      - 0.1957599683664739
      - 0.20134465396404266
      - 0.20098993473220617
      - 0.20014088146854192
      - 0.22489810816477984
      - 0.19892955286195502
      - 0.19789029681123793
      - 0.19717897073132917
      - 0.20087755238637328
      - 0.19998139527160674
      - 0.21093404456041753
      val_loss:
      - 0.4346848875284195
      - 0.3782324402127415
      - 0.3723128723213449
      - 0.32878794614225626
      - 0.34177165548317134
      - 0.3167979421559721
      - 0.3062973904889077
      - 0.2754349261522293
      - 0.2791125151561573
      - 0.2590220497222617
      - 0.2762749028624967
      - 0.2521140605676919
      - 0.24542238004505634
      - 0.23742754792328924
      - 0.2412685095332563
      - 0.24092631274834275
      - 0.22773677273653448
      - 0.25030510360375047
      - 0.22908968350384384
      - 0.2265718198614195
      - 0.22349441261030734
      - 0.22415679157711565
      - 0.2242262288928032
      - 0.22250401636119932
      - 0.21369672776199877
      - 0.220237847417593
      - 0.21932814293541014
      - 0.22602354013361037
      - 0.21209324442315847
      - 0.21852783125359565
      - 0.21624899213202298
      - 0.2125648088986054
      - 0.20933117729146034
      - 0.21307461592368782
      - 0.21170391968917102
      - 0.2107160387095064
      - 0.20729916321579367
      - 0.20646883395966142
      - 0.21393151639495045
      - 0.21789333410561085
      - 0.20711109699914232
      - 0.20536984503269196
      - 0.2090337445260957
      - 0.2059389044297859
      - 0.21424760692752898
      - 0.2052775271004066
      - 0.20563906367169693
      - 0.2039011570159346
      - 0.20563049567863345
      - 0.2020519640063867
      - 0.1920299381017685
      - 0.20854448236059397
      - 0.21637594979256392
      - 0.2042998515535146
      - 0.20462059904821217
      - 0.2049634851864539
      - 0.21639365778537467
      - 0.20313313300721347
      - 0.20028522470965981
      - 0.20321329100988805
      - 0.1957599683664739
      - 0.20134465396404266
      - 0.20098993473220617
      - 0.20014088146854192
      - 0.22489810816477984
      - 0.19892955286195502
      - 0.19789029681123793
      - 0.19717897073132917
      - 0.20087755238637328
      - 0.19998139527160674
      - 0.21093404456041753
  task_id: 3
