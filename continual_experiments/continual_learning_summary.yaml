continual_learning_setup:
  lambda_reg: 1.0
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: /export/home/anandr/diffusion/Continual_TSDiff/configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: null
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.01
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 1.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3556940296592791
      NRMSE: 0.7137843000925149
      mean_wQuantileLoss: 0.2737766157333218
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      - 116
      - 117
      - 118
      - 119
      - 120
      - 121
      train_loss:
      - 0.40898432314861566
      - 0.2890598381636664
      - 0.24210883828345686
      - 0.21793014428112656
      - 0.2148120221681893
      - 0.19547593034803867
      - 0.19848808378446847
      - 0.19997167243855074
      - 0.1975783008383587
      - 0.1933060041628778
      - 0.18493402283638716
      - 0.183914142427966
      - 0.18527776410337538
      - 0.1796051591518335
      - 0.18023379723308608
      - 0.1777088091475889
      - 0.1775394044816494
      - 0.17739565746160224
      - 0.17187677137553692
      - 0.1771614711615257
      - 0.1744478425825946
      - 0.17183805350214243
      - 0.1678182821488008
      - 0.17036562215071172
      - 0.1658164507825859
      - 0.17158207669854164
      - 0.17421275877859443
      - 0.16160152369411662
      - 0.16634697193512693
      - 0.1655501757049933
      - 0.167772141227033
      - 0.17068807635223493
      - 0.1663959623547271
      - 0.16797177464468405
      - 0.16656077472725883
      - 0.16045373264933005
      - 0.15972293866798282
      - 0.16309943969827145
      - 0.16106388438493013
      - 0.16156768752261996
      - 0.15780782781075686
      - 0.1601978250546381
      - 0.16341716062743217
      - 0.1613419649656862
      - 0.157630157249514
      - 0.15620362316258252
      - 0.16186831437516958
      - 0.1594445594237186
      - 0.16109868191415444
      - 0.15720785991288722
      - 0.1610086527070962
      - 0.15846959600457922
      - 0.15688051556935534
      - 0.16074025112902746
      - 0.158689834701363
      - 0.15573321847477928
      - 0.15872210229281336
      - 0.1569313951767981
      - 0.15497254254296422
      - 0.15929522406077012
      - 0.15575739607447758
      - 0.15713264310033992
      - 0.15041490126168355
      - 0.160601612937171
      - 0.1611302868113853
      - 0.15554952883394435
      - 0.1538274473277852
      - 0.15936630265787244
      - 0.15335957135539502
      - 0.15867377014365047
      - 0.15896264667389914
      - 0.15781286545097828
      - 0.15820987324696034
      - 0.15719544957391918
      - 0.15382526500616223
      - 0.15235293557634577
      - 0.15852026536595076
      - 0.15020352450665087
      - 0.15267401083838195
      - 0.15131247747922316
      - 0.15264798334101215
      - 0.14838169963331893
      - 0.15314914332702756
      - 0.1499310969375074
      - 0.15455112158088014
      - 0.14851620368426666
      - 0.1517853196710348
      - 0.15091262257192284
      - 0.15368043130729347
      - 0.15081971342442557
      - 0.1529574524029158
      - 0.15198719123145565
      - 0.1499247484607622
      - 0.153114419314079
      - 0.1463877911446616
      - 0.14985555928433314
      - 0.14998402335913852
      - 0.1523387446650304
      - 0.14925275422865525
      - 0.15363170788623393
      - 0.1535496732685715
      - 0.1492709761369042
      - 0.14711467892630026
      - 0.15063064801506698
      - 0.14950884296558797
      - 0.1490399771137163
      - 0.14732891140738502
      - 0.14668687002267689
      - 0.14843961916631088
      - 0.15252813400002196
      - 0.1471892151166685
      - 0.14819205907406285
      - 0.14847000100417063
      - 0.15132841630838811
      - 0.15028379735304043
      - 0.14464854216203094
      - 0.14737116760807112
      - 0.1479361653327942
      - 0.1456364342593588
      - 0.14531236968468875
      - 0.14906306902412325
      val_loss:
      - 0.2979393005371094
      - 0.2890598381636664
      - 0.24210883828345686
      - 0.21793014428112656
      - 0.2148120221681893
      - 0.19547593034803867
      - 0.19848808378446847
      - 0.19997167243855074
      - 0.1975783008383587
      - 0.1933060041628778
      - 0.18493402283638716
      - 0.183914142427966
      - 0.18527776410337538
      - 0.1796051591518335
      - 0.18023379723308608
      - 0.1777088091475889
      - 0.1775394044816494
      - 0.17739565746160224
      - 0.17187677137553692
      - 0.1771614711615257
      - 0.1744478425825946
      - 0.17183805350214243
      - 0.1678182821488008
      - 0.17036562215071172
      - 0.1658164507825859
      - 0.17158207669854164
      - 0.17421275877859443
      - 0.16160152369411662
      - 0.16634697193512693
      - 0.1655501757049933
      - 0.167772141227033
      - 0.17068807635223493
      - 0.1663959623547271
      - 0.16797177464468405
      - 0.16656077472725883
      - 0.16045373264933005
      - 0.15972293866798282
      - 0.16309943969827145
      - 0.16106388438493013
      - 0.16156768752261996
      - 0.15780782781075686
      - 0.1601978250546381
      - 0.16341716062743217
      - 0.1613419649656862
      - 0.157630157249514
      - 0.15620362316258252
      - 0.16186831437516958
      - 0.1594445594237186
      - 0.16109868191415444
      - 0.15720785991288722
      - 0.16374460309743882
      - 0.15846959600457922
      - 0.15688051556935534
      - 0.16074025112902746
      - 0.158689834701363
      - 0.15573321847477928
      - 0.15872210229281336
      - 0.1569313951767981
      - 0.15497254254296422
      - 0.15929522406077012
      - 0.15575739607447758
      - 0.15713264310033992
      - 0.15041490126168355
      - 0.160601612937171
      - 0.1611302868113853
      - 0.15554952883394435
      - 0.1538274473277852
      - 0.15936630265787244
      - 0.15335957135539502
      - 0.15867377014365047
      - 0.15896264667389914
      - 0.15781286545097828
      - 0.15820987324696034
      - 0.15719544957391918
      - 0.15382526500616223
      - 0.15235293557634577
      - 0.15852026536595076
      - 0.15020352450665087
      - 0.15267401083838195
      - 0.15131247747922316
      - 0.15264798334101215
      - 0.14838169963331893
      - 0.15314914332702756
      - 0.1499310969375074
      - 0.15455112158088014
      - 0.14851620368426666
      - 0.1517853196710348
      - 0.15091262257192284
      - 0.15368043130729347
      - 0.15081971342442557
      - 0.1529574524029158
      - 0.15198719123145565
      - 0.1499247484607622
      - 0.153114419314079
      - 0.1463877911446616
      - 0.14985555928433314
      - 0.14998402335913852
      - 0.1523387446650304
      - 0.14925275422865525
      - 0.15363170788623393
      - 0.14401856511831285
      - 0.1492709761369042
      - 0.14711467892630026
      - 0.15063064801506698
      - 0.14950884296558797
      - 0.1490399771137163
      - 0.14732891140738502
      - 0.14668687002267689
      - 0.14843961916631088
      - 0.15252813400002196
      - 0.1471892151166685
      - 0.14819205907406285
      - 0.14847000100417063
      - 0.15132841630838811
      - 0.15028379735304043
      - 0.14464854216203094
      - 0.14737116760807112
      - 0.1479361653327942
      - 0.1456364342593588
      - 0.14531236968468875
      - 0.14906306902412325
  task_id: 1
- config_path: /export/home/anandr/diffusion/Continual_TSDiff/configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: null
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.01
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 1.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.1968089696103515
      NRMSE: 0.6816569062750594
      mean_wQuantileLoss: 0.1585822485447826
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      - 116
      - 117
      - 118
      - 119
      - 120
      - 121
      train_loss:
      - 0.5012301922542974
      - 0.3839315513614565
      - 0.3011006952729076
      - 0.3090288800885901
      - 0.2562067467952147
      - 0.22054884157842025
      - 0.21447488968260586
      - 0.21115449769422412
      - 0.1906818487914279
      - 0.18901421158807352
      - 0.18700182059546933
      - 0.18942398019134998
      - 0.18254857941064984
      - 0.1779940125416033
      - 0.17667867382988334
      - 0.17334427899913862
      - 0.22017934184987098
      - 0.17142370162764564
      - 0.1667309264303185
      - 0.16586559527786449
      - 0.16864467097911984
      - 0.17987821967108175
      - 0.1945153782144189
      - 0.1633812818909064
      - 0.1596172550925985
      - 0.15908328449586406
      - 0.17327340075280517
      - 0.19284821499604732
      - 0.1646619148668833
      - 0.15790143323829398
      - 0.15953282470582053
      - 0.15859876229660586
      - 0.16115363378776237
      - 0.1574211156112142
      - 0.17894313455326483
      - 0.157565166358836
      - 0.15566717192996293
      - 0.1563993546878919
      - 0.1565548566286452
      - 0.15208136168075725
      - 0.15646479709539562
      - 0.17868510307744145
      - 0.1575000723823905
      - 0.15198126004543155
      - 0.15483338054036722
      - 0.15268146607559174
      - 0.15015800338005647
      - 0.19069136888720095
      - 0.14837094291578978
      - 0.1592766382964328
      - 0.15389424446038902
      - 0.15156110672978684
      - 0.15872358169872314
      - 0.1634629040490836
      - 0.15131936583202332
      - 0.15975048212567344
      - 0.14649127604207024
      - 0.1474185876431875
      - 0.17131334549048916
      - 0.14987504831515253
      - 0.14536555693484843
      - 0.14541839761659503
      - 0.15038114215712994
      - 0.15309111477108672
      - 0.15107228001579642
      - 0.15020235517295077
      - 0.14716049312846735
      - 0.14756488293642178
      - 0.14772013440961018
      - 0.14981084602186456
      - 0.16810382629046217
      - 0.1564738431479782
      - 0.14588627347256988
      - 0.165353411459364
      - 0.1416287405299954
      - 0.1447667886968702
      - 0.16191158193396404
      - 0.14265090093249455
      - 0.14669770427281037
      - 0.14938123093452305
      - 0.14108675107127056
      - 0.14025059872074053
      - 0.1419331063516438
      - 0.14636518293991685
      - 0.14013774949125946
      - 0.14451030525378883
      - 0.14438601076835766
      - 0.13754098973004147
      - 0.14109976857434958
      - 0.14493861270602793
      - 0.14050543715711683
      - 0.1413930943235755
      - 0.1398541527451016
      - 0.14412390819052234
      - 0.1443376322858967
      - 0.14564530202187598
      - 0.14256500138435513
      - 0.13899513497017324
      - 0.14008722105063498
      - 0.15359594422625378
      - 0.14145872642984614
      - 0.14030736952554435
      - 0.13855927140684798
      - 0.13774884154554456
      - 0.13952866499312222
      - 0.13887176848948002
      - 0.14086057135136798
      - 0.14218581083696336
      - 0.14128387399250641
      - 0.14527345582609996
      - 0.15025708475150168
      - 0.14230232895351946
      - 0.14031102956505492
      - 0.13923070614691824
      - 0.1408086337032728
      - 0.14051189774181694
      - 0.1462008441449143
      - 0.1519113247632049
      - 0.13541860750410706
      - 0.13565668946830556
      - 0.13596836337819695
      val_loss:
      - 0.33903468400239944
      - 0.3839315513614565
      - 0.3011006952729076
      - 0.3090288800885901
      - 0.2562067467952147
      - 0.22054884157842025
      - 0.21447488968260586
      - 0.21115449769422412
      - 0.1906818487914279
      - 0.18901421158807352
      - 0.18700182059546933
      - 0.18942398019134998
      - 0.18254857941064984
      - 0.1779940125416033
      - 0.17667867382988334
      - 0.17334427899913862
      - 0.22017934184987098
      - 0.17142370162764564
      - 0.1667309264303185
      - 0.16586559527786449
      - 0.16864467097911984
      - 0.17987821967108175
      - 0.1945153782144189
      - 0.1633812818909064
      - 0.1596172550925985
      - 0.15908328449586406
      - 0.17327340075280517
      - 0.19284821499604732
      - 0.1646619148668833
      - 0.15790143323829398
      - 0.15953282470582053
      - 0.15859876229660586
      - 0.16115363378776237
      - 0.1574211156112142
      - 0.17894313455326483
      - 0.157565166358836
      - 0.15566717192996293
      - 0.1563993546878919
      - 0.1565548566286452
      - 0.15208136168075725
      - 0.15646479709539562
      - 0.17868510307744145
      - 0.1575000723823905
      - 0.15198126004543155
      - 0.15483338054036722
      - 0.15268146607559174
      - 0.15015800338005647
      - 0.19069136888720095
      - 0.14837094291578978
      - 0.1592766382964328
      - 0.28080595284700394
      - 0.15156110672978684
      - 0.15872358169872314
      - 0.1634629040490836
      - 0.15131936583202332
      - 0.15975048212567344
      - 0.14649127604207024
      - 0.1474185876431875
      - 0.17131334549048916
      - 0.14987504831515253
      - 0.14536555693484843
      - 0.14541839761659503
      - 0.15038114215712994
      - 0.15309111477108672
      - 0.15107228001579642
      - 0.15020235517295077
      - 0.14716049312846735
      - 0.14756488293642178
      - 0.14772013440961018
      - 0.14981084602186456
      - 0.16810382629046217
      - 0.1564738431479782
      - 0.14588627347256988
      - 0.165353411459364
      - 0.1416287405299954
      - 0.1447667886968702
      - 0.16191158193396404
      - 0.14265090093249455
      - 0.14669770427281037
      - 0.14938123093452305
      - 0.14108675107127056
      - 0.14025059872074053
      - 0.1419331063516438
      - 0.14636518293991685
      - 0.14013774949125946
      - 0.14451030525378883
      - 0.14438601076835766
      - 0.13754098973004147
      - 0.14109976857434958
      - 0.14493861270602793
      - 0.14050543715711683
      - 0.1413930943235755
      - 0.1398541527451016
      - 0.14412390819052234
      - 0.1443376322858967
      - 0.14564530202187598
      - 0.14256500138435513
      - 0.13899513497017324
      - 0.14008722105063498
      - 0.15359594422625378
      - 0.1316593401134014
      - 0.14030736952554435
      - 0.13855927140684798
      - 0.13774884154554456
      - 0.13952866499312222
      - 0.13887176848948002
      - 0.14086057135136798
      - 0.14218581083696336
      - 0.14128387399250641
      - 0.14527345582609996
      - 0.15025708475150168
      - 0.14230232895351946
      - 0.14031102956505492
      - 0.13923070614691824
      - 0.1408086337032728
      - 0.14051189774181694
      - 0.1462008441449143
      - 0.1519113247632049
      - 0.13541860750410706
      - 0.13565668946830556
      - 0.13596836337819695
  task_id: 2
- config_path: /export/home/anandr/diffusion/Continual_TSDiff/configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: null
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.01
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 1.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.21506085817560416
      NRMSE: 0.5034489563358203
      mean_wQuantileLoss: 0.17509430409605098
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4991975468583405
      - 0.3390156084205955
      - 0.31524114322382957
      - 0.3101086103124544
      - 0.30267357896082103
      - 0.29239185876213014
      - 0.2863986975280568
      - 0.2899120590882376
      - 0.2853381468448788
      - 0.27535123378038406
      - 0.2780486874980852
      - 0.27340745797846466
      - 0.27301356266252697
      - 0.2747927571181208
      - 0.26902152388356626
      - 0.2672914204886183
      - 0.26187913096509874
      - 0.2664707489311695
      - 0.2632363684242591
      - 0.2608717024559155
      - 0.25700894626788795
      - 0.25825936580076814
      - 0.26085583947133273
      - 0.2586131839780137
      - 0.25629843468777835
      - 0.26068173127714545
      - 0.25111518369521946
      - 0.25501419324427843
      - 0.255662924493663
      - 0.2512330387253314
      - 0.2514627642231062
      - 0.24901653383858502
      - 0.2507490678690374
      - 0.2537037825677544
      - 0.2495823543285951
      - 0.25074943504296243
      - 0.2464299553539604
      - 0.24812502483837306
      - 0.2499768314883113
      - 0.24890205066185445
      - 0.2444404768757522
      - 0.24827222130261362
      - 0.24613302305806428
      - 0.23904813174158335
      - 0.24850347079336643
      - 0.24238370056264102
      - 0.242095653899014
      - 0.2454747874289751
      - 0.24185901891905814
      - 0.24448377138469368
      - 0.24072684405837208
      - 0.24260692216921598
      - 0.24133372423239052
      - 0.2375922945793718
      - 0.24042012193240225
      - 0.2427213218761608
      - 0.24075924966018647
      - 0.23821896663866937
      - 0.23803275963291526
      - 0.24112143623642623
      - 0.24675260507501662
      - 0.23786779737565666
      - 0.24020586162805557
      - 0.2302061445079744
      - 0.24398649879731238
      - 0.23780455940868706
      - 0.2355570790823549
      - 0.24155544908717275
      - 0.24630478105973452
      - 0.23609427316114306
      - 0.24047911074012518
      val_loss:
      - 0.34133688509464266
      - 0.3390156084205955
      - 0.31524114322382957
      - 0.3101086103124544
      - 0.30267357896082103
      - 0.29239185876213014
      - 0.2863986975280568
      - 0.2899120590882376
      - 0.2853381468448788
      - 0.27535123378038406
      - 0.2780486874980852
      - 0.27340745797846466
      - 0.27301356266252697
      - 0.2747927571181208
      - 0.26902152388356626
      - 0.2672914204886183
      - 0.26187913096509874
      - 0.2664707489311695
      - 0.2632363684242591
      - 0.2608717024559155
      - 0.25700894626788795
      - 0.25825936580076814
      - 0.26085583947133273
      - 0.2586131839780137
      - 0.25629843468777835
      - 0.26068173127714545
      - 0.25111518369521946
      - 0.25501419324427843
      - 0.255662924493663
      - 0.2512330387253314
      - 0.2514627642231062
      - 0.24901653383858502
      - 0.2507490678690374
      - 0.2537037825677544
      - 0.2495823543285951
      - 0.25074943504296243
      - 0.2464299553539604
      - 0.24812502483837306
      - 0.2499768314883113
      - 0.24890205066185445
      - 0.2444404768757522
      - 0.24827222130261362
      - 0.24613302305806428
      - 0.23904813174158335
      - 0.24850347079336643
      - 0.24238370056264102
      - 0.242095653899014
      - 0.2454747874289751
      - 0.24185901891905814
      - 0.24448377138469368
      - 0.2172204375267029
      - 0.24260692216921598
      - 0.24133372423239052
      - 0.2375922945793718
      - 0.24042012193240225
      - 0.2427213218761608
      - 0.24075924966018647
      - 0.23821896663866937
      - 0.23803275963291526
      - 0.24112143623642623
      - 0.24675260507501662
      - 0.23786779737565666
      - 0.24020586162805557
      - 0.2302061445079744
      - 0.24398649879731238
      - 0.23780455940868706
      - 0.2355570790823549
      - 0.24155544908717275
      - 0.24630478105973452
      - 0.23609427316114306
      - 0.24047911074012518
  task_id: 3
