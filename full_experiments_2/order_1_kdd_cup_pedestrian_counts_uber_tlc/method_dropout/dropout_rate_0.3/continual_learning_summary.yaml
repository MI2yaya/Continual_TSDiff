continual_learning_setup:
  method: dropout
  method_params:
    dropout_rate: 0.3
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_2/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.3/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.3
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.32228189592962886
      NRMSE: 0.6836348271697021
      mean_wQuantileLoss: 0.2551873892473432
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4286052866373211
      - 0.33592970424797386
      - 0.287794025731273
      - 0.2775283691007644
      - 0.25772812974173576
      - 0.23953111667651683
      - 0.23304683610331267
      - 0.23701872455421835
      - 0.2275742336641997
      - 0.228683638619259
      - 0.22298965591471642
      - 0.22117747587617487
      - 0.21031057520303875
      - 0.20324072253424674
      - 0.2063715651165694
      - 0.21092299662996083
      - 0.20798792783170938
      - 0.20403918449301273
      - 0.2059494418790564
      - 0.2055871661286801
      - 0.2076190774096176
      - 0.20527497422881424
      - 0.20042625640053302
      - 0.20019360398873687
      - 0.19757749210111797
      - 0.20081918174400926
      - 0.19881647638976574
      - 0.20060511957854033
      - 0.199921888532117
      - 0.2020452736178413
      - 0.19674595771357417
      - 0.19996165810152888
      - 0.20053377555450425
      - 0.19938716373872012
      - 0.1970012987148948
      - 0.20363312889821827
      - 0.19438159913988784
      - 0.19512977777048945
      - 0.19157982931938022
      - 0.19036568579031155
      - 0.19539258792065084
      - 0.20252274558879435
      - 0.19236206356436014
      - 0.19248938269447535
      - 0.1899670840939507
      - 0.1913273210520856
      - 0.1896040077554062
      - 0.19057159178191796
      - 0.19684911461081356
      - 0.19046366767724976
      - 0.19126321480143815
      - 0.19277014228282496
      - 0.18834488926222548
      - 0.19058199948631227
      - 0.19152736815158278
      - 0.19123823486734182
      - 0.18797948281280696
      - 0.18973345891572535
      - 0.19412455341080204
      - 0.19008524680975825
      - 0.1942972092074342
      - 0.1921521172625944
      - 0.19021499092923477
      - 0.18817153968848288
      - 0.18989657773636281
      - 0.19474237621761858
      - 0.1851959713967517
      - 0.18831629649503157
      - 0.18747345928568393
      - 0.1868188809021376
      - 0.18773310468532145
      val_loss:
      - 0.3014885812997818
      - 0.33592970424797386
      - 0.287794025731273
      - 0.2775283691007644
      - 0.25772812974173576
      - 0.23953111667651683
      - 0.23304683610331267
      - 0.23701872455421835
      - 0.2275742336641997
      - 0.228683638619259
      - 0.22298965591471642
      - 0.22117747587617487
      - 0.21031057520303875
      - 0.20324072253424674
      - 0.2063715651165694
      - 0.21092299662996083
      - 0.20798792783170938
      - 0.20403918449301273
      - 0.2059494418790564
      - 0.2055871661286801
      - 0.2076190774096176
      - 0.20527497422881424
      - 0.20042625640053302
      - 0.20019360398873687
      - 0.19757749210111797
      - 0.20081918174400926
      - 0.19881647638976574
      - 0.20060511957854033
      - 0.199921888532117
      - 0.2020452736178413
      - 0.19674595771357417
      - 0.19996165810152888
      - 0.20053377555450425
      - 0.19938716373872012
      - 0.1970012987148948
      - 0.20363312889821827
      - 0.19438159913988784
      - 0.19512977777048945
      - 0.19157982931938022
      - 0.19036568579031155
      - 0.19539258792065084
      - 0.20252274558879435
      - 0.19236206356436014
      - 0.19248938269447535
      - 0.1899670840939507
      - 0.1913273210520856
      - 0.1896040077554062
      - 0.19057159178191796
      - 0.19684911461081356
      - 0.19046366767724976
      - 0.13089987635612488
      - 0.19277014228282496
      - 0.18834488926222548
      - 0.19058199948631227
      - 0.19152736815158278
      - 0.19123823486734182
      - 0.18797948281280696
      - 0.18973345891572535
      - 0.19412455341080204
      - 0.19008524680975825
      - 0.1942972092074342
      - 0.1921521172625944
      - 0.19021499092923477
      - 0.18817153968848288
      - 0.18989657773636281
      - 0.19474237621761858
      - 0.1851959713967517
      - 0.18831629649503157
      - 0.18747345928568393
      - 0.1868188809021376
      - 0.18773310468532145
  task_id: 1
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_2/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.3/task_2_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.3
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.21877198792722863
      NRMSE: 0.755894741292215
      mean_wQuantileLoss: 0.17685798194205135
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      train_loss:
      - 0.5191890121204779
      - 0.3910925177624449
      - 0.36193132388871163
      - 0.32331004145089537
      - 0.31093868042808026
      - 0.28943748015444726
      - 0.29704456496983767
      - 0.25486046471633017
      - 0.2529364920919761
      - 0.24774087977129966
      - 0.23135247360914946
      - 0.22410747490357608
      - 0.21683545608539134
      - 0.22818546032067388
      - 0.21794721123296767
      - 0.2388475261395797
      - 0.21246636379510164
      - 0.2065076023573056
      - 0.2050943449139595
      - 0.20637431275099516
      - 0.20433115388732404
      - 0.19655799551401287
      - 0.19721923663746566
      - 0.2098105886252597
      - 0.203340892738197
      - 0.19898584665497765
      - 0.19000263360794634
      - 0.21338812971953303
      - 0.19244756398256868
      - 0.1929508101893589
      - 0.1962040705839172
      - 0.19406298029934987
      - 0.21408713317941874
      - 0.18419339566025883
      - 0.18776367377722636
      - 0.18397216452285647
      - 0.18441745691234246
      - 0.220149077125825
      - 0.2038574970792979
      - 0.18307202775031328
      - 0.2027471243054606
      - 0.18828067556023598
      - 0.18706834712065756
      - 0.19154361553955823
      - 0.18367693515028805
      - 0.18425102368928492
      - 0.17996131908148527
      - 0.18821698921965435
      - 0.18905519979307428
      - 0.18365124356932938
      - 0.17821374675258994
      - 0.17794926918577403
      - 0.19472957885591313
      - 0.1837863672990352
      - 0.18078152212547138
      - 0.175515394017566
      - 0.1817138430196792
      - 0.18929999694228172
      - 0.1775084674009122
      - 0.18314735224703327
      - 0.18865245423512533
      - 0.1786546579678543
      - 0.1795898994896561
      - 0.17550344386836514
      - 0.17489686358021572
      - 0.1759599082870409
      - 0.17563654511468485
      - 0.1757256879354827
      - 0.1785948463366367
      - 0.17864089913200587
      - 0.1830079536885023
      - 0.1749028250342235
      - 0.1822389992303215
      - 0.17846990621183068
      - 0.18986667884746566
      - 0.17356127948733047
      - 0.17718140635406598
      - 0.17392969579668716
      - 0.1787664161529392
      - 0.17289076000452042
      - 0.17331341095268726
      - 0.17085384589154273
      - 0.17157694255001843
      - 0.18499084992799908
      - 0.18639203580096364
      - 0.1718540448928252
      - 0.17698042059782892
      - 0.17330883437534794
      - 0.16973951080581173
      - 0.17593642725842074
      - 0.17582539917202666
      - 0.17342831776477396
      - 0.17637162515893579
      - 0.1731052030227147
      - 0.16741212439956143
      - 0.16807280713692307
      - 0.17156278737820685
      - 0.17266517673851922
      - 0.16793017054442316
      - 0.17696813424117863
      - 0.17578210186911747
      - 0.16941877675708383
      - 0.1716116009047255
      - 0.17263441701652482
      - 0.17026663816068321
      - 0.17050211440073326
      - 0.16962233709637076
      - 0.1728046553907916
      - 0.16771553189028054
      - 0.17375493526924402
      - 0.18350611435016617
      - 0.17315577302360907
      - 0.17943849379662424
      - 0.1736613495158963
      - 0.1731212256127037
      val_loss:
      - 0.47547389566898346
      - 0.3910925177624449
      - 0.36193132388871163
      - 0.32331004145089537
      - 0.31093868042808026
      - 0.28943748015444726
      - 0.29704456496983767
      - 0.25486046471633017
      - 0.2529364920919761
      - 0.24774087977129966
      - 0.23135247360914946
      - 0.22410747490357608
      - 0.21683545608539134
      - 0.22818546032067388
      - 0.21794721123296767
      - 0.2388475261395797
      - 0.21246636379510164
      - 0.2065076023573056
      - 0.2050943449139595
      - 0.20637431275099516
      - 0.20433115388732404
      - 0.19655799551401287
      - 0.19721923663746566
      - 0.2098105886252597
      - 0.203340892738197
      - 0.19898584665497765
      - 0.19000263360794634
      - 0.21338812971953303
      - 0.19244756398256868
      - 0.1929508101893589
      - 0.1962040705839172
      - 0.19406298029934987
      - 0.21408713317941874
      - 0.18419339566025883
      - 0.18776367377722636
      - 0.18397216452285647
      - 0.18441745691234246
      - 0.220149077125825
      - 0.2038574970792979
      - 0.18307202775031328
      - 0.2027471243054606
      - 0.18828067556023598
      - 0.18706834712065756
      - 0.19154361553955823
      - 0.18367693515028805
      - 0.18425102368928492
      - 0.17996131908148527
      - 0.18821698921965435
      - 0.18905519979307428
      - 0.18365124356932938
      - 0.2536720186471939
      - 0.17794926918577403
      - 0.19472957885591313
      - 0.1837863672990352
      - 0.18078152212547138
      - 0.175515394017566
      - 0.1817138430196792
      - 0.18929999694228172
      - 0.1775084674009122
      - 0.18314735224703327
      - 0.18865245423512533
      - 0.1786546579678543
      - 0.1795898994896561
      - 0.17550344386836514
      - 0.17489686358021572
      - 0.1759599082870409
      - 0.17563654511468485
      - 0.1757256879354827
      - 0.1785948463366367
      - 0.17864089913200587
      - 0.1830079536885023
      - 0.1749028250342235
      - 0.1822389992303215
      - 0.17846990621183068
      - 0.18986667884746566
      - 0.17356127948733047
      - 0.17718140635406598
      - 0.17392969579668716
      - 0.1787664161529392
      - 0.17289076000452042
      - 0.17331341095268726
      - 0.17085384589154273
      - 0.17157694255001843
      - 0.18499084992799908
      - 0.18639203580096364
      - 0.1718540448928252
      - 0.17698042059782892
      - 0.17330883437534794
      - 0.16973951080581173
      - 0.17593642725842074
      - 0.17582539917202666
      - 0.17342831776477396
      - 0.17637162515893579
      - 0.1731052030227147
      - 0.16741212439956143
      - 0.16807280713692307
      - 0.17156278737820685
      - 0.17266517673851922
      - 0.16793017054442316
      - 0.17696813424117863
      - 0.19805677980184555
      - 0.16941877675708383
      - 0.1716116009047255
      - 0.17263441701652482
      - 0.17026663816068321
      - 0.17050211440073326
      - 0.16962233709637076
      - 0.1728046553907916
      - 0.16771553189028054
      - 0.17375493526924402
      - 0.18350611435016617
      - 0.17315577302360907
      - 0.17943849379662424
      - 0.1736613495158963
      - 0.1731212256127037
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_2/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.3/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.3
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.21853387056059126
      NRMSE: 0.5132788457067649
      mean_wQuantileLoss: 0.18645057767325843
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5636352000292391
      - 0.3722867185715586
      - 0.34219089848920703
      - 0.3275994814466685
      - 0.32216477731708437
      - 0.31265018275007606
      - 0.3146523117320612
      - 0.3058299684198573
      - 0.30494881549384445
      - 0.3031361704925075
      - 0.2999140425818041
      - 0.2980858702212572
      - 0.2904886973556131
      - 0.2990175698651001
      - 0.3006853003753349
      - 0.3006353103555739
      - 0.29225149331614375
      - 0.29295107698999345
      - 0.2990378348622471
      - 0.295109766186215
      - 0.28677085030358285
      - 0.2884712959639728
      - 0.2932937312871218
      - 0.29602668317966163
      - 0.2914834078401327
      - 0.2902294158702716
      - 0.2927335581043735
      - 0.2906328982207924
      - 0.29275315208360553
      - 0.2931189958471805
      - 0.2950143871130422
      - 0.2896756597328931
      - 0.2897540177218616
      - 0.2869199289707467
      - 0.28560560184996575
      - 0.29224644624628127
      - 0.28834272432141006
      - 0.2818223452195525
      - 0.28502248192671686
      - 0.2842855644412339
      - 0.28191315615549684
      - 0.2819905864307657
      - 0.28178089449647814
      - 0.28338384779635817
      - 0.27926901064347476
      - 0.2823820795165375
      - 0.27948746841866523
      - 0.2808437591884285
      - 0.2848691593389958
      - 0.2798612454207614
      - 0.2807279695989564
      - 0.2808465549023822
      - 0.28573614032939076
      - 0.2774922288954258
      - 0.27900962019339204
      - 0.28205022297333926
      - 0.28000186278950423
      - 0.27691841311752796
      - 0.28246228815987706
      - 0.27710426796693355
      - 0.27871477126609534
      - 0.278252707910724
      - 0.2805494893109426
      - 0.2778463934082538
      - 0.2813996927579865
      - 0.2737759945448488
      - 0.2740389493992552
      - 0.27057320659514517
      - 0.27328781969845295
      - 0.2820037059718743
      - 0.27176493033766747
      val_loss:
      - 0.326729878783226
      - 0.3722867185715586
      - 0.34219089848920703
      - 0.3275994814466685
      - 0.32216477731708437
      - 0.31265018275007606
      - 0.3146523117320612
      - 0.3058299684198573
      - 0.30494881549384445
      - 0.3031361704925075
      - 0.2999140425818041
      - 0.2980858702212572
      - 0.2904886973556131
      - 0.2990175698651001
      - 0.3006853003753349
      - 0.3006353103555739
      - 0.29225149331614375
      - 0.29295107698999345
      - 0.2990378348622471
      - 0.295109766186215
      - 0.28677085030358285
      - 0.2884712959639728
      - 0.2932937312871218
      - 0.29602668317966163
      - 0.2914834078401327
      - 0.2902294158702716
      - 0.2927335581043735
      - 0.2906328982207924
      - 0.29275315208360553
      - 0.2931189958471805
      - 0.2950143871130422
      - 0.2896756597328931
      - 0.2897540177218616
      - 0.2869199289707467
      - 0.28560560184996575
      - 0.29224644624628127
      - 0.28834272432141006
      - 0.2818223452195525
      - 0.28502248192671686
      - 0.2842855644412339
      - 0.28191315615549684
      - 0.2819905864307657
      - 0.28178089449647814
      - 0.28338384779635817
      - 0.27926901064347476
      - 0.2823820795165375
      - 0.27948746841866523
      - 0.2808437591884285
      - 0.2848691593389958
      - 0.2798612454207614
      - 0.2299630045890808
      - 0.2808465549023822
      - 0.28573614032939076
      - 0.2774922288954258
      - 0.27900962019339204
      - 0.28205022297333926
      - 0.28000186278950423
      - 0.27691841311752796
      - 0.28246228815987706
      - 0.27710426796693355
      - 0.27871477126609534
      - 0.278252707910724
      - 0.2805494893109426
      - 0.2778463934082538
      - 0.2813996927579865
      - 0.2737759945448488
      - 0.2740389493992552
      - 0.27057320659514517
      - 0.27328781969845295
      - 0.2820037059718743
      - 0.27176493033766747
  task_id: 3
