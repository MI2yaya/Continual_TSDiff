continual_learning_setup:
  method: naive
  method_params: {}
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_2/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_naive/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.45412579186959817
      NRMSE: 0.8136339093912018
      mean_wQuantileLoss: 0.34858897277180173
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      - 116
      - 117
      - 118
      - 119
      - 120
      - 121
      train_loss:
      - 0.4006321254419163
      - 0.2694685540627688
      - 0.23361991660203785
      - 0.21881372644566
      - 0.212460991286207
      - 0.19915185746503994
      - 0.19461742421844974
      - 0.1934365319320932
      - 0.18920978863025084
      - 0.18202639260562137
      - 0.18477583886124194
      - 0.18984444288071245
      - 0.18506479787174612
      - 0.1805832384270616
      - 0.17956681235227734
      - 0.17440354719292372
      - 0.1763247141498141
      - 0.1776610271190293
      - 0.17221455194521695
      - 0.1732047685654834
      - 0.175879996153526
      - 0.17555733275366947
      - 0.1720327028306201
      - 0.17321994854137301
      - 0.17661520704859868
      - 0.1712834969512187
      - 0.1704743235022761
      - 0.16745023027760908
      - 0.16862326033879071
      - 0.1673574876622297
      - 0.16836266277823597
      - 0.16532702674157917
      - 0.17289722198620439
      - 0.16751656244741753
      - 0.1674166494049132
      - 0.16859298705821857
      - 0.1653545874869451
      - 0.16652921942295507
      - 0.16716273687779903
      - 0.165443018253427
      - 0.16340703860623762
      - 0.16349778196308762
      - 0.16179078869754449
      - 0.16200067865429446
      - 0.16266277484828606
      - 0.16067875229055062
      - 0.16160162357846275
      - 0.16213187493849546
      - 0.16426431969739497
      - 0.15913328644819558
      - 0.16137075319420546
      - 0.16634397127199918
      - 0.16200541955186054
      - 0.16106058232253417
      - 0.16312354599358514
      - 0.15641924244118854
      - 0.16094426182098687
      - 0.1579966708086431
      - 0.15958010405302048
      - 0.1621346488245763
      - 0.15702557074837387
      - 0.159019211307168
      - 0.15684602205874398
      - 0.1611978476285003
      - 0.1597351438831538
      - 0.15952103480231017
      - 0.15589558571809903
      - 0.15514477394754067
      - 0.15675083897076547
      - 0.15994588151806965
      - 0.15726504166377708
      - 0.1588915134780109
      - 0.15778962295735255
      - 0.160574117093347
      - 0.1588028001715429
      - 0.15267971815774217
      - 0.1576553518534638
      - 0.16115629847627133
      - 0.1554305988829583
      - 0.155225318914745
      - 0.16186953941360116
      - 0.15730804076883942
      - 0.15420546144014224
      - 0.1564766758820042
      - 0.15496923279715702
      - 0.15300429903436452
      - 0.15538203559117392
      - 0.15611812827410176
      - 0.15301788866054267
      - 0.1562325495760888
      - 0.1519254114245996
      - 0.15209190151654184
      - 0.15039543760940433
      - 0.1534302546060644
      - 0.15553261630702764
      - 0.15530241501983255
      - 0.15333140816073865
      - 0.1526712688500993
      - 0.15433114126790315
      - 0.15483793162275106
      - 0.1532579510821961
      - 0.1562173513812013
      - 0.1528444026480429
      - 0.15441703208489344
      - 0.15208201145287603
      - 0.15020168747287244
      - 0.1532680649543181
      - 0.1562648772378452
      - 0.15327668364625424
      - 0.15143097518011928
      - 0.15321688837138936
      - 0.1515615672688
      - 0.1499081162037328
      - 0.15257400227710605
      - 0.15524789737537503
      - 0.15366310806712136
      - 0.14885800599586219
      - 0.1535938114975579
      - 0.14998581766849384
      - 0.14861862431280315
      - 0.14996628550579771
      val_loss:
      - 0.28274297118186953
      - 0.2694685540627688
      - 0.23361991660203785
      - 0.21881372644566
      - 0.212460991286207
      - 0.19915185746503994
      - 0.19461742421844974
      - 0.1934365319320932
      - 0.18920978863025084
      - 0.18202639260562137
      - 0.18477583886124194
      - 0.18984444288071245
      - 0.18506479787174612
      - 0.1805832384270616
      - 0.17956681235227734
      - 0.17440354719292372
      - 0.1763247141498141
      - 0.1776610271190293
      - 0.17221455194521695
      - 0.1732047685654834
      - 0.175879996153526
      - 0.17555733275366947
      - 0.1720327028306201
      - 0.17321994854137301
      - 0.17661520704859868
      - 0.1712834969512187
      - 0.1704743235022761
      - 0.16745023027760908
      - 0.16862326033879071
      - 0.1673574876622297
      - 0.16836266277823597
      - 0.16532702674157917
      - 0.17289722198620439
      - 0.16751656244741753
      - 0.1674166494049132
      - 0.16859298705821857
      - 0.1653545874869451
      - 0.16652921942295507
      - 0.16716273687779903
      - 0.165443018253427
      - 0.16340703860623762
      - 0.16349778196308762
      - 0.16179078869754449
      - 0.16200067865429446
      - 0.16266277484828606
      - 0.16067875229055062
      - 0.16160162357846275
      - 0.16213187493849546
      - 0.16426431969739497
      - 0.15913328644819558
      - 0.15910445600748063
      - 0.16634397127199918
      - 0.16200541955186054
      - 0.16106058232253417
      - 0.16312354599358514
      - 0.15641924244118854
      - 0.16094426182098687
      - 0.1579966708086431
      - 0.15958010405302048
      - 0.1621346488245763
      - 0.15702557074837387
      - 0.159019211307168
      - 0.15684602205874398
      - 0.1611978476285003
      - 0.1597351438831538
      - 0.15952103480231017
      - 0.15589558571809903
      - 0.15514477394754067
      - 0.15675083897076547
      - 0.15994588151806965
      - 0.15726504166377708
      - 0.1588915134780109
      - 0.15778962295735255
      - 0.160574117093347
      - 0.1588028001715429
      - 0.15267971815774217
      - 0.1576553518534638
      - 0.16115629847627133
      - 0.1554305988829583
      - 0.155225318914745
      - 0.16186953941360116
      - 0.15730804076883942
      - 0.15420546144014224
      - 0.1564766758820042
      - 0.15496923279715702
      - 0.15300429903436452
      - 0.15538203559117392
      - 0.15611812827410176
      - 0.15301788866054267
      - 0.1562325495760888
      - 0.1519254114245996
      - 0.15209190151654184
      - 0.15039543760940433
      - 0.1534302546060644
      - 0.15553261630702764
      - 0.15530241501983255
      - 0.15333140816073865
      - 0.1526712688500993
      - 0.15433114126790315
      - 0.15483793162275106
      - 0.1428369849920273
      - 0.1562173513812013
      - 0.1528444026480429
      - 0.15441703208489344
      - 0.15208201145287603
      - 0.15020168747287244
      - 0.1532680649543181
      - 0.1562648772378452
      - 0.15327668364625424
      - 0.15143097518011928
      - 0.15321688837138936
      - 0.1515615672688
      - 0.1499081162037328
      - 0.15257400227710605
      - 0.15524789737537503
      - 0.15366310806712136
      - 0.14885800599586219
      - 0.1535938114975579
      - 0.14998581766849384
      - 0.14861862431280315
      - 0.14996628550579771
  task_id: 1
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_2/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_naive/task_2_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.20893629695398538
      NRMSE: 0.7295071372709754
      mean_wQuantileLoss: 0.16951262432392425
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5005803928943351
      - 0.3323619938455522
      - 0.3013159637339413
      - 0.26099690271075815
      - 0.22860551613848656
      - 0.21657406887970865
      - 0.19873245910275728
      - 0.21090626361547038
      - 0.2115443540387787
      - 0.18310762388864532
      - 0.18420180608518422
      - 0.17472436442039907
      - 0.1721846889704466
      - 0.18070821347646415
      - 0.17552075575804338
      - 0.17826252174563706
      - 0.1676602985826321
      - 0.167074287077412
      - 0.16597506403923035
      - 0.16509772470453754
      - 0.16623711836291477
      - 0.16012672119541094
      - 0.15695779642555863
      - 0.15967977710533887
      - 0.1595603347523138
      - 0.1598142801085487
      - 0.1644766021054238
      - 0.17002762854099274
      - 0.1652327214833349
      - 0.15787968639051542
      - 0.15299492183839902
      - 0.15755974617786705
      - 0.16149399272399023
      - 0.15802092658123001
      - 0.18052881862968206
      - 0.1569980988278985
      - 0.15056063706288114
      - 0.14993348310235888
      - 0.14938242023345083
      - 0.15273692307528108
      - 0.15328741946723312
      - 0.15224346006289124
      - 0.1530807102099061
      - 0.15221622510580346
      - 0.1579290372901596
      - 0.150387704372406
      - 0.14754341955995187
      - 0.15003065491328016
      - 0.15658259275369346
      - 0.14772851089946926
      - 0.14566100446972996
      - 0.1498174670850858
      - 0.15289532335009426
      - 0.14641175454016775
      - 0.1455558721208945
      - 0.14981796249048784
      - 0.14922972861677408
      - 0.14904860826209188
      - 0.14683773188153282
      - 0.14143236936070025
      - 0.14866490702843294
      - 0.14648470311658457
      - 0.15155501768458635
      - 0.14738222252344713
      - 0.14672376634553075
      - 0.15238371852319688
      - 0.1439570343354717
      - 0.14354067528620362
      - 0.153075187059585
      - 0.14522697305073962
      - 0.1517393519752659
      val_loss:
      - 0.276515930891037
      - 0.3323619938455522
      - 0.3013159637339413
      - 0.26099690271075815
      - 0.22860551613848656
      - 0.21657406887970865
      - 0.19873245910275728
      - 0.21090626361547038
      - 0.2115443540387787
      - 0.18310762388864532
      - 0.18420180608518422
      - 0.17472436442039907
      - 0.1721846889704466
      - 0.18070821347646415
      - 0.17552075575804338
      - 0.17826252174563706
      - 0.1676602985826321
      - 0.167074287077412
      - 0.16597506403923035
      - 0.16509772470453754
      - 0.16623711836291477
      - 0.16012672119541094
      - 0.15695779642555863
      - 0.15967977710533887
      - 0.1595603347523138
      - 0.1598142801085487
      - 0.1644766021054238
      - 0.17002762854099274
      - 0.1652327214833349
      - 0.15787968639051542
      - 0.15299492183839902
      - 0.15755974617786705
      - 0.16149399272399023
      - 0.15802092658123001
      - 0.18052881862968206
      - 0.1569980988278985
      - 0.15056063706288114
      - 0.14993348310235888
      - 0.14938242023345083
      - 0.15273692307528108
      - 0.15328741946723312
      - 0.15224346006289124
      - 0.1530807102099061
      - 0.15221622510580346
      - 0.1579290372901596
      - 0.150387704372406
      - 0.14754341955995187
      - 0.15003065491328016
      - 0.15658259275369346
      - 0.14772851089946926
      - 0.12160174548625946
      - 0.1498174670850858
      - 0.15289532335009426
      - 0.14641175454016775
      - 0.1455558721208945
      - 0.14981796249048784
      - 0.14922972861677408
      - 0.14904860826209188
      - 0.14683773188153282
      - 0.14143236936070025
      - 0.14866490702843294
      - 0.14648470311658457
      - 0.15155501768458635
      - 0.14738222252344713
      - 0.14672376634553075
      - 0.15238371852319688
      - 0.1439570343354717
      - 0.14354067528620362
      - 0.153075187059585
      - 0.14522697305073962
      - 0.1517393519752659
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_2/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_naive/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.20779314264447957
      NRMSE: 0.4834798811803253
      mean_wQuantileLoss: 0.17138210996302072
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      - 116
      - 117
      - 118
      - 119
      - 120
      - 121
      - 122
      - 123
      - 124
      - 125
      - 126
      - 127
      - 128
      - 129
      - 130
      - 131
      - 132
      - 133
      - 134
      - 135
      - 136
      - 137
      - 138
      - 139
      - 140
      - 141
      - 142
      - 143
      - 144
      - 145
      - 146
      - 147
      - 148
      - 149
      - 150
      - 151
      - 152
      - 153
      - 154
      - 155
      - 156
      - 157
      - 158
      - 159
      - 160
      - 161
      - 162
      - 163
      - 164
      - 165
      - 166
      - 167
      - 168
      - 169
      - 170
      - 171
      train_loss:
      - 0.4827414443716407
      - 0.32580841588787735
      - 0.3121627309592441
      - 0.30313370388466865
      - 0.29301184706855565
      - 0.28579909133259207
      - 0.28880120743997395
      - 0.2851262999465689
      - 0.27684955671429634
      - 0.27881853887811303
      - 0.2753988185431808
      - 0.27316783368587494
      - 0.26892503269482404
      - 0.2690091640688479
      - 0.2689034310169518
      - 0.2575303493067622
      - 0.2586435421835631
      - 0.2628583770710975
      - 0.26807642390485853
      - 0.2558268294669688
      - 0.25349311903119087
      - 0.2573841121047735
      - 0.2612198427086696
      - 0.25962424150202423
      - 0.2527032612124458
      - 0.255941741168499
      - 0.2591476042289287
      - 0.2587266367627308
      - 0.2574633347103372
      - 0.25134033360518515
      - 0.2510285656899214
      - 0.25091409764718264
      - 0.24876398069318384
      - 0.24999098817352206
      - 0.24664831964764744
      - 0.2496610525995493
      - 0.25349367456510663
      - 0.24591090600006282
      - 0.24973412067629397
      - 0.24839332175906748
      - 0.2499840639065951
      - 0.2449518624925986
      - 0.24498291523195803
      - 0.24481006409041584
      - 0.24359235377050936
      - 0.247294201515615
      - 0.2500316050136462
      - 0.24468004831578583
      - 0.24226537661161274
      - 0.24922783847432584
      - 0.2464034945005551
      - 0.24371319299098104
      - 0.24431296868715435
      - 0.24394573899917305
      - 0.2420769575983286
      - 0.2429883659351617
      - 0.24492298369295895
      - 0.24777784827165306
      - 0.24298320803791285
      - 0.24001119134481996
      - 0.23773351754061878
      - 0.24673762323800474
      - 0.2422417252091691
      - 0.23936573951505125
      - 0.24144351517315954
      - 0.24098836665507406
      - 0.23684599192347378
      - 0.24339456239249557
      - 0.2418173934565857
      - 0.23279920441564173
      - 0.23991733230650425
      - 0.235848005162552
      - 0.24138075090013444
      - 0.23814975656569004
      - 0.2375546763651073
      - 0.23311540752183646
      - 0.23507875902578235
      - 0.23633251781575382
      - 0.23534748051315546
      - 0.2335662910481915
      - 0.23627676453907043
      - 0.23421024635899812
      - 0.23454856488388032
      - 0.23778938315808773
      - 0.239735868293792
      - 0.23693770088721067
      - 0.2385785486549139
      - 0.23782457830384374
      - 0.23776319494936615
      - 0.22939511924050748
      - 0.23476712952833623
      - 0.23912997031584382
      - 0.23337146418634802
      - 0.23557268059812486
      - 0.23401998938061297
      - 0.2308994565391913
      - 0.2368060238659382
      - 0.23763852112460881
      - 0.2326717380201444
      - 0.23151098284870386
      - 0.23277670692186803
      - 0.23789066332392395
      - 0.22981704387348145
      - 0.23381261655595154
      - 0.24013383779674768
      - 0.22924565756693482
      - 0.2316554996650666
      - 0.23571869800798595
      - 0.22947454708628356
      - 0.2296086736023426
      - 0.23467054776847363
      - 0.2319953921250999
      - 0.23283452820032835
      - 0.2311822168994695
      - 0.23481276992242783
      - 0.2286002926994115
      - 0.22801621130201966
      - 0.22877535212319344
      - 0.22816080844495445
      - 0.2337562182219699
      - 0.2308680535061285
      - 0.2302975298371166
      - 0.23561494692694396
      - 0.228455072036013
      - 0.22772438591346145
      - 0.23560947633814067
      - 0.22974542807787657
      - 0.2299639325356111
      - 0.2267991116968915
      - 0.2267651786096394
      - 0.23093408718705177
      - 0.2310608457773924
      - 0.2272024662233889
      - 0.2338878153823316
      - 0.2272559111006558
      - 0.2280409384984523
      - 0.23232626472599804
      - 0.2269433360779658
      - 0.23680865566711873
      - 0.22782926948275417
      - 0.23323599237482995
      - 0.23047464154660702
      - 0.23327526240609586
      - 0.22056648950092494
      - 0.2323591965250671
      - 0.2309480020776391
      - 0.2302108908770606
      - 0.22999979672022164
      - 0.22701226884964854
      - 0.2271636154036969
      - 0.22679171385243535
      - 0.2229525587754324
      - 0.2251530244247988
      - 0.22935910569503903
      - 0.22603369574062526
      - 0.2316386456368491
      - 0.23142181348521262
      - 0.2250083943363279
      - 0.23187158873770386
      - 0.22710867191199213
      - 0.22719300829339772
      - 0.22342341317562386
      - 0.22531320003326982
      - 0.22121151717146859
      - 0.22774491307791322
      - 0.22608077782206237
      - 0.22510147700086236
      - 0.2289561638608575
      - 0.22672714455984533
      - 0.22878319025039673
      - 0.2239724559476599
      val_loss:
      - 0.3258406385779381
      - 0.32580841588787735
      - 0.3121627309592441
      - 0.30313370388466865
      - 0.29301184706855565
      - 0.28579909133259207
      - 0.28880120743997395
      - 0.2851262999465689
      - 0.27684955671429634
      - 0.27881853887811303
      - 0.2753988185431808
      - 0.27316783368587494
      - 0.26892503269482404
      - 0.2690091640688479
      - 0.2689034310169518
      - 0.2575303493067622
      - 0.2586435421835631
      - 0.2628583770710975
      - 0.26807642390485853
      - 0.2558268294669688
      - 0.25349311903119087
      - 0.2573841121047735
      - 0.2612198427086696
      - 0.25962424150202423
      - 0.2527032612124458
      - 0.255941741168499
      - 0.2591476042289287
      - 0.2587266367627308
      - 0.2574633347103372
      - 0.25134033360518515
      - 0.2510285656899214
      - 0.25091409764718264
      - 0.24876398069318384
      - 0.24999098817352206
      - 0.24664831964764744
      - 0.2496610525995493
      - 0.25349367456510663
      - 0.24591090600006282
      - 0.24973412067629397
      - 0.24839332175906748
      - 0.2499840639065951
      - 0.2449518624925986
      - 0.24498291523195803
      - 0.24481006409041584
      - 0.24359235377050936
      - 0.247294201515615
      - 0.2500316050136462
      - 0.24468004831578583
      - 0.24226537661161274
      - 0.24922783847432584
      - 0.2614908546209335
      - 0.24371319299098104
      - 0.24431296868715435
      - 0.24394573899917305
      - 0.2420769575983286
      - 0.2429883659351617
      - 0.24492298369295895
      - 0.24777784827165306
      - 0.24298320803791285
      - 0.24001119134481996
      - 0.23773351754061878
      - 0.24673762323800474
      - 0.2422417252091691
      - 0.23936573951505125
      - 0.24144351517315954
      - 0.24098836665507406
      - 0.23684599192347378
      - 0.24339456239249557
      - 0.2418173934565857
      - 0.23279920441564173
      - 0.23991733230650425
      - 0.235848005162552
      - 0.24138075090013444
      - 0.23814975656569004
      - 0.2375546763651073
      - 0.23311540752183646
      - 0.23507875902578235
      - 0.23633251781575382
      - 0.23534748051315546
      - 0.2335662910481915
      - 0.23627676453907043
      - 0.23421024635899812
      - 0.23454856488388032
      - 0.23778938315808773
      - 0.239735868293792
      - 0.23693770088721067
      - 0.2385785486549139
      - 0.23782457830384374
      - 0.23776319494936615
      - 0.22939511924050748
      - 0.23476712952833623
      - 0.23912997031584382
      - 0.23337146418634802
      - 0.23557268059812486
      - 0.23401998938061297
      - 0.2308994565391913
      - 0.2368060238659382
      - 0.23763852112460881
      - 0.2326717380201444
      - 0.23151098284870386
      - 0.24251313805580138
      - 0.23789066332392395
      - 0.22981704387348145
      - 0.23381261655595154
      - 0.24013383779674768
      - 0.22924565756693482
      - 0.2316554996650666
      - 0.23571869800798595
      - 0.22947454708628356
      - 0.2296086736023426
      - 0.23467054776847363
      - 0.2319953921250999
      - 0.23283452820032835
      - 0.2311822168994695
      - 0.23481276992242783
      - 0.2286002926994115
      - 0.22801621130201966
      - 0.22877535212319344
      - 0.22816080844495445
      - 0.2337562182219699
      - 0.2308680535061285
      - 0.2302975298371166
      - 0.23561494692694396
      - 0.228455072036013
      - 0.22772438591346145
      - 0.23560947633814067
      - 0.22974542807787657
      - 0.2299639325356111
      - 0.2267991116968915
      - 0.2267651786096394
      - 0.23093408718705177
      - 0.2310608457773924
      - 0.2272024662233889
      - 0.2338878153823316
      - 0.2272559111006558
      - 0.2280409384984523
      - 0.23232626472599804
      - 0.2269433360779658
      - 0.23680865566711873
      - 0.22782926948275417
      - 0.23323599237482995
      - 0.23047464154660702
      - 0.23327526240609586
      - 0.22056648950092494
      - 0.2323591965250671
      - 0.2309480020776391
      - 0.2302108908770606
      - 0.22999979672022164
      - 0.22701226884964854
      - 0.2271636154036969
      - 0.20031278133392333
      - 0.2229525587754324
      - 0.2251530244247988
      - 0.22935910569503903
      - 0.22603369574062526
      - 0.2316386456368491
      - 0.23142181348521262
      - 0.2250083943363279
      - 0.23187158873770386
      - 0.22710867191199213
      - 0.22719300829339772
      - 0.22342341317562386
      - 0.22531320003326982
      - 0.22121151717146859
      - 0.22774491307791322
      - 0.22608077782206237
      - 0.22510147700086236
      - 0.2289561638608575
      - 0.22672714455984533
      - 0.22878319025039673
      - 0.2239724559476599
  task_id: 3
