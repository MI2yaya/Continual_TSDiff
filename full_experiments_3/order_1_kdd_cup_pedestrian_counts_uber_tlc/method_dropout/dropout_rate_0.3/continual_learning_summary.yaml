continual_learning_setup:
  method: dropout
  method_params:
    dropout_rate: 0.3
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.3/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.3
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3471901872515738
      NRMSE: 0.7219090814548905
      mean_wQuantileLoss: 0.27630556349199226
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.44960841990541667
      - 0.33814421109855175
      - 0.29570720938500017
      - 0.26982263138052076
      - 0.2566576609387994
      - 0.2440062443492934
      - 0.22894909442402422
      - 0.23021773365326226
      - 0.22008464497048408
      - 0.22288498550187796
      - 0.21220055170124397
      - 0.21657239296473563
      - 0.2089227094547823
      - 0.21159168414305896
      - 0.20652659697225317
      - 0.2062044502235949
      - 0.2062598638003692
      - 0.20171198283787817
      - 0.20477298100013286
      - 0.20665782154537737
      - 0.20506122114602476
      - 0.1995683794375509
      - 0.1962168132304214
      - 0.1984665278578177
      - 0.19665006827563047
      - 0.20414136780891567
      - 0.19854031538125128
      - 0.20343707758001983
      - 0.19712774630170316
      - 0.19629624055232853
      - 0.19160973839461803
      - 0.18864850554382429
      - 0.19269168266328052
      - 0.19745499244891107
      - 0.19492923317011446
      - 0.19373403827194124
      - 0.1946546623366885
      - 0.21121193503495306
      - 0.192074493621476
      - 0.20002214168198407
      - 0.19451844785362482
      - 0.1935588701744564
      - 0.1922989106969908
      - 0.1896759220981039
      - 0.1946532818255946
      - 0.19376597134396434
      - 0.18683260073885322
      - 0.19360838376451284
      - 0.18815659719984978
      - 0.18971556512406096
      - 0.19408558763097972
      - 0.18540479126386344
      - 0.18884056899696589
      - 0.1878681888920255
      - 0.19620642851805314
      - 0.19241093326127157
      - 0.19094473612494767
      - 0.18687457166379318
      - 0.1843614538665861
      - 0.1908085573813878
      - 0.1932255157153122
      - 0.1879951850278303
      - 0.18927755032200366
      - 0.18685263907536864
      - 0.1849085529684089
      - 0.1831166641204618
      - 0.19089663994964212
      - 0.18921522959135473
      - 0.18310035386821255
      - 0.18911956576630473
      - 0.19103121140506119
      val_loss:
      - 0.3116567671298981
      - 0.33814421109855175
      - 0.29570720938500017
      - 0.26982263138052076
      - 0.2566576609387994
      - 0.2440062443492934
      - 0.22894909442402422
      - 0.23021773365326226
      - 0.22008464497048408
      - 0.22288498550187796
      - 0.21220055170124397
      - 0.21657239296473563
      - 0.2089227094547823
      - 0.21159168414305896
      - 0.20652659697225317
      - 0.2062044502235949
      - 0.2062598638003692
      - 0.20171198283787817
      - 0.20477298100013286
      - 0.20665782154537737
      - 0.20506122114602476
      - 0.1995683794375509
      - 0.1962168132304214
      - 0.1984665278578177
      - 0.19665006827563047
      - 0.20414136780891567
      - 0.19854031538125128
      - 0.20343707758001983
      - 0.19712774630170316
      - 0.19629624055232853
      - 0.19160973839461803
      - 0.18864850554382429
      - 0.19269168266328052
      - 0.19745499244891107
      - 0.19492923317011446
      - 0.19373403827194124
      - 0.1946546623366885
      - 0.21121193503495306
      - 0.192074493621476
      - 0.20002214168198407
      - 0.19451844785362482
      - 0.1935588701744564
      - 0.1922989106969908
      - 0.1896759220981039
      - 0.1946532818255946
      - 0.19376597134396434
      - 0.18683260073885322
      - 0.19360838376451284
      - 0.18815659719984978
      - 0.18971556512406096
      - 0.17628166675567628
      - 0.18540479126386344
      - 0.18884056899696589
      - 0.1878681888920255
      - 0.19620642851805314
      - 0.19241093326127157
      - 0.19094473612494767
      - 0.18687457166379318
      - 0.1843614538665861
      - 0.1908085573813878
      - 0.1932255157153122
      - 0.1879951850278303
      - 0.18927755032200366
      - 0.18685263907536864
      - 0.1849085529684089
      - 0.1831166641204618
      - 0.19089663994964212
      - 0.18921522959135473
      - 0.18310035386821255
      - 0.18911956576630473
      - 0.19103121140506119
  task_id: 1
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.3/task_2_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.3
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.21409915282442646
      NRMSE: 0.7662311407303044
      mean_wQuantileLoss: 0.17799509354718313
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5226087511982769
      - 0.3675034763291478
      - 0.3711726599140093
      - 0.3352471557445824
      - 0.31696469034068286
      - 0.2716796420281753
      - 0.2691830445546657
      - 0.2630751603282988
      - 0.26453514432068914
      - 0.24224093498196453
      - 0.2386509811040014
      - 0.23487403988838196
      - 0.21851013810373843
      - 0.2187171466066502
      - 0.21452934038825333
      - 0.20949607342481613
      - 0.2138023265870288
      - 0.21149742766283453
      - 0.20314456697087735
      - 0.22911491082049906
      - 0.1961583506781608
      - 0.19976471096742898
      - 0.2135741984238848
      - 0.19699551298981532
      - 0.19781633431557566
      - 0.2147479499108158
      - 0.19448094203835353
      - 0.19485105207422748
      - 0.18610816251020879
      - 0.1879190910840407
      - 0.20048190257512033
      - 0.1875518841552548
      - 0.18936743296217173
      - 0.18268968182383105
      - 0.18602055648807436
      - 0.1931856513256207
      - 0.18721281655598432
      - 0.1872529269894585
      - 0.20163357257843018
      - 0.18238903564633802
      - 0.18045713688479736
      - 0.18371702259173617
      - 0.1844661101931706
      - 0.1810441791312769
      - 0.18522892094915733
      - 0.1790809520171024
      - 0.1855250063817948
      - 0.18415973405353725
      - 0.19094482384389266
      - 0.21172212559031323
      - 0.1787997495266609
      - 0.1843213001266122
      - 0.1775303593603894
      - 0.1797585249878466
      - 0.17844045348465443
      - 0.18278800998814404
      - 0.19236606894992292
      - 0.17871960037155077
      - 0.1823219567304477
      - 0.17937664967030287
      - 0.17998263332992792
      - 0.17737730412045494
      - 0.18831188516924158
      - 0.17701912653865293
      - 0.17824450152693316
      - 0.20248569321120158
      - 0.18030194728635252
      - 0.1793205657741055
      - 0.1736785628600046
      - 0.17411299335071817
      - 0.17700187704758719
      val_loss:
      - 0.3234754204750061
      - 0.3675034763291478
      - 0.3711726599140093
      - 0.3352471557445824
      - 0.31696469034068286
      - 0.2716796420281753
      - 0.2691830445546657
      - 0.2630751603282988
      - 0.26453514432068914
      - 0.24224093498196453
      - 0.2386509811040014
      - 0.23487403988838196
      - 0.21851013810373843
      - 0.2187171466066502
      - 0.21452934038825333
      - 0.20949607342481613
      - 0.2138023265870288
      - 0.21149742766283453
      - 0.20314456697087735
      - 0.22911491082049906
      - 0.1961583506781608
      - 0.19976471096742898
      - 0.2135741984238848
      - 0.19699551298981532
      - 0.19781633431557566
      - 0.2147479499108158
      - 0.19448094203835353
      - 0.19485105207422748
      - 0.18610816251020879
      - 0.1879190910840407
      - 0.20048190257512033
      - 0.1875518841552548
      - 0.18936743296217173
      - 0.18268968182383105
      - 0.18602055648807436
      - 0.1931856513256207
      - 0.18721281655598432
      - 0.1872529269894585
      - 0.20163357257843018
      - 0.18238903564633802
      - 0.18045713688479736
      - 0.18371702259173617
      - 0.1844661101931706
      - 0.1810441791312769
      - 0.18522892094915733
      - 0.1790809520171024
      - 0.1855250063817948
      - 0.18415973405353725
      - 0.19094482384389266
      - 0.21172212559031323
      - 0.16544747352600098
      - 0.1843213001266122
      - 0.1775303593603894
      - 0.1797585249878466
      - 0.17844045348465443
      - 0.18278800998814404
      - 0.19236606894992292
      - 0.17871960037155077
      - 0.1823219567304477
      - 0.17937664967030287
      - 0.17998263332992792
      - 0.17737730412045494
      - 0.18831188516924158
      - 0.17701912653865293
      - 0.17824450152693316
      - 0.20248569321120158
      - 0.18030194728635252
      - 0.1793205657741055
      - 0.1736785628600046
      - 0.17411299335071817
      - 0.17700187704758719
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.3/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.3
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.21664024058936837
      NRMSE: 0.5479192491440563
      mean_wQuantileLoss: 0.19266844596467086
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5781344301067293
      - 0.3773848426062614
      - 0.3445278445724398
      - 0.3316526575945318
      - 0.3180928338551894
      - 0.31056645419448614
      - 0.31056805106345564
      - 0.30462181312032044
      - 0.30738242028746754
      - 0.29981273389421403
      - 0.30237054475583136
      - 0.30120809376239777
      - 0.3002975630806759
      - 0.29643547802697867
      - 0.2995012120809406
      - 0.2949659910518676
      - 0.29702190042007715
      - 0.2925233024870977
      - 0.291478518396616
      - 0.29512001015245914
      - 0.29336190060712397
      - 0.29227219393942505
      - 0.2893461266066879
      - 0.29231571429409087
      - 0.28529110667295754
      - 0.28938717069104314
      - 0.2909420731011778
      - 0.28810489643365145
      - 0.29018221283331513
      - 0.2872139511164278
      - 0.28697585861664265
      - 0.29095394269097596
      - 0.28713236353360116
      - 0.28787140059284866
      - 0.28231745050288737
      - 0.28294523747172207
      - 0.28698707302100956
      - 0.28459796076640487
      - 0.2773985223611817
      - 0.2810804503969848
      - 0.28766965970862657
      - 0.28304587735328823
      - 0.28068213863298297
      - 0.2828817841364071
      - 0.2821178031153977
      - 0.2837511863326654
      - 0.28108867874834687
      - 0.2763685754034668
      - 0.28436681255698204
      - 0.27926580887287855
      - 0.28061304497532547
      - 0.27867263357620686
      - 0.2801869068061933
      - 0.27667331334669143
      - 0.2801903134677559
      - 0.2817672851961106
      - 0.274136254680343
      - 0.2737028057454154
      - 0.27319293189793825
      - 0.2751717616338283
      - 0.2750033640768379
      - 0.2776904753409326
      - 0.27755987260024995
      - 0.2715827700449154
      - 0.27465076732914895
      - 0.2748082127654925
      - 0.27362228266429156
      - 0.27323022729251534
      - 0.2730462949257344
      - 0.2737450258573517
      - 0.26836179476231337
      val_loss:
      - 0.32422306537628176
      - 0.3773848426062614
      - 0.3445278445724398
      - 0.3316526575945318
      - 0.3180928338551894
      - 0.31056645419448614
      - 0.31056805106345564
      - 0.30462181312032044
      - 0.30738242028746754
      - 0.29981273389421403
      - 0.30237054475583136
      - 0.30120809376239777
      - 0.3002975630806759
      - 0.29643547802697867
      - 0.2995012120809406
      - 0.2949659910518676
      - 0.29702190042007715
      - 0.2925233024870977
      - 0.291478518396616
      - 0.29512001015245914
      - 0.29336190060712397
      - 0.29227219393942505
      - 0.2893461266066879
      - 0.29231571429409087
      - 0.28529110667295754
      - 0.28938717069104314
      - 0.2909420731011778
      - 0.28810489643365145
      - 0.29018221283331513
      - 0.2872139511164278
      - 0.28697585861664265
      - 0.29095394269097596
      - 0.28713236353360116
      - 0.28787140059284866
      - 0.28231745050288737
      - 0.28294523747172207
      - 0.28698707302100956
      - 0.28459796076640487
      - 0.2773985223611817
      - 0.2810804503969848
      - 0.28766965970862657
      - 0.28304587735328823
      - 0.28068213863298297
      - 0.2828817841364071
      - 0.2821178031153977
      - 0.2837511863326654
      - 0.28108867874834687
      - 0.2763685754034668
      - 0.28436681255698204
      - 0.27926580887287855
      - 0.2451343297958374
      - 0.27867263357620686
      - 0.2801869068061933
      - 0.27667331334669143
      - 0.2801903134677559
      - 0.2817672851961106
      - 0.274136254680343
      - 0.2737028057454154
      - 0.27319293189793825
      - 0.2751717616338283
      - 0.2750033640768379
      - 0.2776904753409326
      - 0.27755987260024995
      - 0.2715827700449154
      - 0.27465076732914895
      - 0.2748082127654925
      - 0.27362228266429156
      - 0.27323022729251534
      - 0.2730462949257344
      - 0.2737450258573517
      - 0.26836179476231337
  task_id: 3
