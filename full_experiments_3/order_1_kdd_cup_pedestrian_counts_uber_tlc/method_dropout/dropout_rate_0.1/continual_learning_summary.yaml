continual_learning_setup:
  method: dropout
  method_params:
    dropout_rate: 0.1
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.1/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.1
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3300841182779551
      NRMSE: 0.6852521557099097
      mean_wQuantileLoss: 0.25745787782801965
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4287857364397496
      - 0.2981514282291755
      - 0.2678352314978838
      - 0.24824874789919704
      - 0.22523928235750645
      - 0.21725663030520082
      - 0.2130302080186084
      - 0.2084110361756757
      - 0.20217535831034184
      - 0.20251816883683205
      - 0.19349398801568896
      - 0.19347799447132275
      - 0.19485575472936034
      - 0.1895739254541695
      - 0.18746161059243605
      - 0.1856470945640467
      - 0.18392612907337025
      - 0.18925318971741945
      - 0.18704644491663203
      - 0.18301175633678213
      - 0.17829437518958002
      - 0.18421930586919188
      - 0.17711809714091942
      - 0.18073517398443073
      - 0.17974191845860332
      - 0.18009464582428336
      - 0.17764558078488335
      - 0.17419759882614017
      - 0.180448051658459
      - 0.17491315730148926
      - 0.1711892919265665
      - 0.1796481286874041
      - 0.17847180110402405
      - 0.18083220365224406
      - 0.17017615516670048
      - 0.1750664901919663
      - 0.17726597748696804
      - 0.17429109086515382
      - 0.175874165201094
      - 0.17439826996997
      - 0.16953203931916505
      - 0.17577694484498352
      - 0.1715458445250988
      - 0.1717302558827214
      - 0.16978566983016208
      - 0.1751915678032674
      - 0.17499049252364784
      - 0.1676499039749615
      - 0.16971689398633316
      - 0.17228919675108045
      - 0.17136646033031866
      - 0.17199417646043003
      - 0.17286613903706893
      - 0.16818194510415196
      - 0.16902324015973136
      - 0.1721236122539267
      - 0.16935040953103453
      - 0.16814629541477188
      - 0.16810284677194431
      - 0.1695089390850626
      - 0.16976536303991452
      - 0.15984802861930802
      - 0.1643809505039826
      - 0.16710058209719136
      - 0.17291110922815278
      - 0.16605716513004154
      - 0.1671190759516321
      - 0.16462977731134742
      - 0.16866428719367832
      - 0.16716125066159293
      - 0.16955155553296208
      val_loss:
      - 0.29132863879203796
      - 0.2981514282291755
      - 0.2678352314978838
      - 0.24824874789919704
      - 0.22523928235750645
      - 0.21725663030520082
      - 0.2130302080186084
      - 0.2084110361756757
      - 0.20217535831034184
      - 0.20251816883683205
      - 0.19349398801568896
      - 0.19347799447132275
      - 0.19485575472936034
      - 0.1895739254541695
      - 0.18746161059243605
      - 0.1856470945640467
      - 0.18392612907337025
      - 0.18925318971741945
      - 0.18704644491663203
      - 0.18301175633678213
      - 0.17829437518958002
      - 0.18421930586919188
      - 0.17711809714091942
      - 0.18073517398443073
      - 0.17974191845860332
      - 0.18009464582428336
      - 0.17764558078488335
      - 0.17419759882614017
      - 0.180448051658459
      - 0.17491315730148926
      - 0.1711892919265665
      - 0.1796481286874041
      - 0.17847180110402405
      - 0.18083220365224406
      - 0.17017615516670048
      - 0.1750664901919663
      - 0.17726597748696804
      - 0.17429109086515382
      - 0.175874165201094
      - 0.17439826996997
      - 0.16953203931916505
      - 0.17577694484498352
      - 0.1715458445250988
      - 0.1717302558827214
      - 0.16978566983016208
      - 0.1751915678032674
      - 0.17499049252364784
      - 0.1676499039749615
      - 0.16971689398633316
      - 0.17228919675108045
      - 0.14441848993301393
      - 0.17199417646043003
      - 0.17286613903706893
      - 0.16818194510415196
      - 0.16902324015973136
      - 0.1721236122539267
      - 0.16935040953103453
      - 0.16814629541477188
      - 0.16810284677194431
      - 0.1695089390850626
      - 0.16976536303991452
      - 0.15984802861930802
      - 0.1643809505039826
      - 0.16710058209719136
      - 0.17291110922815278
      - 0.16605716513004154
      - 0.1671190759516321
      - 0.16462977731134742
      - 0.16866428719367832
      - 0.16716125066159293
      - 0.16955155553296208
  task_id: 1
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.1/task_2_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.1
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.1972483388682821
      NRMSE: 0.6633423861344718
      mean_wQuantileLoss: 0.16134629461710934
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      - 116
      - 117
      - 118
      - 119
      - 120
      train_loss:
      - 0.5291981236077845
      - 0.3550493608927354
      - 0.30952727422118187
      - 0.2859106136020273
      - 0.266678674495779
      - 0.31394743430428207
      - 0.23078209161758423
      - 0.2162618855945766
      - 0.2295925549697131
      - 0.2204162793350406
      - 0.196241153171286
      - 0.19447536277584732
      - 0.18923543160781264
      - 0.19324579974636436
      - 0.1866914844722487
      - 0.18264449090929702
      - 0.17827068956103176
      - 0.1853122488828376
      - 0.18701364018488675
      - 0.1795642118086107
      - 0.17915482132229954
      - 0.18927817448275164
      - 0.17662361165275797
      - 0.18419116834411398
      - 0.18971148366108537
      - 0.17351339192828164
      - 0.1758586569922045
      - 0.167980169178918
      - 0.17081295774551108
      - 0.17441025451989844
      - 0.17150766646955162
      - 0.1710013062111102
      - 0.16510102833854035
      - 0.18176981870783493
      - 0.1626176442950964
      - 0.16444915597094223
      - 0.16692014707950875
      - 0.16184383985819295
      - 0.17348259291611612
      - 0.176548401650507
      - 0.16246304084779695
      - 0.16591021296335384
      - 0.17267153912689537
      - 0.16672486229799688
      - 0.16915473999688402
      - 0.1679839866119437
      - 0.16294472914887592
      - 0.15962759498506784
      - 0.16643932834267616
      - 0.16040804760996252
      - 0.1656850675935857
      - 0.1724497486720793
      - 0.16258490143809468
      - 0.16480134619632736
      - 0.1584231273154728
      - 0.16197695792652667
      - 0.15729071520036086
      - 0.15258729527704418
      - 0.16247431986266747
      - 0.16063190420391038
      - 0.16345687967259437
      - 0.15938315057428554
      - 0.15924876928329468
      - 0.1580022931448184
      - 0.1740170941920951
      - 0.17231169500155374
      - 0.15642824582755566
      - 0.15321219735778868
      - 0.1559516876586713
      - 0.15635638061212376
      - 0.15940454829251394
      - 0.15606580674648285
      - 0.15615423739654943
      - 0.15180132148088887
      - 0.15356175653869286
      - 0.15303864306770265
      - 0.15537518286146224
      - 0.15621031465707347
      - 0.1570585408480838
      - 0.15964170004008338
      - 0.1535528540262021
      - 0.15503856434952468
      - 0.15481394447851926
      - 0.15295930730644614
      - 0.15352126909419894
      - 0.14902028831420466
      - 0.15685442194808275
      - 0.15124939620727673
      - 0.1526051090331748
      - 0.15282597270561382
      - 0.15238643321208656
      - 0.15015228535048664
      - 0.15323101880494505
      - 0.15371142170624807
      - 0.1501305392012
      - 0.15496928052743897
      - 0.15381764562334865
      - 0.14850046491483226
      - 0.1536347572109662
      - 0.14629931858507916
      - 0.14907089120242745
      - 0.17766165878856555
      - 0.15734033501939848
      - 0.15510505792917684
      - 0.1503296989831142
      - 0.1499658773536794
      - 0.1532724694116041
      - 0.15506228915182874
      - 0.15208109514787793
      - 0.15332763514015824
      - 0.14876708830706775
      - 0.15047167317243293
      - 0.15720942412735894
      - 0.14668980962596834
      - 0.15323851158609614
      - 0.15256519860122353
      - 0.14828772778855637
      - 0.14815750165143982
      - 0.15055817761458457
      - 0.15093658183468506
      val_loss:
      - 0.33925002813339233
      - 0.3550493608927354
      - 0.30952727422118187
      - 0.2859106136020273
      - 0.266678674495779
      - 0.31394743430428207
      - 0.23078209161758423
      - 0.2162618855945766
      - 0.2295925549697131
      - 0.2204162793350406
      - 0.196241153171286
      - 0.19447536277584732
      - 0.18923543160781264
      - 0.19324579974636436
      - 0.1866914844722487
      - 0.18264449090929702
      - 0.17827068956103176
      - 0.1853122488828376
      - 0.18701364018488675
      - 0.1795642118086107
      - 0.17915482132229954
      - 0.18927817448275164
      - 0.17662361165275797
      - 0.18419116834411398
      - 0.18971148366108537
      - 0.17351339192828164
      - 0.1758586569922045
      - 0.167980169178918
      - 0.17081295774551108
      - 0.17441025451989844
      - 0.17150766646955162
      - 0.1710013062111102
      - 0.16510102833854035
      - 0.18176981870783493
      - 0.1626176442950964
      - 0.16444915597094223
      - 0.16692014707950875
      - 0.16184383985819295
      - 0.17348259291611612
      - 0.176548401650507
      - 0.16246304084779695
      - 0.16591021296335384
      - 0.17267153912689537
      - 0.16672486229799688
      - 0.16915473999688402
      - 0.1679839866119437
      - 0.16294472914887592
      - 0.15962759498506784
      - 0.16643932834267616
      - 0.16040804760996252
      - 0.1731937825679779
      - 0.1724497486720793
      - 0.16258490143809468
      - 0.16480134619632736
      - 0.1584231273154728
      - 0.16197695792652667
      - 0.15729071520036086
      - 0.15258729527704418
      - 0.16247431986266747
      - 0.16063190420391038
      - 0.16345687967259437
      - 0.15938315057428554
      - 0.15924876928329468
      - 0.1580022931448184
      - 0.1740170941920951
      - 0.17231169500155374
      - 0.15642824582755566
      - 0.15321219735778868
      - 0.1559516876586713
      - 0.15635638061212376
      - 0.15940454829251394
      - 0.15606580674648285
      - 0.15615423739654943
      - 0.15180132148088887
      - 0.15356175653869286
      - 0.15303864306770265
      - 0.15537518286146224
      - 0.15621031465707347
      - 0.1570585408480838
      - 0.15964170004008338
      - 0.1535528540262021
      - 0.15503856434952468
      - 0.15481394447851926
      - 0.15295930730644614
      - 0.15352126909419894
      - 0.14902028831420466
      - 0.15685442194808275
      - 0.15124939620727673
      - 0.1526051090331748
      - 0.15282597270561382
      - 0.15238643321208656
      - 0.15015228535048664
      - 0.15323101880494505
      - 0.15371142170624807
      - 0.1501305392012
      - 0.15496928052743897
      - 0.15381764562334865
      - 0.14850046491483226
      - 0.1536347572109662
      - 0.14629931858507916
      - 0.1903390809893608
      - 0.17766165878856555
      - 0.15734033501939848
      - 0.15510505792917684
      - 0.1503296989831142
      - 0.1499658773536794
      - 0.1532724694116041
      - 0.15506228915182874
      - 0.15208109514787793
      - 0.15332763514015824
      - 0.14876708830706775
      - 0.15047167317243293
      - 0.15720942412735894
      - 0.14668980962596834
      - 0.15323851158609614
      - 0.15256519860122353
      - 0.14828772778855637
      - 0.14815750165143982
      - 0.15055817761458457
      - 0.15093658183468506
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_dropout/dropout_rate_0.1/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.1
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.20732243540052928
      NRMSE: 0.506855428472518
      mean_wQuantileLoss: 0.17513418734972797
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5218988778069615
      - 0.359253945061937
      - 0.3270196483936161
      - 0.3117899829521775
      - 0.29718425206374377
      - 0.30137889401521534
      - 0.29546813084743917
      - 0.2891037230147049
      - 0.2847505066310987
      - 0.2885824233526364
      - 0.2779094063444063
      - 0.27550271269865334
      - 0.28147070098202676
      - 0.2725315534044057
      - 0.2752781657036394
      - 0.27505436982028186
      - 0.27162527362816036
      - 0.2692956579849124
      - 0.26607634918764234
      - 0.26862152852118015
      - 0.2676765605574474
      - 0.2670259439619258
      - 0.269531125202775
      - 0.2711183713981882
      - 0.26957632636185735
      - 0.26594227412715554
      - 0.26032866653986275
      - 0.26403227634727955
      - 0.2667044891277328
      - 0.2648954715114087
      - 0.25634087540674955
      - 0.2582125187618658
      - 0.25827580015175045
      - 0.25810128485318273
      - 0.26303246850147843
      - 0.25916851649526507
      - 0.26095281064044684
      - 0.2540250214515254
      - 0.26061228255275637
      - 0.2594910649349913
      - 0.26018984825350344
      - 0.26092830358538777
      - 0.2584387998795137
      - 0.2537071427796036
      - 0.2524894009111449
      - 0.2502763186348602
      - 0.2594617772847414
      - 0.2538022978696972
      - 0.25248353951610625
      - 0.2501434616278857
      - 0.25107828830368817
      - 0.2534051174297929
      - 0.25813600968103856
      - 0.2527619192842394
      - 0.2518123284680769
      - 0.2617337160045281
      - 0.24658572813495994
      - 0.2535064498661086
      - 0.25555566675029695
      - 0.25278929248452187
      - 0.2518988121300936
      - 0.2561674624448642
      - 0.25259726820513606
      - 0.2491827841149643
      - 0.24825183663051575
      - 0.25617701141163707
      - 0.24908969027455896
      - 0.2511508275056258
      - 0.2526527186855674
      - 0.24526601389516145
      - 0.2512123433407396
      val_loss:
      - 0.36195887327194215
      - 0.359253945061937
      - 0.3270196483936161
      - 0.3117899829521775
      - 0.29718425206374377
      - 0.30137889401521534
      - 0.29546813084743917
      - 0.2891037230147049
      - 0.2847505066310987
      - 0.2885824233526364
      - 0.2779094063444063
      - 0.27550271269865334
      - 0.28147070098202676
      - 0.2725315534044057
      - 0.2752781657036394
      - 0.27505436982028186
      - 0.27162527362816036
      - 0.2692956579849124
      - 0.26607634918764234
      - 0.26862152852118015
      - 0.2676765605574474
      - 0.2670259439619258
      - 0.269531125202775
      - 0.2711183713981882
      - 0.26957632636185735
      - 0.26594227412715554
      - 0.26032866653986275
      - 0.26403227634727955
      - 0.2667044891277328
      - 0.2648954715114087
      - 0.25634087540674955
      - 0.2582125187618658
      - 0.25827580015175045
      - 0.25810128485318273
      - 0.26303246850147843
      - 0.25916851649526507
      - 0.26095281064044684
      - 0.2540250214515254
      - 0.26061228255275637
      - 0.2594910649349913
      - 0.26018984825350344
      - 0.26092830358538777
      - 0.2584387998795137
      - 0.2537071427796036
      - 0.2524894009111449
      - 0.2502763186348602
      - 0.2594617772847414
      - 0.2538022978696972
      - 0.25248353951610625
      - 0.2501434616278857
      - 0.1730572260916233
      - 0.2534051174297929
      - 0.25813600968103856
      - 0.2527619192842394
      - 0.2518123284680769
      - 0.2617337160045281
      - 0.24658572813495994
      - 0.2535064498661086
      - 0.25555566675029695
      - 0.25278929248452187
      - 0.2518988121300936
      - 0.2561674624448642
      - 0.25259726820513606
      - 0.2491827841149643
      - 0.24825183663051575
      - 0.25617701141163707
      - 0.24908969027455896
      - 0.2511508275056258
      - 0.2526527186855674
      - 0.24526601389516145
      - 0.2512123433407396
  task_id: 3
