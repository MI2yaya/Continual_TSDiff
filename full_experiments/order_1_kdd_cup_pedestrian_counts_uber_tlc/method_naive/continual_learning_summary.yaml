continual_learning_setup:
  method: naive
  method_params: {}
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_naive/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      l1_weight: 0.0
      l2_weight: 0.0
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.40438318020818503
      NRMSE: 0.7789858433380953
      mean_wQuantileLoss: 0.3017952158786089
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4055209808284417
      - 0.2905929525149986
      - 0.23444822640158236
      - 0.2113470986369066
      - 0.21472568321041763
      - 0.2026538415811956
      - 0.19847334502264857
      - 0.19859629857819527
      - 0.1828829767764546
      - 0.18473711662227288
      - 0.1866632552118972
      - 0.18382324476260692
      - 0.17806040064897388
      - 0.18283973832149059
      - 0.1754070426686667
      - 0.1792277317144908
      - 0.17877733299974352
      - 0.1752140667522326
      - 0.17479409306542948
      - 0.1729930571746081
      - 0.1799824681947939
      - 0.16725335089722648
      - 0.17227727925637737
      - 0.16785551101202145
      - 0.16713320795679465
      - 0.1709399688988924
      - 0.17090637038927525
      - 0.1688230392173864
      - 0.17412847181549296
      - 0.1666945035685785
      - 0.16291990166064352
      - 0.16778292320668697
      - 0.16779793397290632
      - 0.16384866466978565
      - 0.16171300265705213
      - 0.1672949017956853
      - 0.1636042066384107
      - 0.16115285229170695
      - 0.16701201582327485
      - 0.1618104765075259
      - 0.16375944222090766
      - 0.16393597627757117
      - 0.1598336910828948
      - 0.1571905515738763
      - 0.1676472135586664
      - 0.1624753555515781
      - 0.16929998103296384
      - 0.15987327112816274
      - 0.15606488421326503
      - 0.16625816462328658
      - 0.1616289001540281
      - 0.1605634976294823
      - 0.1581922861514613
      - 0.15898578613996506
      - 0.1590671991580166
      - 0.15707087493501604
      - 0.15734463522676378
      - 0.16009680146817118
      - 0.16167807078454643
      - 0.15583656926173717
      - 0.16084595723077655
      - 0.1554811975802295
      - 0.15909448300953954
      - 0.15760965587105602
      - 0.1624135267920792
      - 0.15808132343227044
      - 0.15741054096724838
      - 0.16157232347177342
      - 0.16066562302876264
      - 0.1595865394338034
      - 0.15824150596745312
      val_loss:
      - 0.27316126227378845
      - 0.2905929525149986
      - 0.23444822640158236
      - 0.2113470986369066
      - 0.21472568321041763
      - 0.2026538415811956
      - 0.19847334502264857
      - 0.19859629857819527
      - 0.1828829767764546
      - 0.18473711662227288
      - 0.1866632552118972
      - 0.18382324476260692
      - 0.17806040064897388
      - 0.18283973832149059
      - 0.1754070426686667
      - 0.1792277317144908
      - 0.17877733299974352
      - 0.1752140667522326
      - 0.17479409306542948
      - 0.1729930571746081
      - 0.1799824681947939
      - 0.16725335089722648
      - 0.17227727925637737
      - 0.16785551101202145
      - 0.16713320795679465
      - 0.1709399688988924
      - 0.17090637038927525
      - 0.1688230392173864
      - 0.17412847181549296
      - 0.1666945035685785
      - 0.16291990166064352
      - 0.16778292320668697
      - 0.16779793397290632
      - 0.16384866466978565
      - 0.16171300265705213
      - 0.1672949017956853
      - 0.1636042066384107
      - 0.16115285229170695
      - 0.16701201582327485
      - 0.1618104765075259
      - 0.16375944222090766
      - 0.16393597627757117
      - 0.1598336910828948
      - 0.1571905515738763
      - 0.1676472135586664
      - 0.1624753555515781
      - 0.16929998103296384
      - 0.15987327112816274
      - 0.15606488421326503
      - 0.16625816462328658
      - 0.14140390008687972
      - 0.1605634976294823
      - 0.1581922861514613
      - 0.15898578613996506
      - 0.1590671991580166
      - 0.15707087493501604
      - 0.15734463522676378
      - 0.16009680146817118
      - 0.16167807078454643
      - 0.15583656926173717
      - 0.16084595723077655
      - 0.1554811975802295
      - 0.15909448300953954
      - 0.15760965587105602
      - 0.1624135267920792
      - 0.15808132343227044
      - 0.15741054096724838
      - 0.16157232347177342
      - 0.16066562302876264
      - 0.1595865394338034
      - 0.15824150596745312
  task_id: 1
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_naive/task_2_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      l1_weight: 0.0
      l2_weight: 0.0
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.21687346135322894
      NRMSE: 0.686315942638256
      mean_wQuantileLoss: 0.1744343141633016
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4648445957573131
      - 0.3481783338356763
      - 0.310354619869031
      - 0.2643842459656298
      - 0.24448015610687435
      - 0.2252868568757549
      - 0.21138186828466132
      - 0.21385771728819236
      - 0.2039991621277295
      - 0.18098199117230251
      - 0.1818755793501623
      - 0.19490672391839325
      - 0.18230580823728815
      - 0.18199537886539474
      - 0.17106584511930123
      - 0.17311133834300563
      - 0.17440506001003087
      - 0.20447106647770852
      - 0.1660167621448636
      - 0.16697441134601831
      - 0.16246367496205494
      - 0.16708575381198898
      - 0.1774940094910562
      - 0.16385817452101037
      - 0.1841116773430258
      - 0.15932418272132054
      - 0.15903083642479032
      - 0.18755116389365867
      - 0.1612491806736216
      - 0.15746794984443113
      - 0.15973066666629165
      - 0.1659660103614442
      - 0.1588486303226091
      - 0.1571045386372134
      - 0.15759035281371325
      - 0.1575124507653527
      - 0.15674638526979834
      - 0.1564483751426451
      - 0.15369479334913194
      - 0.15558400889858603
      - 0.15545095049310476
      - 0.15099561214447021
      - 0.1548665264272131
      - 0.15140786120900884
      - 0.15002578345593065
      - 0.15252163755940273
      - 0.1515493459883146
      - 0.1555328211397864
      - 0.1550149327958934
      - 0.15102802048204467
      - 0.1476714372402057
      - 0.151193177618552
      - 0.1515848653507419
      - 0.1512833580491133
      - 0.1494679654133506
      - 0.14873535587685183
      - 0.1497204588376917
      - 0.14942145807435736
      - 0.1522816409706138
      - 0.14782922953600064
      - 0.14933323935838416
      - 0.15092713350895792
      - 0.1479300007922575
      - 0.14976322220172733
      - 0.1488575249677524
      - 0.15019141335505992
      - 0.1526580209028907
      - 0.15371553873410448
      - 0.14676545793190598
      - 0.1530743184266612
      - 0.1462317532277666
      val_loss:
      - 0.2720669284462929
      - 0.3481783338356763
      - 0.310354619869031
      - 0.2643842459656298
      - 0.24448015610687435
      - 0.2252868568757549
      - 0.21138186828466132
      - 0.21385771728819236
      - 0.2039991621277295
      - 0.18098199117230251
      - 0.1818755793501623
      - 0.19490672391839325
      - 0.18230580823728815
      - 0.18199537886539474
      - 0.17106584511930123
      - 0.17311133834300563
      - 0.17440506001003087
      - 0.20447106647770852
      - 0.1660167621448636
      - 0.16697441134601831
      - 0.16246367496205494
      - 0.16708575381198898
      - 0.1774940094910562
      - 0.16385817452101037
      - 0.1841116773430258
      - 0.15932418272132054
      - 0.15903083642479032
      - 0.18755116389365867
      - 0.1612491806736216
      - 0.15746794984443113
      - 0.15973066666629165
      - 0.1659660103614442
      - 0.1588486303226091
      - 0.1571045386372134
      - 0.15759035281371325
      - 0.1575124507653527
      - 0.15674638526979834
      - 0.1564483751426451
      - 0.15369479334913194
      - 0.15558400889858603
      - 0.15545095049310476
      - 0.15099561214447021
      - 0.1548665264272131
      - 0.15140786120900884
      - 0.15002578345593065
      - 0.15252163755940273
      - 0.1515493459883146
      - 0.1555328211397864
      - 0.1550149327958934
      - 0.15102802048204467
      - 0.10655320435762405
      - 0.151193177618552
      - 0.1515848653507419
      - 0.1512833580491133
      - 0.1494679654133506
      - 0.14873535587685183
      - 0.1497204588376917
      - 0.14942145807435736
      - 0.1522816409706138
      - 0.14782922953600064
      - 0.14933323935838416
      - 0.15092713350895792
      - 0.1479300007922575
      - 0.14976322220172733
      - 0.1488575249677524
      - 0.15019141335505992
      - 0.1526580209028907
      - 0.15371553873410448
      - 0.14676545793190598
      - 0.1530743184266612
      - 0.1462317532277666
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_naive/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      l1_weight: 0.0
      l2_weight: 0.0
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.2167884009044275
      NRMSE: 0.47852145789546496
      mean_wQuantileLoss: 0.18230309515941243
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.49758451082743704
      - 0.33600906771607697
      - 0.30979790398851037
      - 0.3058758914703503
      - 0.29069339379202574
      - 0.28654739703051746
      - 0.28922330029308796
      - 0.28135621966794133
      - 0.2711798301897943
      - 0.27582565520424396
      - 0.27216018503531814
      - 0.2667432794114575
      - 0.2665254061575979
      - 0.2615599336568266
      - 0.2571095433086157
      - 0.26356435229536146
      - 0.26075828284956515
      - 0.25696458958555013
      - 0.25422851473558694
      - 0.26172607648186386
      - 0.2523625592002645
      - 0.25604134891182184
      - 0.25349020899739116
      - 0.25016957405023277
      - 0.24595197674352676
      - 0.2571771095972508
      - 0.25029827444814146
      - 0.24900655925739557
      - 0.2462792021688074
      - 0.24726752052083611
      - 0.24884620564989746
      - 0.25056829885579646
      - 0.2501599017996341
      - 0.24775917443912476
      - 0.24678931420203298
      - 0.24402261048089713
      - 0.24027140578255057
      - 0.24105476355180144
      - 0.23695088841486722
      - 0.24235911620780826
      - 0.24429546168539673
      - 0.24867791531141847
      - 0.24155430344399065
      - 0.24418043438345194
      - 0.24283541121985763
      - 0.23887226858641952
      - 0.24137736216653138
      - 0.24544048320967704
      - 0.24552837654482573
      - 0.2438549881335348
      - 0.24774933140724897
      - 0.24218603735789657
      - 0.24350642680656165
      - 0.23798324656672776
      - 0.23739003797527403
      - 0.23834590916521847
      - 0.2424919499317184
      - 0.23757556965574622
      - 0.23523872252553701
      - 0.24506417487282306
      - 0.23734645871445537
      - 0.24170536815654486
      - 0.24139071255922318
      - 0.2374552539549768
      - 0.23389455489814281
      - 0.24368755088653415
      - 0.23835273832082748
      - 0.2364312334684655
      - 0.23856903694104403
      - 0.23894203605595976
      - 0.23661053914111108
      val_loss:
      - 0.28171150088310243
      - 0.33600906771607697
      - 0.30979790398851037
      - 0.3058758914703503
      - 0.29069339379202574
      - 0.28654739703051746
      - 0.28922330029308796
      - 0.28135621966794133
      - 0.2711798301897943
      - 0.27582565520424396
      - 0.27216018503531814
      - 0.2667432794114575
      - 0.2665254061575979
      - 0.2615599336568266
      - 0.2571095433086157
      - 0.26356435229536146
      - 0.26075828284956515
      - 0.25696458958555013
      - 0.25422851473558694
      - 0.26172607648186386
      - 0.2523625592002645
      - 0.25604134891182184
      - 0.25349020899739116
      - 0.25016957405023277
      - 0.24595197674352676
      - 0.2571771095972508
      - 0.25029827444814146
      - 0.24900655925739557
      - 0.2462792021688074
      - 0.24726752052083611
      - 0.24884620564989746
      - 0.25056829885579646
      - 0.2501599017996341
      - 0.24775917443912476
      - 0.24678931420203298
      - 0.24402261048089713
      - 0.24027140578255057
      - 0.24105476355180144
      - 0.23695088841486722
      - 0.24235911620780826
      - 0.24429546168539673
      - 0.24867791531141847
      - 0.24155430344399065
      - 0.24418043438345194
      - 0.24283541121985763
      - 0.23887226858641952
      - 0.24137736216653138
      - 0.24544048320967704
      - 0.24552837654482573
      - 0.2438549881335348
      - 0.21594471633434295
      - 0.24218603735789657
      - 0.24350642680656165
      - 0.23798324656672776
      - 0.23739003797527403
      - 0.23834590916521847
      - 0.2424919499317184
      - 0.23757556965574622
      - 0.23523872252553701
      - 0.24506417487282306
      - 0.23734645871445537
      - 0.24170536815654486
      - 0.24139071255922318
      - 0.2374552539549768
      - 0.23389455489814281
      - 0.24368755088653415
      - 0.23835273832082748
      - 0.2364312334684655
      - 0.23856903694104403
      - 0.23894203605595976
      - 0.23661053914111108
  task_id: 3
