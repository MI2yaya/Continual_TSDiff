continual_learning_setup:
  method: dropout
  method_params:
    dropout_rate: 0.3
  num_tasks: 3
  task_sequence:
  - train_pedestrian_counts.yaml
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_dropout/dropout_rate_0.3/task_1_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.3
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.21073881578962486
      NRMSE: 0.6924585570835514
      mean_wQuantileLoss: 0.1712662368199528
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5216688476502895
      - 0.389961245120503
      - 0.3381279195891693
      - 0.2980346424737945
      - 0.2972422366729006
      - 0.28849079937208444
      - 0.2649620051961392
      - 0.24968018976505846
      - 0.24452462664339691
      - 0.22876835195347667
      - 0.26391097670421004
      - 0.21732639626134187
      - 0.20972577028442174
      - 0.2340028800535947
      - 0.21634072891902179
      - 0.2126214144518599
      - 0.20413497451227158
      - 0.20414115727180615
      - 0.19359702977817506
      - 0.20842394407372922
      - 0.19980439683422446
      - 0.1951476265094243
      - 0.19448654900770634
      - 0.1867041554651223
      - 0.1924688633880578
      - 0.18821083061629906
      - 0.1961262682452798
      - 0.19606297102291137
      - 0.19332730793394148
      - 0.1851314467494376
      - 0.19251904304837808
      - 0.19110488827573135
      - 0.18978165980661288
      - 0.19377094553783536
      - 0.19099436711985618
      - 0.18531518382951617
      - 0.18260684312554076
      - 0.18656685634050518
      - 0.18598995171487331
      - 0.18393202789593488
      - 0.18453172186855227
      - 0.18537237925920635
      - 0.1846211445517838
      - 0.18091187323443592
      - 0.19640999723924324
      - 0.18689645966514945
      - 0.18342124664923176
      - 0.1828486431040801
      - 0.18326502264244482
      - 0.17909935675561428
      - 0.17548324284143746
      - 0.1834136948455125
      - 0.17767339380225167
      - 0.20753272261936218
      - 0.18300452874973416
      - 0.19830113486386836
      - 0.17870988266076893
      - 0.17990360263502225
      - 0.2076890266034752
      - 0.18355136644095182
      - 0.1764396622311324
      - 0.17926271905889735
      - 0.17518935963744298
      - 0.18266541563207284
      - 0.17482824681792408
      - 0.17701700510224327
      - 0.17637011170154437
      - 0.17436345771420747
      - 0.17918817803729326
      - 0.1778538998332806
      - 0.17092887696344405
      val_loss:
      - 0.33905758522450924
      - 0.389961245120503
      - 0.3381279195891693
      - 0.2980346424737945
      - 0.2972422366729006
      - 0.28849079937208444
      - 0.2649620051961392
      - 0.24968018976505846
      - 0.24452462664339691
      - 0.22876835195347667
      - 0.26391097670421004
      - 0.21732639626134187
      - 0.20972577028442174
      - 0.2340028800535947
      - 0.21634072891902179
      - 0.2126214144518599
      - 0.20413497451227158
      - 0.20414115727180615
      - 0.19359702977817506
      - 0.20842394407372922
      - 0.19980439683422446
      - 0.1951476265094243
      - 0.19448654900770634
      - 0.1867041554651223
      - 0.1924688633880578
      - 0.18821083061629906
      - 0.1961262682452798
      - 0.19606297102291137
      - 0.19332730793394148
      - 0.1851314467494376
      - 0.19251904304837808
      - 0.19110488827573135
      - 0.18978165980661288
      - 0.19377094553783536
      - 0.19099436711985618
      - 0.18531518382951617
      - 0.18260684312554076
      - 0.18656685634050518
      - 0.18598995171487331
      - 0.18393202789593488
      - 0.18453172186855227
      - 0.18537237925920635
      - 0.1846211445517838
      - 0.18091187323443592
      - 0.19640999723924324
      - 0.18689645966514945
      - 0.18342124664923176
      - 0.1828486431040801
      - 0.18326502264244482
      - 0.17909935675561428
      - 0.08174498379230499
      - 0.1834136948455125
      - 0.17767339380225167
      - 0.20753272261936218
      - 0.18300452874973416
      - 0.19830113486386836
      - 0.17870988266076893
      - 0.17990360263502225
      - 0.2076890266034752
      - 0.18355136644095182
      - 0.1764396622311324
      - 0.17926271905889735
      - 0.17518935963744298
      - 0.18266541563207284
      - 0.17482824681792408
      - 0.17701700510224327
      - 0.17637011170154437
      - 0.17436345771420747
      - 0.17918817803729326
      - 0.1778538998332806
      - 0.17092887696344405
  task_id: 1
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_dropout/dropout_rate_0.3/task_2_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.3
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3830488353466806
      NRMSE: 0.7616550441383059
      mean_wQuantileLoss: 0.29935563931837655
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4425514214672148
      - 0.3380826733773574
      - 0.28806691837962717
      - 0.2678143106168136
      - 0.2485215167980641
      - 0.23767831549048424
      - 0.2329716165550053
      - 0.22734173305798322
      - 0.217767046764493
      - 0.2111364600714296
      - 0.21523236436769366
      - 0.21257720823632553
      - 0.21328661870211363
      - 0.2088746598456055
      - 0.21201494487468153
      - 0.20992305141407996
      - 0.2098158379085362
      - 0.20139905891846865
      - 0.20378256170079112
      - 0.20156013156520203
      - 0.20325011090608314
      - 0.200649035978131
      - 0.20054350467398763
      - 0.2023381198523566
      - 0.19767011399380863
      - 0.19982468016678467
      - 0.20358688023407012
      - 0.20057712506968528
      - 0.2005601639393717
      - 0.20073149114614353
      - 0.19297461409587413
      - 0.1947406095569022
      - 0.1953101025428623
      - 0.19492572033777833
      - 0.19011692178901285
      - 0.19411287450930104
      - 0.19159857142949477
      - 0.19345147162675858
      - 0.1901676629204303
      - 0.19961469795089215
      - 0.19522585667436942
      - 0.19842219667043537
      - 0.19678681704681367
      - 0.1900914201978594
      - 0.19013049342902377
      - 0.19162142847198993
      - 0.19165485200937837
      - 0.1923348575946875
      - 0.18658232444431633
      - 0.19140922970836982
      - 0.19044527551159263
      - 0.1898365868255496
      - 0.1889432305470109
      - 0.18883548502344638
      - 0.19087677262723446
      - 0.18835175421554595
      - 0.18353217473486438
      - 0.18932378286262974
      - 0.19010649353731424
      - 0.1870131068280898
      - 0.18773958424571902
      - 0.19336110167205334
      - 0.1874553357483819
      - 0.18808502866886556
      - 0.1802634002524428
      - 0.19073448702692986
      - 0.18986350914929062
      - 0.1844411303754896
      - 0.18411630857735872
      - 0.18427208013599738
      - 0.18540092156035826
      val_loss:
      - 0.28721159398555757
      - 0.3380826733773574
      - 0.28806691837962717
      - 0.2678143106168136
      - 0.2485215167980641
      - 0.23767831549048424
      - 0.2329716165550053
      - 0.22734173305798322
      - 0.217767046764493
      - 0.2111364600714296
      - 0.21523236436769366
      - 0.21257720823632553
      - 0.21328661870211363
      - 0.2088746598456055
      - 0.21201494487468153
      - 0.20992305141407996
      - 0.2098158379085362
      - 0.20139905891846865
      - 0.20378256170079112
      - 0.20156013156520203
      - 0.20325011090608314
      - 0.200649035978131
      - 0.20054350467398763
      - 0.2023381198523566
      - 0.19767011399380863
      - 0.19982468016678467
      - 0.20358688023407012
      - 0.20057712506968528
      - 0.2005601639393717
      - 0.20073149114614353
      - 0.19297461409587413
      - 0.1947406095569022
      - 0.1953101025428623
      - 0.19492572033777833
      - 0.19011692178901285
      - 0.19411287450930104
      - 0.19159857142949477
      - 0.19345147162675858
      - 0.1901676629204303
      - 0.19961469795089215
      - 0.19522585667436942
      - 0.19842219667043537
      - 0.19678681704681367
      - 0.1900914201978594
      - 0.19013049342902377
      - 0.19162142847198993
      - 0.19165485200937837
      - 0.1923348575946875
      - 0.18658232444431633
      - 0.19140922970836982
      - 0.16043653190135956
      - 0.1898365868255496
      - 0.1889432305470109
      - 0.18883548502344638
      - 0.19087677262723446
      - 0.18835175421554595
      - 0.18353217473486438
      - 0.18932378286262974
      - 0.19010649353731424
      - 0.1870131068280898
      - 0.18773958424571902
      - 0.19336110167205334
      - 0.1874553357483819
      - 0.18808502866886556
      - 0.1802634002524428
      - 0.19073448702692986
      - 0.18986350914929062
      - 0.1844411303754896
      - 0.18411630857735872
      - 0.18427208013599738
      - 0.18540092156035826
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_dropout/dropout_rate_0.3/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.3
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.21943208783701412
      NRMSE: 0.5093521051980333
      mean_wQuantileLoss: 0.18922247584946475
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.5662198441568762
      - 0.3826591558754444
      - 0.3469080713111907
      - 0.32875295786652714
      - 0.3179295965237543
      - 0.31611791462637484
      - 0.3156704193679616
      - 0.3118663851637393
      - 0.3107787797925994
      - 0.30602886353153735
      - 0.30411893886048347
      - 0.2994050480192527
      - 0.3066572101088241
      - 0.30358869256451726
      - 0.3002037728438154
      - 0.2945465595694259
      - 0.2977012824267149
      - 0.2957599909277633
      - 0.290823191171512
      - 0.29439294117037207
      - 0.2925809120060876
      - 0.28511052404064685
      - 0.2922932207584381
      - 0.2880325955338776
      - 0.287713656318374
      - 0.2883774795336649
      - 0.29198928317055106
      - 0.28434455988463014
      - 0.28943628165870905
      - 0.2904105529887602
      - 0.28378099063411355
      - 0.2886585376691073
      - 0.2789802991319448
      - 0.28394291445147246
      - 0.2814476395724341
      - 0.28730529418680817
      - 0.2785469674272463
      - 0.285626616794616
      - 0.2839326849207282
      - 0.2810673554195091
      - 0.2787065497832373
      - 0.27941235003527254
      - 0.2845241534523666
      - 0.2809075901750475
      - 0.2765425880206749
      - 0.2811376937897876
      - 0.2774817058816552
      - 0.2780756432330236
      - 0.27700964908581227
      - 0.2792095401091501
      - 0.27947540499735624
      - 0.27677160140592605
      - 0.2702458578860387
      - 0.27351662691216916
      - 0.27438483003061265
      - 0.27235762123018503
      - 0.2718124947277829
      - 0.27604980289470404
      - 0.27630749868694693
      - 0.2733871740056202
      - 0.2733842533780262
      - 0.27452243177685887
      - 0.27608790900558233
      - 0.27397798525635153
      - 0.2777445833198726
      - 0.2760299559449777
      - 0.2733044693013653
      - 0.27221218729391694
      - 0.2765298093436286
      - 0.27679366478696465
      - 0.2753898029914126
      val_loss:
      - 0.3407581329345703
      - 0.3826591558754444
      - 0.3469080713111907
      - 0.32875295786652714
      - 0.3179295965237543
      - 0.31611791462637484
      - 0.3156704193679616
      - 0.3118663851637393
      - 0.3107787797925994
      - 0.30602886353153735
      - 0.30411893886048347
      - 0.2994050480192527
      - 0.3066572101088241
      - 0.30358869256451726
      - 0.3002037728438154
      - 0.2945465595694259
      - 0.2977012824267149
      - 0.2957599909277633
      - 0.290823191171512
      - 0.29439294117037207
      - 0.2925809120060876
      - 0.28511052404064685
      - 0.2922932207584381
      - 0.2880325955338776
      - 0.287713656318374
      - 0.2883774795336649
      - 0.29198928317055106
      - 0.28434455988463014
      - 0.28943628165870905
      - 0.2904105529887602
      - 0.28378099063411355
      - 0.2886585376691073
      - 0.2789802991319448
      - 0.28394291445147246
      - 0.2814476395724341
      - 0.28730529418680817
      - 0.2785469674272463
      - 0.285626616794616
      - 0.2839326849207282
      - 0.2810673554195091
      - 0.2787065497832373
      - 0.27941235003527254
      - 0.2845241534523666
      - 0.2809075901750475
      - 0.2765425880206749
      - 0.2811376937897876
      - 0.2774817058816552
      - 0.2780756432330236
      - 0.27700964908581227
      - 0.2792095401091501
      - 0.21072982847690583
      - 0.27677160140592605
      - 0.2702458578860387
      - 0.27351662691216916
      - 0.27438483003061265
      - 0.27235762123018503
      - 0.2718124947277829
      - 0.27604980289470404
      - 0.27630749868694693
      - 0.2733871740056202
      - 0.2733842533780262
      - 0.27452243177685887
      - 0.27608790900558233
      - 0.27397798525635153
      - 0.2777445833198726
      - 0.2760299559449777
      - 0.2733044693013653
      - 0.27221218729391694
      - 0.2765298093436286
      - 0.27679366478696465
      - 0.2753898029914126
  task_id: 3
