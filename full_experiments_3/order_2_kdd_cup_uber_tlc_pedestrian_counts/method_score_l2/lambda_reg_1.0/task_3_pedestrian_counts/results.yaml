best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l2/lambda_reg_1.0/task_3_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
config:
  batch_size: 64
  context_length: 336
  dataset: pedestrian_counts
  device: cuda:1
  diffusion_config: diffusion_small_config
  dropout_rate: 0.0
  eval_every: 50
  freq: H
  gradient_clip_val: 0.5
  init_skip: true
  lambda_reg: 1.0
  lr: 0.0005
  max_epochs: 1000
  model: unconditional
  normalization: mean
  num_batches_per_epoch: 128
  prediction_length: 24
  sampler: ddpm
  sampler_params:
    guidance: quantile
    scale: 2
  score_loss_type: l2
  setup: forecasting
  use_features: false
  use_lags: true
  use_validation_set: true
metrics:
- ND: 0.3878702188772311
  NRMSE: 1.3033297430674986
  mean_wQuantileLoss: 0.3185893077771575
  missing_scenario: none
  missing_values: 0
training_history:
  epochs:
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  - 20
  - 21
  - 22
  - 23
  - 24
  - 25
  - 26
  - 27
  - 28
  - 29
  - 30
  - 31
  - 32
  - 33
  - 34
  - 35
  - 36
  - 37
  - 38
  - 39
  - 40
  - 41
  - 42
  - 43
  - 44
  - 45
  - 46
  - 47
  - 48
  - 49
  - 50
  - 51
  - 52
  - 53
  - 54
  - 55
  - 56
  - 57
  - 58
  - 59
  - 60
  - 61
  - 62
  - 63
  - 64
  - 65
  - 66
  - 67
  - 68
  - 69
  - 70
  - 71
  - 72
  - 73
  - 74
  - 75
  - 76
  - 77
  - 78
  - 79
  - 80
  - 81
  - 82
  - 83
  - 84
  - 85
  - 86
  - 87
  - 88
  - 89
  - 90
  - 91
  - 92
  - 93
  - 94
  - 95
  - 96
  train_loss:
  - 0.7825403418391943
  - 0.5566869783215225
  - 0.5430842647328973
  - 0.4990413775667548
  - 0.5116311418823898
  - 0.48863022355362773
  - 0.4738228351343423
  - 0.45954722980968654
  - 0.4531869173515588
  - 0.4712868691422045
  - 0.4531244211830199
  - 0.4574803300201893
  - 0.44169295975007117
  - 0.4391246901359409
  - 0.4836923435796052
  - 0.42947116889990866
  - 0.44125416269525886
  - 0.4398878114297986
  - 0.4710883351508528
  - 0.4325421859975904
  - 0.43059361982159317
  - 0.43553635757416487
  - 0.43852891167625785
  - 0.4245505949947983
  - 0.4361705770716071
  - 0.4475854008924216
  - 0.4373778561130166
  - 0.4401078699156642
  - 0.4263548606541008
  - 0.4474364707712084
  - 0.4760241156909615
  - 0.4325739834457636
  - 0.43386285356245935
  - 0.4333268234040588
  - 0.4273913176730275
  - 0.4417822426185012
  - 0.4344677028711885
  - 0.4358243450988084
  - 0.422711769817397
  - 0.432222509291023
  - 0.4271228064317256
  - 0.429663788061589
  - 0.4262564575765282
  - 0.46813582186587155
  - 0.424304966814816
  - 0.43668179283849895
  - 0.42174427141435444
  - 0.449387680971995
  - 0.4321008448023349
  - 0.43771294970065355
  - 0.4310510787181556
  - 0.4420944885350764
  - 0.438500622054562
  - 0.42664567148312926
  - 0.44848464033566415
  - 0.4154821871779859
  - 0.4203490598592907
  - 0.45976445311680436
  - 0.4341688519343734
  - 0.4286042314488441
  - 0.458755238680169
  - 0.42301437072455883
  - 0.434770890744403
  - 0.42637956608086824
  - 0.4332284489646554
  - 0.4255506405606866
  - 0.4286628505215049
  - 0.4275328880175948
  - 0.4330634262878448
  - 0.44040559511631727
  - 0.4364115661010146
  - 0.41873715352267027
  - 0.4446340112481266
  - 0.43792332615703344
  - 0.4239354282617569
  - 0.41082750586792827
  - 0.4165694359689951
  - 0.4285324120428413
  - 0.42568147275596857
  - 0.42305894545279443
  - 0.419112931471318
  - 0.4213113831356168
  - 0.4193457996007055
  - 0.4268331278581172
  - 0.4375571752898395
  - 0.4247654171194881
  - 0.4242285187356174
  - 0.42388950707390904
  - 0.4536699987947941
  - 0.41674759006127715
  - 0.4230565601028502
  - 0.4220215701498091
  - 0.42128884233534336
  - 0.4272276589181274
  - 0.4179871219675988
  - 0.4266295353882015
  val_loss:
  - 0.6736430525779724
  - 0.5566869783215225
  - 0.5430842647328973
  - 0.4990413775667548
  - 0.5116311418823898
  - 0.48863022355362773
  - 0.4738228351343423
  - 0.45954722980968654
  - 0.4531869173515588
  - 0.4712868691422045
  - 0.4531244211830199
  - 0.4574803300201893
  - 0.44169295975007117
  - 0.4391246901359409
  - 0.4836923435796052
  - 0.42947116889990866
  - 0.44125416269525886
  - 0.4398878114297986
  - 0.4710883351508528
  - 0.4325421859975904
  - 0.43059361982159317
  - 0.43553635757416487
  - 0.43852891167625785
  - 0.4245505949947983
  - 0.4361705770716071
  - 0.4475854008924216
  - 0.4373778561130166
  - 0.4401078699156642
  - 0.4263548606541008
  - 0.4474364707712084
  - 0.4760241156909615
  - 0.4325739834457636
  - 0.43386285356245935
  - 0.4333268234040588
  - 0.4273913176730275
  - 0.4417822426185012
  - 0.4344677028711885
  - 0.4358243450988084
  - 0.422711769817397
  - 0.432222509291023
  - 0.4271228064317256
  - 0.429663788061589
  - 0.4262564575765282
  - 0.46813582186587155
  - 0.424304966814816
  - 0.43668179283849895
  - 0.42174427141435444
  - 0.449387680971995
  - 0.4321008448023349
  - 0.43771294970065355
  - 0.4947092831134796
  - 0.4420944885350764
  - 0.438500622054562
  - 0.42664567148312926
  - 0.44848464033566415
  - 0.4154821871779859
  - 0.4203490598592907
  - 0.45976445311680436
  - 0.4341688519343734
  - 0.4286042314488441
  - 0.458755238680169
  - 0.42301437072455883
  - 0.434770890744403
  - 0.42637956608086824
  - 0.4332284489646554
  - 0.4255506405606866
  - 0.4286628505215049
  - 0.4275328880175948
  - 0.4330634262878448
  - 0.44040559511631727
  - 0.4364115661010146
  - 0.41873715352267027
  - 0.4446340112481266
  - 0.43792332615703344
  - 0.4239354282617569
  - 0.41082750586792827
  - 0.4165694359689951
  - 0.4285324120428413
  - 0.42568147275596857
  - 0.42305894545279443
  - 0.419112931471318
  - 0.4213113831356168
  - 0.4193457996007055
  - 0.4268331278581172
  - 0.4375571752898395
  - 0.4247654171194881
  - 0.4242285187356174
  - 0.42388950707390904
  - 0.4536699987947941
  - 0.41674759006127715
  - 0.4230565601028502
  - 0.4220215701498091
  - 0.42128884233534336
  - 0.4272276589181274
  - 0.4179871219675988
  - 0.4266295353882015
