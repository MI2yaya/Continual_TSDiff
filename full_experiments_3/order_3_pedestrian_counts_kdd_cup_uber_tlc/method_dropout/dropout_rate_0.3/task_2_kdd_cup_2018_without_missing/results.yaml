best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_dropout/dropout_rate_0.3/task_2_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
config:
  batch_size: 64
  context_length: 312
  dataset: kdd_cup_2018_without_missing
  device: cuda:1
  diffusion_config: diffusion_small_config
  dropout_rate: 0.3
  eval_every: 50
  freq: H
  gradient_clip_val: 0.5
  init_skip: true
  lambda_reg: 0.0
  lr: 0.001
  max_epochs: 1000
  model: unconditional
  normalization: mean
  num_batches_per_epoch: 128
  num_samples: 16
  prediction_length: 24
  sampler: ddpm
  sampler_params:
    guidance: quantile
    scale: 1
  score_loss_type: l2
  setup: forecasting
  use_features: false
  use_lags: true
  use_validation_set: true
metrics:
- ND: 0.3830488353466806
  NRMSE: 0.7616550441383059
  mean_wQuantileLoss: 0.29935563931837655
  missing_scenario: none
  missing_values: 0
training_history:
  epochs:
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  - 20
  - 21
  - 22
  - 23
  - 24
  - 25
  - 26
  - 27
  - 28
  - 29
  - 30
  - 31
  - 32
  - 33
  - 34
  - 35
  - 36
  - 37
  - 38
  - 39
  - 40
  - 41
  - 42
  - 43
  - 44
  - 45
  - 46
  - 47
  - 48
  - 49
  - 50
  - 51
  - 52
  - 53
  - 54
  - 55
  - 56
  - 57
  - 58
  - 59
  - 60
  - 61
  - 62
  - 63
  - 64
  - 65
  - 66
  - 67
  - 68
  - 69
  - 70
  - 71
  train_loss:
  - 0.4425514214672148
  - 0.3380826733773574
  - 0.28806691837962717
  - 0.2678143106168136
  - 0.2485215167980641
  - 0.23767831549048424
  - 0.2329716165550053
  - 0.22734173305798322
  - 0.217767046764493
  - 0.2111364600714296
  - 0.21523236436769366
  - 0.21257720823632553
  - 0.21328661870211363
  - 0.2088746598456055
  - 0.21201494487468153
  - 0.20992305141407996
  - 0.2098158379085362
  - 0.20139905891846865
  - 0.20378256170079112
  - 0.20156013156520203
  - 0.20325011090608314
  - 0.200649035978131
  - 0.20054350467398763
  - 0.2023381198523566
  - 0.19767011399380863
  - 0.19982468016678467
  - 0.20358688023407012
  - 0.20057712506968528
  - 0.2005601639393717
  - 0.20073149114614353
  - 0.19297461409587413
  - 0.1947406095569022
  - 0.1953101025428623
  - 0.19492572033777833
  - 0.19011692178901285
  - 0.19411287450930104
  - 0.19159857142949477
  - 0.19345147162675858
  - 0.1901676629204303
  - 0.19961469795089215
  - 0.19522585667436942
  - 0.19842219667043537
  - 0.19678681704681367
  - 0.1900914201978594
  - 0.19013049342902377
  - 0.19162142847198993
  - 0.19165485200937837
  - 0.1923348575946875
  - 0.18658232444431633
  - 0.19140922970836982
  - 0.19044527551159263
  - 0.1898365868255496
  - 0.1889432305470109
  - 0.18883548502344638
  - 0.19087677262723446
  - 0.18835175421554595
  - 0.18353217473486438
  - 0.18932378286262974
  - 0.19010649353731424
  - 0.1870131068280898
  - 0.18773958424571902
  - 0.19336110167205334
  - 0.1874553357483819
  - 0.18808502866886556
  - 0.1802634002524428
  - 0.19073448702692986
  - 0.18986350914929062
  - 0.1844411303754896
  - 0.18411630857735872
  - 0.18427208013599738
  - 0.18540092156035826
  val_loss:
  - 0.28721159398555757
  - 0.3380826733773574
  - 0.28806691837962717
  - 0.2678143106168136
  - 0.2485215167980641
  - 0.23767831549048424
  - 0.2329716165550053
  - 0.22734173305798322
  - 0.217767046764493
  - 0.2111364600714296
  - 0.21523236436769366
  - 0.21257720823632553
  - 0.21328661870211363
  - 0.2088746598456055
  - 0.21201494487468153
  - 0.20992305141407996
  - 0.2098158379085362
  - 0.20139905891846865
  - 0.20378256170079112
  - 0.20156013156520203
  - 0.20325011090608314
  - 0.200649035978131
  - 0.20054350467398763
  - 0.2023381198523566
  - 0.19767011399380863
  - 0.19982468016678467
  - 0.20358688023407012
  - 0.20057712506968528
  - 0.2005601639393717
  - 0.20073149114614353
  - 0.19297461409587413
  - 0.1947406095569022
  - 0.1953101025428623
  - 0.19492572033777833
  - 0.19011692178901285
  - 0.19411287450930104
  - 0.19159857142949477
  - 0.19345147162675858
  - 0.1901676629204303
  - 0.19961469795089215
  - 0.19522585667436942
  - 0.19842219667043537
  - 0.19678681704681367
  - 0.1900914201978594
  - 0.19013049342902377
  - 0.19162142847198993
  - 0.19165485200937837
  - 0.1923348575946875
  - 0.18658232444431633
  - 0.19140922970836982
  - 0.16043653190135956
  - 0.1898365868255496
  - 0.1889432305470109
  - 0.18883548502344638
  - 0.19087677262723446
  - 0.18835175421554595
  - 0.18353217473486438
  - 0.18932378286262974
  - 0.19010649353731424
  - 0.1870131068280898
  - 0.18773958424571902
  - 0.19336110167205334
  - 0.1874553357483819
  - 0.18808502866886556
  - 0.1802634002524428
  - 0.19073448702692986
  - 0.18986350914929062
  - 0.1844411303754896
  - 0.18411630857735872
  - 0.18427208013599738
  - 0.18540092156035826
