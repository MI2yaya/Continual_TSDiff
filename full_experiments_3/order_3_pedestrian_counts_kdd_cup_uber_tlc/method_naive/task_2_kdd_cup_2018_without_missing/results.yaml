best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_naive/task_2_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
config:
  batch_size: 64
  context_length: 312
  dataset: kdd_cup_2018_without_missing
  device: cuda:1
  diffusion_config: diffusion_small_config
  dropout_rate: 0.0
  eval_every: 50
  freq: H
  gradient_clip_val: 0.5
  init_skip: true
  lambda_reg: 0.0
  lr: 0.001
  max_epochs: 1000
  model: unconditional
  normalization: mean
  num_batches_per_epoch: 128
  num_samples: 16
  prediction_length: 24
  sampler: ddpm
  sampler_params:
    guidance: quantile
    scale: 1
  score_loss_type: l2
  setup: forecasting
  use_features: false
  use_lags: true
  use_validation_set: true
metrics:
- ND: 0.4596187418358904
  NRMSE: 0.8228018020712213
  mean_wQuantileLoss: 0.355335262039553
  missing_scenario: none
  missing_values: 0
training_history:
  epochs:
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  - 20
  - 21
  - 22
  - 23
  - 24
  - 25
  - 26
  - 27
  - 28
  - 29
  - 30
  - 31
  - 32
  - 33
  - 34
  - 35
  - 36
  - 37
  - 38
  - 39
  - 40
  - 41
  - 42
  - 43
  - 44
  - 45
  - 46
  - 47
  - 48
  - 49
  - 50
  - 51
  - 52
  - 53
  - 54
  - 55
  - 56
  - 57
  - 58
  - 59
  - 60
  - 61
  - 62
  - 63
  - 64
  - 65
  - 66
  - 67
  - 68
  - 69
  - 70
  - 71
  - 72
  - 73
  - 74
  - 75
  - 76
  - 77
  - 78
  - 79
  - 80
  - 81
  - 82
  - 83
  - 84
  - 85
  - 86
  - 87
  - 88
  - 89
  - 90
  train_loss:
  - 0.40948468656279147
  - 0.267779357614927
  - 0.23161187814548612
  - 0.20422775950282812
  - 0.1988285127445124
  - 0.1955993581796065
  - 0.1933081682655029
  - 0.19246470124926418
  - 0.18902965623419732
  - 0.1861172957578674
  - 0.1885739605058916
  - 0.17548350436845794
  - 0.17829684301977977
  - 0.17919728497508913
  - 0.17732345080003142
  - 0.17203789937775582
  - 0.17174463614355773
  - 0.17576775804627687
  - 0.17630482860840857
  - 0.17354856041492894
  - 0.17248408106388524
  - 0.16719516646116972
  - 0.16762952896533534
  - 0.17017391446279362
  - 0.17014729813672602
  - 0.16744170541642234
  - 0.16780326701700687
  - 0.16708864422980696
  - 0.166333144181408
  - 0.16379593301098794
  - 0.16542218648828566
  - 0.1669494491070509
  - 0.1677754232659936
  - 0.1639436180703342
  - 0.16260700160637498
  - 0.16875894117401913
  - 0.1655528325936757
  - 0.16317152179544792
  - 0.1632016018847935
  - 0.15776066854596138
  - 0.1642529835808091
  - 0.1598383939708583
  - 0.1626197583391331
  - 0.16119095223257318
  - 0.16373194981133565
  - 0.16576328611699864
  - 0.15768391179153696
  - 0.16336273710476235
  - 0.1595405769185163
  - 0.16167678526835516
  - 0.1597870448604226
  - 0.16169352387078106
  - 0.16196710406802595
  - 0.16028217424172908
  - 0.15850885584950447
  - 0.15899626188911498
  - 0.15161871910095215
  - 0.15634388325270265
  - 0.15881970949703828
  - 0.15871930500725284
  - 0.15894001361448318
  - 0.15638182865222916
  - 0.15760274388594553
  - 0.16445464052958414
  - 0.16015898442128673
  - 0.15497826552018523
  - 0.16324235929641873
  - 0.15352655237074941
  - 0.15738044161116704
  - 0.15070351044414565
  - 0.160277868504636
  - 0.15725018788361922
  - 0.1621728881727904
  - 0.15784524101763964
  - 0.15594413597136736
  - 0.15375151432817802
  - 0.153353868809063
  - 0.1543078916729428
  - 0.15832786064129323
  - 0.15170911024324596
  - 0.1548403960186988
  - 0.15656535618472844
  - 0.15431229810928926
  - 0.15635829349048436
  - 0.1561389463604428
  - 0.15361862728605047
  - 0.15142458624904975
  - 0.15298739715944976
  - 0.157247073831968
  - 0.1580877312226221
  val_loss:
  - 0.26645477712154386
  - 0.267779357614927
  - 0.23161187814548612
  - 0.20422775950282812
  - 0.1988285127445124
  - 0.1955993581796065
  - 0.1933081682655029
  - 0.19246470124926418
  - 0.18902965623419732
  - 0.1861172957578674
  - 0.1885739605058916
  - 0.17548350436845794
  - 0.17829684301977977
  - 0.17919728497508913
  - 0.17732345080003142
  - 0.17203789937775582
  - 0.17174463614355773
  - 0.17576775804627687
  - 0.17630482860840857
  - 0.17354856041492894
  - 0.17248408106388524
  - 0.16719516646116972
  - 0.16762952896533534
  - 0.17017391446279362
  - 0.17014729813672602
  - 0.16744170541642234
  - 0.16780326701700687
  - 0.16708864422980696
  - 0.166333144181408
  - 0.16379593301098794
  - 0.16542218648828566
  - 0.1669494491070509
  - 0.1677754232659936
  - 0.1639436180703342
  - 0.16260700160637498
  - 0.16875894117401913
  - 0.1655528325936757
  - 0.16317152179544792
  - 0.1632016018847935
  - 0.15776066854596138
  - 0.1642529835808091
  - 0.1598383939708583
  - 0.1626197583391331
  - 0.16119095223257318
  - 0.16373194981133565
  - 0.16576328611699864
  - 0.15768391179153696
  - 0.16336273710476235
  - 0.1595405769185163
  - 0.16167678526835516
  - 0.15338521897792817
  - 0.16169352387078106
  - 0.16196710406802595
  - 0.16028217424172908
  - 0.15850885584950447
  - 0.15899626188911498
  - 0.15161871910095215
  - 0.15634388325270265
  - 0.15881970949703828
  - 0.15871930500725284
  - 0.15894001361448318
  - 0.15638182865222916
  - 0.15760274388594553
  - 0.16445464052958414
  - 0.16015898442128673
  - 0.15497826552018523
  - 0.16324235929641873
  - 0.15352655237074941
  - 0.15738044161116704
  - 0.15070351044414565
  - 0.160277868504636
  - 0.15725018788361922
  - 0.1621728881727904
  - 0.15784524101763964
  - 0.15594413597136736
  - 0.15375151432817802
  - 0.153353868809063
  - 0.1543078916729428
  - 0.15832786064129323
  - 0.15170911024324596
  - 0.1548403960186988
  - 0.15656535618472844
  - 0.15431229810928926
  - 0.15635829349048436
  - 0.1561389463604428
  - 0.15361862728605047
  - 0.15142458624904975
  - 0.15298739715944976
  - 0.157247073831968
  - 0.1580877312226221
