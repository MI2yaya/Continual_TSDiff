continual_learning_setup:
  method: score_l2
  method_params:
    lambda_reg: 1.0
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
  - train_pedestrian_counts.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l2/lambda_reg_1.0/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 1.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3703233746366903
      NRMSE: 0.737343961263846
      mean_wQuantileLoss: 0.28300728099835104
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      - 116
      - 117
      - 118
      - 119
      - 120
      - 121
      train_loss:
      - 0.4063981173094362
      - 0.27954805409535766
      - 0.2480586221208796
      - 0.2169991476694122
      - 0.20260010659694672
      - 0.2047288062167354
      - 0.1990452401805669
      - 0.19346092274645343
      - 0.19133805122692138
      - 0.18808508914662525
      - 0.1904004780226387
      - 0.1780045148334466
      - 0.18179883481934667
      - 0.18182878918014467
      - 0.18040902441134676
      - 0.18278075743000954
      - 0.17728750657988712
      - 0.17644275195198134
      - 0.1716434204718098
      - 0.17671865253942087
      - 0.17876236140727997
      - 0.17778215039288625
      - 0.17101756669580936
      - 0.1742531186901033
      - 0.17438558966387063
      - 0.17303676111623645
      - 0.1694505043560639
      - 0.17191888665547594
      - 0.17228498205076903
      - 0.16468064312357455
      - 0.16480140149360523
      - 0.16590800398262218
      - 0.16980307950871065
      - 0.16875542741036043
      - 0.16533982288092375
      - 0.16755772032774985
      - 0.1675325448741205
      - 0.16600744047900662
      - 0.16561457380885258
      - 0.16074270429089665
      - 0.16771483060438186
      - 0.17033267364604399
      - 0.16745016915956512
      - 0.16587158368201926
      - 0.1657527611241676
      - 0.16572630568407476
      - 0.16496180137619376
      - 0.1664393424289301
      - 0.15766809578053653
      - 0.16211188735906035
      - 0.1607169471681118
      - 0.16945342277176678
      - 0.16413630585884675
      - 0.15871304232859984
      - 0.1656516171642579
      - 0.157592601433862
      - 0.16306815826101229
      - 0.163126940082293
      - 0.16245365922804922
      - 0.1611037829425186
      - 0.16102015908109024
      - 0.1641373226302676
      - 0.1570475420448929
      - 0.15808357868809253
      - 0.16268634563311934
      - 0.15709239640273154
      - 0.159234122838825
      - 0.15832220920128748
      - 0.15664063452277333
      - 0.15998661640333012
      - 0.15724436263553798
      - 0.15515392104862258
      - 0.15570079541066661
      - 0.16134712466737255
      - 0.1545036225579679
      - 0.15890628844499588
      - 0.15503526834072545
      - 0.15598503674846143
      - 0.15494784858310595
      - 0.1527115295175463
      - 0.15810661227442324
      - 0.15455766901141033
      - 0.15606232042773627
      - 0.160475303651765
      - 0.15187649172730744
      - 0.15845000429544598
      - 0.15406452567549422
      - 0.15257567452499643
      - 0.15884235460544005
      - 0.15261145564727485
      - 0.15667424944695085
      - 0.15761756239226088
      - 0.15718283876776695
      - 0.1555982306599617
      - 0.1543111446662806
      - 0.15882090578088537
      - 0.15708636061754078
      - 0.15174519800348207
      - 0.157001705665607
      - 0.15840709739131853
      - 0.15552747953915969
      - 0.15037150983698666
      - 0.15356271411292255
      - 0.15278492314973846
      - 0.15293556259712204
      - 0.14669152535498142
      - 0.15196563565405086
      - 0.15595699386904016
      - 0.1476773483445868
      - 0.15485328453360125
      - 0.15137885679723695
      - 0.15014052123297006
      - 0.15447605162626132
      - 0.15394390968140215
      - 0.15422226715600118
      - 0.1531095919199288
      - 0.14795889134984463
      - 0.1544254906475544
      - 0.152848428231664
      - 0.15339693491114303
      - 0.15360090328613296
      val_loss:
      - 0.2844802439212799
      - 0.27954805409535766
      - 0.2480586221208796
      - 0.2169991476694122
      - 0.20260010659694672
      - 0.2047288062167354
      - 0.1990452401805669
      - 0.19346092274645343
      - 0.19133805122692138
      - 0.18808508914662525
      - 0.1904004780226387
      - 0.1780045148334466
      - 0.18179883481934667
      - 0.18182878918014467
      - 0.18040902441134676
      - 0.18278075743000954
      - 0.17728750657988712
      - 0.17644275195198134
      - 0.1716434204718098
      - 0.17671865253942087
      - 0.17876236140727997
      - 0.17778215039288625
      - 0.17101756669580936
      - 0.1742531186901033
      - 0.17438558966387063
      - 0.17303676111623645
      - 0.1694505043560639
      - 0.17191888665547594
      - 0.17228498205076903
      - 0.16468064312357455
      - 0.16480140149360523
      - 0.16590800398262218
      - 0.16980307950871065
      - 0.16875542741036043
      - 0.16533982288092375
      - 0.16755772032774985
      - 0.1675325448741205
      - 0.16600744047900662
      - 0.16561457380885258
      - 0.16074270429089665
      - 0.16771483060438186
      - 0.17033267364604399
      - 0.16745016915956512
      - 0.16587158368201926
      - 0.1657527611241676
      - 0.16572630568407476
      - 0.16496180137619376
      - 0.1664393424289301
      - 0.15766809578053653
      - 0.16211188735906035
      - 0.15700239837169647
      - 0.16945342277176678
      - 0.16413630585884675
      - 0.15871304232859984
      - 0.1656516171642579
      - 0.157592601433862
      - 0.16306815826101229
      - 0.163126940082293
      - 0.16245365922804922
      - 0.1611037829425186
      - 0.16102015908109024
      - 0.1641373226302676
      - 0.1570475420448929
      - 0.15808357868809253
      - 0.16268634563311934
      - 0.15709239640273154
      - 0.159234122838825
      - 0.15832220920128748
      - 0.15664063452277333
      - 0.15998661640333012
      - 0.15724436263553798
      - 0.15515392104862258
      - 0.15570079541066661
      - 0.16134712466737255
      - 0.1545036225579679
      - 0.15890628844499588
      - 0.15503526834072545
      - 0.15598503674846143
      - 0.15494784858310595
      - 0.1527115295175463
      - 0.15810661227442324
      - 0.15455766901141033
      - 0.15606232042773627
      - 0.160475303651765
      - 0.15187649172730744
      - 0.15845000429544598
      - 0.15406452567549422
      - 0.15257567452499643
      - 0.15884235460544005
      - 0.15261145564727485
      - 0.15667424944695085
      - 0.15761756239226088
      - 0.15718283876776695
      - 0.1555982306599617
      - 0.1543111446662806
      - 0.15882090578088537
      - 0.15708636061754078
      - 0.15174519800348207
      - 0.157001705665607
      - 0.15840709739131853
      - 0.13824745267629623
      - 0.15037150983698666
      - 0.15356271411292255
      - 0.15278492314973846
      - 0.15293556259712204
      - 0.14669152535498142
      - 0.15196563565405086
      - 0.15595699386904016
      - 0.1476773483445868
      - 0.15485328453360125
      - 0.15137885679723695
      - 0.15014052123297006
      - 0.15447605162626132
      - 0.15394390968140215
      - 0.15422226715600118
      - 0.1531095919199288
      - 0.14795889134984463
      - 0.1544254906475544
      - 0.152848428231664
      - 0.15339693491114303
      - 0.15360090328613296
  task_id: 1
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l2/lambda_reg_1.0/task_2_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 1.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.9015006160121318
      NRMSE: 8.6261102996001
      mean_wQuantileLoss: 0.9611869517422833
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 2.252155529335141
      - 1.7375076776370406
      - 1.6847328003495932
      - 1.674661879427731
      - 1.5972233158536255
      - 1.6214121505618095
      - 1.5831909123808146
      - 1.6113954689353704
      - 1.570461026392877
      - 1.5110153802670538
      - 1.585188799072057
      - 1.6245386484079063
      - 1.6175079997628927
      - 1.5472236098721623
      - 1.5280624544247985
      - 1.5886917412281036
      - 1.6719084233045578
      - 1.5417811078950763
      - 1.5553703177720308
      - 1.541492862161249
      - 1.5828457390889525
      val_loss:
      - 0.7164753556251526
      - 1.7375076776370406
      - 1.6847328003495932
      - 1.674661879427731
      - 1.5972233158536255
      - 1.6214121505618095
      - 1.5831909123808146
      - 1.6113954689353704
      - 1.570461026392877
      - 1.5110153802670538
      - 1.585188799072057
      - 1.6245386484079063
      - 1.6175079997628927
      - 1.5472236098721623
      - 1.5280624544247985
      - 1.5886917412281036
      - 1.6719084233045578
      - 1.5417811078950763
      - 1.5553703177720308
      - 1.541492862161249
      - 1.5828457390889525
  task_id: 2
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l2/lambda_reg_1.0/task_3_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 1.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3878702188772311
      NRMSE: 1.3033297430674986
      mean_wQuantileLoss: 0.3185893077771575
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      train_loss:
      - 0.7825403418391943
      - 0.5566869783215225
      - 0.5430842647328973
      - 0.4990413775667548
      - 0.5116311418823898
      - 0.48863022355362773
      - 0.4738228351343423
      - 0.45954722980968654
      - 0.4531869173515588
      - 0.4712868691422045
      - 0.4531244211830199
      - 0.4574803300201893
      - 0.44169295975007117
      - 0.4391246901359409
      - 0.4836923435796052
      - 0.42947116889990866
      - 0.44125416269525886
      - 0.4398878114297986
      - 0.4710883351508528
      - 0.4325421859975904
      - 0.43059361982159317
      - 0.43553635757416487
      - 0.43852891167625785
      - 0.4245505949947983
      - 0.4361705770716071
      - 0.4475854008924216
      - 0.4373778561130166
      - 0.4401078699156642
      - 0.4263548606541008
      - 0.4474364707712084
      - 0.4760241156909615
      - 0.4325739834457636
      - 0.43386285356245935
      - 0.4333268234040588
      - 0.4273913176730275
      - 0.4417822426185012
      - 0.4344677028711885
      - 0.4358243450988084
      - 0.422711769817397
      - 0.432222509291023
      - 0.4271228064317256
      - 0.429663788061589
      - 0.4262564575765282
      - 0.46813582186587155
      - 0.424304966814816
      - 0.43668179283849895
      - 0.42174427141435444
      - 0.449387680971995
      - 0.4321008448023349
      - 0.43771294970065355
      - 0.4310510787181556
      - 0.4420944885350764
      - 0.438500622054562
      - 0.42664567148312926
      - 0.44848464033566415
      - 0.4154821871779859
      - 0.4203490598592907
      - 0.45976445311680436
      - 0.4341688519343734
      - 0.4286042314488441
      - 0.458755238680169
      - 0.42301437072455883
      - 0.434770890744403
      - 0.42637956608086824
      - 0.4332284489646554
      - 0.4255506405606866
      - 0.4286628505215049
      - 0.4275328880175948
      - 0.4330634262878448
      - 0.44040559511631727
      - 0.4364115661010146
      - 0.41873715352267027
      - 0.4446340112481266
      - 0.43792332615703344
      - 0.4239354282617569
      - 0.41082750586792827
      - 0.4165694359689951
      - 0.4285324120428413
      - 0.42568147275596857
      - 0.42305894545279443
      - 0.419112931471318
      - 0.4213113831356168
      - 0.4193457996007055
      - 0.4268331278581172
      - 0.4375571752898395
      - 0.4247654171194881
      - 0.4242285187356174
      - 0.42388950707390904
      - 0.4536699987947941
      - 0.41674759006127715
      - 0.4230565601028502
      - 0.4220215701498091
      - 0.42128884233534336
      - 0.4272276589181274
      - 0.4179871219675988
      - 0.4266295353882015
      val_loss:
      - 0.6736430525779724
      - 0.5566869783215225
      - 0.5430842647328973
      - 0.4990413775667548
      - 0.5116311418823898
      - 0.48863022355362773
      - 0.4738228351343423
      - 0.45954722980968654
      - 0.4531869173515588
      - 0.4712868691422045
      - 0.4531244211830199
      - 0.4574803300201893
      - 0.44169295975007117
      - 0.4391246901359409
      - 0.4836923435796052
      - 0.42947116889990866
      - 0.44125416269525886
      - 0.4398878114297986
      - 0.4710883351508528
      - 0.4325421859975904
      - 0.43059361982159317
      - 0.43553635757416487
      - 0.43852891167625785
      - 0.4245505949947983
      - 0.4361705770716071
      - 0.4475854008924216
      - 0.4373778561130166
      - 0.4401078699156642
      - 0.4263548606541008
      - 0.4474364707712084
      - 0.4760241156909615
      - 0.4325739834457636
      - 0.43386285356245935
      - 0.4333268234040588
      - 0.4273913176730275
      - 0.4417822426185012
      - 0.4344677028711885
      - 0.4358243450988084
      - 0.422711769817397
      - 0.432222509291023
      - 0.4271228064317256
      - 0.429663788061589
      - 0.4262564575765282
      - 0.46813582186587155
      - 0.424304966814816
      - 0.43668179283849895
      - 0.42174427141435444
      - 0.449387680971995
      - 0.4321008448023349
      - 0.43771294970065355
      - 0.4947092831134796
      - 0.4420944885350764
      - 0.438500622054562
      - 0.42664567148312926
      - 0.44848464033566415
      - 0.4154821871779859
      - 0.4203490598592907
      - 0.45976445311680436
      - 0.4341688519343734
      - 0.4286042314488441
      - 0.458755238680169
      - 0.42301437072455883
      - 0.434770890744403
      - 0.42637956608086824
      - 0.4332284489646554
      - 0.4255506405606866
      - 0.4286628505215049
      - 0.4275328880175948
      - 0.4330634262878448
      - 0.44040559511631727
      - 0.4364115661010146
      - 0.41873715352267027
      - 0.4446340112481266
      - 0.43792332615703344
      - 0.4239354282617569
      - 0.41082750586792827
      - 0.4165694359689951
      - 0.4285324120428413
      - 0.42568147275596857
      - 0.42305894545279443
      - 0.419112931471318
      - 0.4213113831356168
      - 0.4193457996007055
      - 0.4268331278581172
      - 0.4375571752898395
      - 0.4247654171194881
      - 0.4242285187356174
      - 0.42388950707390904
      - 0.4536699987947941
      - 0.41674759006127715
      - 0.4230565601028502
      - 0.4220215701498091
      - 0.42128884233534336
      - 0.4272276589181274
      - 0.4179871219675988
      - 0.4266295353882015
  task_id: 3
