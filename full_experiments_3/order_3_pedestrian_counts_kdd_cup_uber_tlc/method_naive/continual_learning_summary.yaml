continual_learning_setup:
  method: naive
  method_params: {}
  num_tasks: 3
  task_sequence:
  - train_pedestrian_counts.yaml
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_naive/task_1_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.20171834944131392
      NRMSE: 0.704719505403971
      mean_wQuantileLoss: 0.16488460739963873
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4732156078098342
      - 0.346184425172396
      - 0.2960369838401675
      - 0.26624296174850315
      - 0.22407816641498357
      - 0.20617235190002248
      - 0.20398742257384583
      - 0.21137780684512109
      - 0.194588816317264
      - 0.17986087111057714
      - 0.18578973109833896
      - 0.17926836613332853
      - 0.17235541250556707
      - 0.17020081891678274
      - 0.1805586180416867
      - 0.17256250214995816
      - 0.16849870508303866
      - 0.16426474205218256
      - 0.16662725998321548
      - 0.16719698125962168
      - 0.16588934778701514
      - 0.16603288159240037
      - 0.16264033323386684
      - 0.15974500478478149
      - 0.187005972082261
      - 0.15912874095374718
      - 0.1603744582971558
      - 0.15716583171160892
      - 0.16094146762043238
      - 0.16289313277229667
      - 0.16015607735607773
      - 0.15303540241438895
      - 0.17047216842183843
      - 0.15800466405926272
      - 0.15691097028320655
      - 0.1543221946922131
      - 0.15661524143069983
      - 0.15600657428149134
      - 0.15599131188355386
      - 0.15797443239716813
      - 0.1521740768221207
      - 0.15892406657803804
      - 0.15245852957013994
      - 0.1537630021921359
      - 0.14977802970679477
      - 0.15158614417305216
      - 0.15235732262954116
      - 0.15119745692936704
      - 0.15506176184862852
      - 0.15148171078180894
      - 0.15203515195753425
      - 0.14480009640101343
      - 0.14989609038457274
      - 0.1493482778314501
      - 0.15845912916120142
      - 0.15292522235540673
      - 0.15069432428572327
      - 0.14553216513013467
      - 0.14887300733244047
      - 0.15022524748928845
      - 0.14850589376874268
      - 0.1520073178689927
      - 0.14779522875323892
      - 0.16560722974827513
      - 0.14800643525086343
      - 0.1550515271956101
      - 0.14504374179523438
      - 0.14621409919345751
      - 0.1475211760844104
      - 0.1448021150426939
      - 0.14694315497763455
      val_loss:
      - 0.27613336592912674
      - 0.346184425172396
      - 0.2960369838401675
      - 0.26624296174850315
      - 0.22407816641498357
      - 0.20617235190002248
      - 0.20398742257384583
      - 0.21137780684512109
      - 0.194588816317264
      - 0.17986087111057714
      - 0.18578973109833896
      - 0.17926836613332853
      - 0.17235541250556707
      - 0.17020081891678274
      - 0.1805586180416867
      - 0.17256250214995816
      - 0.16849870508303866
      - 0.16426474205218256
      - 0.16662725998321548
      - 0.16719698125962168
      - 0.16588934778701514
      - 0.16603288159240037
      - 0.16264033323386684
      - 0.15974500478478149
      - 0.187005972082261
      - 0.15912874095374718
      - 0.1603744582971558
      - 0.15716583171160892
      - 0.16094146762043238
      - 0.16289313277229667
      - 0.16015607735607773
      - 0.15303540241438895
      - 0.17047216842183843
      - 0.15800466405926272
      - 0.15691097028320655
      - 0.1543221946922131
      - 0.15661524143069983
      - 0.15600657428149134
      - 0.15599131188355386
      - 0.15797443239716813
      - 0.1521740768221207
      - 0.15892406657803804
      - 0.15245852957013994
      - 0.1537630021921359
      - 0.14977802970679477
      - 0.15158614417305216
      - 0.15235732262954116
      - 0.15119745692936704
      - 0.15506176184862852
      - 0.15148171078180894
      - 0.1262277439236641
      - 0.14480009640101343
      - 0.14989609038457274
      - 0.1493482778314501
      - 0.15845912916120142
      - 0.15292522235540673
      - 0.15069432428572327
      - 0.14553216513013467
      - 0.14887300733244047
      - 0.15022524748928845
      - 0.14850589376874268
      - 0.1520073178689927
      - 0.14779522875323892
      - 0.16560722974827513
      - 0.14800643525086343
      - 0.1550515271956101
      - 0.14504374179523438
      - 0.14621409919345751
      - 0.1475211760844104
      - 0.1448021150426939
      - 0.14694315497763455
  task_id: 1
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_naive/task_2_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.4596187418358904
      NRMSE: 0.8228018020712213
      mean_wQuantileLoss: 0.355335262039553
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      train_loss:
      - 0.40948468656279147
      - 0.267779357614927
      - 0.23161187814548612
      - 0.20422775950282812
      - 0.1988285127445124
      - 0.1955993581796065
      - 0.1933081682655029
      - 0.19246470124926418
      - 0.18902965623419732
      - 0.1861172957578674
      - 0.1885739605058916
      - 0.17548350436845794
      - 0.17829684301977977
      - 0.17919728497508913
      - 0.17732345080003142
      - 0.17203789937775582
      - 0.17174463614355773
      - 0.17576775804627687
      - 0.17630482860840857
      - 0.17354856041492894
      - 0.17248408106388524
      - 0.16719516646116972
      - 0.16762952896533534
      - 0.17017391446279362
      - 0.17014729813672602
      - 0.16744170541642234
      - 0.16780326701700687
      - 0.16708864422980696
      - 0.166333144181408
      - 0.16379593301098794
      - 0.16542218648828566
      - 0.1669494491070509
      - 0.1677754232659936
      - 0.1639436180703342
      - 0.16260700160637498
      - 0.16875894117401913
      - 0.1655528325936757
      - 0.16317152179544792
      - 0.1632016018847935
      - 0.15776066854596138
      - 0.1642529835808091
      - 0.1598383939708583
      - 0.1626197583391331
      - 0.16119095223257318
      - 0.16373194981133565
      - 0.16576328611699864
      - 0.15768391179153696
      - 0.16336273710476235
      - 0.1595405769185163
      - 0.16167678526835516
      - 0.1597870448604226
      - 0.16169352387078106
      - 0.16196710406802595
      - 0.16028217424172908
      - 0.15850885584950447
      - 0.15899626188911498
      - 0.15161871910095215
      - 0.15634388325270265
      - 0.15881970949703828
      - 0.15871930500725284
      - 0.15894001361448318
      - 0.15638182865222916
      - 0.15760274388594553
      - 0.16445464052958414
      - 0.16015898442128673
      - 0.15497826552018523
      - 0.16324235929641873
      - 0.15352655237074941
      - 0.15738044161116704
      - 0.15070351044414565
      - 0.160277868504636
      - 0.15725018788361922
      - 0.1621728881727904
      - 0.15784524101763964
      - 0.15594413597136736
      - 0.15375151432817802
      - 0.153353868809063
      - 0.1543078916729428
      - 0.15832786064129323
      - 0.15170911024324596
      - 0.1548403960186988
      - 0.15656535618472844
      - 0.15431229810928926
      - 0.15635829349048436
      - 0.1561389463604428
      - 0.15361862728605047
      - 0.15142458624904975
      - 0.15298739715944976
      - 0.157247073831968
      - 0.1580877312226221
      val_loss:
      - 0.26645477712154386
      - 0.267779357614927
      - 0.23161187814548612
      - 0.20422775950282812
      - 0.1988285127445124
      - 0.1955993581796065
      - 0.1933081682655029
      - 0.19246470124926418
      - 0.18902965623419732
      - 0.1861172957578674
      - 0.1885739605058916
      - 0.17548350436845794
      - 0.17829684301977977
      - 0.17919728497508913
      - 0.17732345080003142
      - 0.17203789937775582
      - 0.17174463614355773
      - 0.17576775804627687
      - 0.17630482860840857
      - 0.17354856041492894
      - 0.17248408106388524
      - 0.16719516646116972
      - 0.16762952896533534
      - 0.17017391446279362
      - 0.17014729813672602
      - 0.16744170541642234
      - 0.16780326701700687
      - 0.16708864422980696
      - 0.166333144181408
      - 0.16379593301098794
      - 0.16542218648828566
      - 0.1669494491070509
      - 0.1677754232659936
      - 0.1639436180703342
      - 0.16260700160637498
      - 0.16875894117401913
      - 0.1655528325936757
      - 0.16317152179544792
      - 0.1632016018847935
      - 0.15776066854596138
      - 0.1642529835808091
      - 0.1598383939708583
      - 0.1626197583391331
      - 0.16119095223257318
      - 0.16373194981133565
      - 0.16576328611699864
      - 0.15768391179153696
      - 0.16336273710476235
      - 0.1595405769185163
      - 0.16167678526835516
      - 0.15338521897792817
      - 0.16169352387078106
      - 0.16196710406802595
      - 0.16028217424172908
      - 0.15850885584950447
      - 0.15899626188911498
      - 0.15161871910095215
      - 0.15634388325270265
      - 0.15881970949703828
      - 0.15871930500725284
      - 0.15894001361448318
      - 0.15638182865222916
      - 0.15760274388594553
      - 0.16445464052958414
      - 0.16015898442128673
      - 0.15497826552018523
      - 0.16324235929641873
      - 0.15352655237074941
      - 0.15738044161116704
      - 0.15070351044414565
      - 0.160277868504636
      - 0.15725018788361922
      - 0.1621728881727904
      - 0.15784524101763964
      - 0.15594413597136736
      - 0.15375151432817802
      - 0.153353868809063
      - 0.1543078916729428
      - 0.15832786064129323
      - 0.15170911024324596
      - 0.1548403960186988
      - 0.15656535618472844
      - 0.15431229810928926
      - 0.15635829349048436
      - 0.1561389463604428
      - 0.15361862728605047
      - 0.15142458624904975
      - 0.15298739715944976
      - 0.157247073831968
      - 0.1580877312226221
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_naive/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.20819434240769868
      NRMSE: 0.5001779840365809
      mean_wQuantileLoss: 0.17398871263763044
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      train_loss:
      - 0.4880399403627962
      - 0.3395194399636239
      - 0.31286079611163586
      - 0.2936811709078029
      - 0.29676260880660266
      - 0.29017070995178074
      - 0.2878294549882412
      - 0.28446086950134486
      - 0.2806785674765706
      - 0.28047520213294774
      - 0.2718180932570249
      - 0.27062983135692775
      - 0.26897136226762086
      - 0.2685751976678148
      - 0.2624406003160402
      - 0.2708633632864803
      - 0.26251810835674405
      - 0.2609661752358079
      - 0.2610827119788155
      - 0.2575906905112788
      - 0.2577194208279252
      - 0.2557712427806109
      - 0.2528944725636393
      - 0.25911186379380524
      - 0.25449602771550417
      - 0.25600663176737726
      - 0.25417212629690766
      - 0.25371048192027956
      - 0.25555878796149045
      - 0.25423799199052155
      - 0.25259210797958076
      - 0.2505190708907321
      - 0.2506265144329518
      - 0.25173113658092916
      - 0.24736637179739773
      - 0.2486055406043306
      - 0.25181832374073565
      - 0.24571223079692572
      - 0.2483192167710513
      - 0.24825644248630852
      - 0.24701287073548883
      - 0.24485999823082238
      - 0.2424726001918316
      - 0.24949592805933207
      - 0.24319754529278725
      - 0.2449688819469884
      - 0.2410583266755566
      - 0.24429476005025208
      - 0.24628297367598861
      - 0.24838650692254305
      - 0.2502311847638339
      - 0.2490645624930039
      - 0.23973198526073247
      - 0.24694143899250776
      - 0.23762085323687643
      - 0.2422223020112142
      - 0.245161616592668
      - 0.23906177119351923
      - 0.23709422908723354
      - 0.23343466618098319
      - 0.23879096924792975
      - 0.24166056455578655
      - 0.23840394848957658
      - 0.238845766056329
      - 0.23840065603144467
      - 0.243612494552508
      - 0.2437342373887077
      - 0.23516032134648412
      - 0.23962329921778291
      - 0.2430636138888076
      - 0.2385657859267667
      - 0.24015661037992686
      - 0.24192131648305804
      - 0.23923157854005694
      - 0.23861978377681226
      - 0.23387236532289535
      - 0.23891285702120513
      - 0.24133395357057452
      - 0.2428832509322092
      - 0.23829883988946676
      val_loss:
      - 0.29435610920190813
      - 0.3395194399636239
      - 0.31286079611163586
      - 0.2936811709078029
      - 0.29676260880660266
      - 0.29017070995178074
      - 0.2878294549882412
      - 0.28446086950134486
      - 0.2806785674765706
      - 0.28047520213294774
      - 0.2718180932570249
      - 0.27062983135692775
      - 0.26897136226762086
      - 0.2685751976678148
      - 0.2624406003160402
      - 0.2708633632864803
      - 0.26251810835674405
      - 0.2609661752358079
      - 0.2610827119788155
      - 0.2575906905112788
      - 0.2577194208279252
      - 0.2557712427806109
      - 0.2528944725636393
      - 0.25911186379380524
      - 0.25449602771550417
      - 0.25600663176737726
      - 0.25417212629690766
      - 0.25371048192027956
      - 0.25555878796149045
      - 0.25423799199052155
      - 0.25259210797958076
      - 0.2505190708907321
      - 0.2506265144329518
      - 0.25173113658092916
      - 0.24736637179739773
      - 0.2486055406043306
      - 0.25181832374073565
      - 0.24571223079692572
      - 0.2483192167710513
      - 0.24825644248630852
      - 0.24701287073548883
      - 0.24485999823082238
      - 0.2424726001918316
      - 0.24949592805933207
      - 0.24319754529278725
      - 0.2449688819469884
      - 0.2410583266755566
      - 0.24429476005025208
      - 0.24628297367598861
      - 0.24838650692254305
      - 0.243820396065712
      - 0.2490645624930039
      - 0.23973198526073247
      - 0.24694143899250776
      - 0.23762085323687643
      - 0.2422223020112142
      - 0.245161616592668
      - 0.23906177119351923
      - 0.23709422908723354
      - 0.23343466618098319
      - 0.23879096924792975
      - 0.24166056455578655
      - 0.23840394848957658
      - 0.238845766056329
      - 0.23840065603144467
      - 0.243612494552508
      - 0.2437342373887077
      - 0.23516032134648412
      - 0.23962329921778291
      - 0.2430636138888076
      - 0.2385657859267667
      - 0.24015661037992686
      - 0.24192131648305804
      - 0.23923157854005694
      - 0.23861978377681226
      - 0.23387236532289535
      - 0.23891285702120513
      - 0.24133395357057452
      - 0.2428832509322092
      - 0.23829883988946676
  task_id: 3
