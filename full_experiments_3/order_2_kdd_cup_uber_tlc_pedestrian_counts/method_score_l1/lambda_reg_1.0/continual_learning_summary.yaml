continual_learning_setup:
  method: score_l1
  method_params:
    lambda_reg: 1.0
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
  - train_pedestrian_counts.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l1/lambda_reg_1.0/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 1.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.43922404928925185
      NRMSE: 0.8134246231181262
      mean_wQuantileLoss: 0.33521663772045407
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      train_loss:
      - 0.3974357070401311
      - 0.27631965884938836
      - 0.24999526469036937
      - 0.21503499860409647
      - 0.2099426886998117
      - 0.20524597563780844
      - 0.19825596327427775
      - 0.1992253724602051
      - 0.19064683449687436
      - 0.18223202356602997
      - 0.1888882332132198
      - 0.1782034649513662
      - 0.18062626186292619
      - 0.18092247640015557
      - 0.1805898552411236
      - 0.1772132752230391
      - 0.1764809694723226
      - 0.1696835614857264
      - 0.1699307746021077
      - 0.1763406494865194
      - 0.16792575764702633
      - 0.1733401147648692
      - 0.17234791204100475
      - 0.16474802256561816
      - 0.17140093119814992
      - 0.17217191914096475
      - 0.16228854085784405
      - 0.1724791785236448
      - 0.17325516761047766
      - 0.16659810964483768
      - 0.16557579330401495
      - 0.16059386916458607
      - 0.16956443479284644
      - 0.16978487104643136
      - 0.1624477962614037
      - 0.16529753821669146
      - 0.16965108504518867
      - 0.1656893262406811
      - 0.16609047580277547
      - 0.1681170371011831
      - 0.1627673048642464
      - 0.1633234239416197
      - 0.16333241132088006
      - 0.16290557850152254
      - 0.160858616232872
      - 0.15849346679169685
      - 0.1622501329984516
      - 0.15983117563882843
      - 0.1628072462626733
      - 0.16292837302898988
      - 0.15739041042979807
      - 0.15929577982751653
      - 0.16883599886205047
      - 0.16192853421671316
      - 0.1613093065097928
      - 0.1528754752362147
      - 0.16138754592975602
      - 0.15770165860885754
      - 0.15430774859851226
      - 0.15722594951512292
      - 0.15495234675472602
      - 0.16114773438312113
      - 0.1541282896650955
      - 0.16063767654122785
      - 0.1560482537606731
      - 0.1604362775105983
      - 0.15624335198663175
      - 0.16161451791413128
      - 0.15790895686950535
      - 0.15570061834296212
      - 0.16010837681824341
      - 0.15979021054226905
      - 0.1607857079943642
      - 0.15668989066034555
      - 0.15806074830470607
      - 0.1567972864722833
      val_loss:
      - 0.2623425334692001
      - 0.27631965884938836
      - 0.24999526469036937
      - 0.21503499860409647
      - 0.2099426886998117
      - 0.20524597563780844
      - 0.19825596327427775
      - 0.1992253724602051
      - 0.19064683449687436
      - 0.18223202356602997
      - 0.1888882332132198
      - 0.1782034649513662
      - 0.18062626186292619
      - 0.18092247640015557
      - 0.1805898552411236
      - 0.1772132752230391
      - 0.1764809694723226
      - 0.1696835614857264
      - 0.1699307746021077
      - 0.1763406494865194
      - 0.16792575764702633
      - 0.1733401147648692
      - 0.17234791204100475
      - 0.16474802256561816
      - 0.17140093119814992
      - 0.17217191914096475
      - 0.16228854085784405
      - 0.1724791785236448
      - 0.17325516761047766
      - 0.16659810964483768
      - 0.16557579330401495
      - 0.16059386916458607
      - 0.16956443479284644
      - 0.16978487104643136
      - 0.1624477962614037
      - 0.16529753821669146
      - 0.16965108504518867
      - 0.1656893262406811
      - 0.16609047580277547
      - 0.1681170371011831
      - 0.1627673048642464
      - 0.1633234239416197
      - 0.16333241132088006
      - 0.16290557850152254
      - 0.160858616232872
      - 0.15849346679169685
      - 0.1622501329984516
      - 0.15983117563882843
      - 0.1628072462626733
      - 0.16292837302898988
      - 0.15675411224365235
      - 0.15929577982751653
      - 0.16883599886205047
      - 0.16192853421671316
      - 0.1613093065097928
      - 0.1528754752362147
      - 0.16138754592975602
      - 0.15770165860885754
      - 0.15430774859851226
      - 0.15722594951512292
      - 0.15495234675472602
      - 0.16114773438312113
      - 0.1541282896650955
      - 0.16063767654122785
      - 0.1560482537606731
      - 0.1604362775105983
      - 0.15624335198663175
      - 0.16161451791413128
      - 0.15790895686950535
      - 0.15570061834296212
      - 0.16010837681824341
      - 0.15979021054226905
      - 0.1607857079943642
      - 0.15668989066034555
      - 0.15806074830470607
      - 0.1567972864722833
  task_id: 1
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l1/lambda_reg_1.0/task_2_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 1.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.6321969939420689
      NRMSE: 2.7876301086429542
      mean_wQuantileLoss: 0.5095064887466753
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 1.188115996774286
      - 1.0653948243707418
      - 1.039579569362104
      - 1.0217276765033603
      - 1.0092193097807467
      - 1.0140321515500546
      - 1.0097287083044648
      - 1.0020653535611928
      - 0.9962725015357137
      - 0.9942903467454016
      - 0.98356648767367
      - 0.984919112175703
      - 0.9882180448621511
      - 0.9745134869590402
      - 0.977804166264832
      - 0.9803339834325016
      - 0.9811087562702596
      - 0.9683568445034325
      - 0.9783399067819118
      - 0.9773936932906508
      - 0.9698058739304543
      val_loss:
      - 0.4973714590072632
      - 1.0653948243707418
      - 1.039579569362104
      - 1.0217276765033603
      - 1.0092193097807467
      - 1.0140321515500546
      - 1.0097287083044648
      - 1.0020653535611928
      - 0.9962725015357137
      - 0.9942903467454016
      - 0.98356648767367
      - 0.984919112175703
      - 0.9882180448621511
      - 0.9745134869590402
      - 0.977804166264832
      - 0.9803339834325016
      - 0.9811087562702596
      - 0.9683568445034325
      - 0.9783399067819118
      - 0.9773936932906508
      - 0.9698058739304543
  task_id: 2
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_2_kdd_cup_uber_tlc_pedestrian_counts/method_score_l1/lambda_reg_1.0/task_3_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 1.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l1
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.5383988576734979
      NRMSE: 1.313383113837148
      mean_wQuantileLoss: 0.3630778666364757
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 0.9695394597947598
      - 0.7871596654877067
      - 0.74278006516397
      - 0.741891042329371
      - 0.6959323813207448
      - 0.690322533249855
      - 0.6873931046575308
      - 0.6747808586806059
      - 0.6705147558823228
      - 0.7014796691946685
      - 0.6702075502835214
      - 0.6684976946562529
      - 0.6604412160813808
      - 0.6460572760552168
      - 0.6795093049295247
      - 0.6469095521606505
      - 0.6355349998921156
      - 0.6539497547782958
      - 0.6370869451202452
      - 0.6277979472652078
      - 0.6453998093493283
      val_loss:
      - 0.6221411824226379
      - 0.7871596654877067
      - 0.74278006516397
      - 0.741891042329371
      - 0.6959323813207448
      - 0.690322533249855
      - 0.6873931046575308
      - 0.6747808586806059
      - 0.6705147558823228
      - 0.7014796691946685
      - 0.6702075502835214
      - 0.6684976946562529
      - 0.6604412160813808
      - 0.6460572760552168
      - 0.6795093049295247
      - 0.6469095521606505
      - 0.6355349998921156
      - 0.6539497547782958
      - 0.6370869451202452
      - 0.6277979472652078
      - 0.6453998093493283
  task_id: 3
