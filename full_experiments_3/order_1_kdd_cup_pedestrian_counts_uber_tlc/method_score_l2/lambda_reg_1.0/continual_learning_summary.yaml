continual_learning_setup:
  method: score_l2
  method_params:
    lambda_reg: 1.0
  num_tasks: 3
  task_sequence:
  - train_kdd_cup.yaml
  - train_pedestrian_counts.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l2/lambda_reg_1.0/task_1_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 1.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.4212421021860411
      NRMSE: 0.8002795486471984
      mean_wQuantileLoss: 0.3301022759496786
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      - 116
      - 117
      - 118
      - 119
      - 120
      - 121
      train_loss:
      - 0.419161218800582
      - 0.2917103102663532
      - 0.23739832499995828
      - 0.21787372208200395
      - 0.20983704342506826
      - 0.20130193821387365
      - 0.207106240617577
      - 0.20121194201055914
      - 0.18049489328404889
      - 0.1945113984402269
      - 0.1846686212811619
      - 0.17992267798399553
      - 0.18872151081450284
      - 0.18114137992961332
      - 0.18079645273974165
      - 0.17728541663382202
      - 0.17572554433718324
      - 0.17174581123981625
      - 0.17402944481000304
      - 0.17027799470815808
      - 0.17245507758343592
      - 0.1706398706883192
      - 0.17067679582396522
      - 0.1687702769995667
      - 0.16819812689209357
      - 0.17134365160018206
      - 0.17199393938062713
      - 0.1711040245136246
      - 0.17381176922935992
      - 0.16884128836682066
      - 0.16644231393001974
      - 0.16686812049010769
      - 0.16551745525794104
      - 0.16538075060816482
      - 0.16066118044545874
      - 0.16684463765705004
      - 0.1673153374576941
      - 0.16672397585352883
      - 0.1662412298610434
      - 0.1662338575697504
      - 0.16249468782916665
      - 0.16604498703964055
      - 0.18406599437003024
      - 0.1644285082584247
      - 0.1638544494053349
      - 0.16465282544959337
      - 0.1601848605205305
      - 0.16467185935471207
      - 0.16182223451323807
      - 0.16557840286986902
      - 0.16554527846165001
      - 0.16035403322894126
      - 0.16179511841619387
      - 0.15881249477388337
      - 0.15914245054591447
      - 0.15722780540818349
      - 0.15746427129488438
      - 0.16204873321112245
      - 0.16136908740736544
      - 0.15759925759630278
      - 0.16161952284164727
      - 0.15830978780286387
      - 0.16398979641962796
      - 0.16355281259166077
      - 0.15721874247537926
      - 0.16029197606258094
      - 0.1680790184182115
      - 0.16170496807899326
      - 0.15595297020627186
      - 0.15671173221198842
      - 0.156450662703719
      - 0.15251639857888222
      - 0.15298447315581143
      - 0.15655871736817062
      - 0.15559634222881868
      - 0.1564184261369519
      - 0.15979709423845634
      - 0.15916841477155685
      - 0.15610713663045317
      - 0.15648479230003431
      - 0.1552744404762052
      - 0.15073877054965124
      - 0.15260349225718528
      - 0.1548683110740967
      - 0.1590595641755499
      - 0.15397495467914268
      - 0.15755084721604362
      - 0.15335143165430054
      - 0.16123540099943057
      - 0.15780377516057342
      - 0.1564137396053411
      - 0.15282495447900146
      - 0.15065277233952656
      - 0.1568599913152866
      - 0.15459231263957918
      - 0.15208741487003863
      - 0.15097141556907445
      - 0.1535480486927554
      - 0.1532005513436161
      - 0.15517292864387855
      - 0.15106092346832156
      - 0.15472696023061872
      - 0.15344664576696232
      - 0.1535268043517135
      - 0.15141755319200456
      - 0.15362437593284994
      - 0.15198405616683885
      - 0.14933000691235065
      - 0.1492338672396727
      - 0.1543661400792189
      - 0.1518174429074861
      - 0.15246482862858102
      - 0.1512152921059169
      - 0.15229004755383357
      - 0.14835795463295653
      - 0.14944528666092083
      - 0.15015139401657507
      - 0.15227790613425896
      - 0.14990836125798523
      - 0.1485415879287757
      - 0.1519909147755243
      val_loss:
      - 0.27862039804458616
      - 0.2917103102663532
      - 0.23739832499995828
      - 0.21787372208200395
      - 0.20983704342506826
      - 0.20130193821387365
      - 0.207106240617577
      - 0.20121194201055914
      - 0.18049489328404889
      - 0.1945113984402269
      - 0.1846686212811619
      - 0.17992267798399553
      - 0.18872151081450284
      - 0.18114137992961332
      - 0.18079645273974165
      - 0.17728541663382202
      - 0.17572554433718324
      - 0.17174581123981625
      - 0.17402944481000304
      - 0.17027799470815808
      - 0.17245507758343592
      - 0.1706398706883192
      - 0.17067679582396522
      - 0.1687702769995667
      - 0.16819812689209357
      - 0.17134365160018206
      - 0.17199393938062713
      - 0.1711040245136246
      - 0.17381176922935992
      - 0.16884128836682066
      - 0.16644231393001974
      - 0.16686812049010769
      - 0.16551745525794104
      - 0.16538075060816482
      - 0.16066118044545874
      - 0.16684463765705004
      - 0.1673153374576941
      - 0.16672397585352883
      - 0.1662412298610434
      - 0.1662338575697504
      - 0.16249468782916665
      - 0.16604498703964055
      - 0.18406599437003024
      - 0.1644285082584247
      - 0.1638544494053349
      - 0.16465282544959337
      - 0.1601848605205305
      - 0.16467185935471207
      - 0.16182223451323807
      - 0.16557840286986902
      - 0.1607203260064125
      - 0.16035403322894126
      - 0.16179511841619387
      - 0.15881249477388337
      - 0.15914245054591447
      - 0.15722780540818349
      - 0.15746427129488438
      - 0.16204873321112245
      - 0.16136908740736544
      - 0.15759925759630278
      - 0.16161952284164727
      - 0.15830978780286387
      - 0.16398979641962796
      - 0.16355281259166077
      - 0.15721874247537926
      - 0.16029197606258094
      - 0.1680790184182115
      - 0.16170496807899326
      - 0.15595297020627186
      - 0.15671173221198842
      - 0.156450662703719
      - 0.15251639857888222
      - 0.15298447315581143
      - 0.15655871736817062
      - 0.15559634222881868
      - 0.1564184261369519
      - 0.15979709423845634
      - 0.15916841477155685
      - 0.15610713663045317
      - 0.15648479230003431
      - 0.1552744404762052
      - 0.15073877054965124
      - 0.15260349225718528
      - 0.1548683110740967
      - 0.1590595641755499
      - 0.15397495467914268
      - 0.15755084721604362
      - 0.15335143165430054
      - 0.16123540099943057
      - 0.15780377516057342
      - 0.1564137396053411
      - 0.15282495447900146
      - 0.15065277233952656
      - 0.1568599913152866
      - 0.15459231263957918
      - 0.15208741487003863
      - 0.15097141556907445
      - 0.1535480486927554
      - 0.1532005513436161
      - 0.15517292864387855
      - 0.1320090800523758
      - 0.15472696023061872
      - 0.15344664576696232
      - 0.1535268043517135
      - 0.15141755319200456
      - 0.15362437593284994
      - 0.15198405616683885
      - 0.14933000691235065
      - 0.1492338672396727
      - 0.1543661400792189
      - 0.1518174429074861
      - 0.15246482862858102
      - 0.1512152921059169
      - 0.15229004755383357
      - 0.14835795463295653
      - 0.14944528666092083
      - 0.15015139401657507
      - 0.15227790613425896
      - 0.14990836125798523
      - 0.1485415879287757
      - 0.1519909147755243
  task_id: 1
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l2/lambda_reg_1.0/task_2_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 1.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.24418304651904946
      NRMSE: 0.8326982244855863
      mean_wQuantileLoss: 0.19702044037214556
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      - 116
      - 117
      - 118
      - 119
      - 120
      - 121
      train_loss:
      - 0.7840315259527415
      - 0.5085781335365027
      - 0.5220604711212218
      - 0.368507549748756
      - 0.32144499418791384
      - 0.30696570535656065
      - 0.304536335519515
      - 0.26393952884245664
      - 0.26308974460698664
      - 0.255820722784847
      - 0.24675870744977146
      - 0.24408835894428194
      - 0.24089083599392325
      - 0.24639280268456787
      - 0.23382723855320364
      - 0.2285215777810663
      - 0.23409735481254756
      - 0.22787662385962903
      - 0.2223169362405315
      - 0.22210832498967648
      - 0.22093710978515446
      - 0.21516938030254096
      - 0.21487296698614955
      - 0.21930868574418128
      - 0.23177896672859788
      - 0.21539869590196759
      - 0.21535530069377273
      - 0.2146411562571302
      - 0.21247501531615853
      - 0.22792163689155132
      - 0.21962564636487514
      - 0.22798660525586456
      - 0.21180331916548312
      - 0.21332230744883418
      - 0.21230666537303478
      - 0.20818749017780647
      - 0.21013936318922788
      - 0.24581115855835378
      - 0.20853730523958802
      - 0.2131857017520815
      - 0.20915836980566382
      - 0.20418996893567964
      - 0.2072173625929281
      - 0.2067230719840154
      - 0.2121413517743349
      - 0.23122897488065064
      - 0.20722359523642808
      - 0.20356217282824218
      - 0.21599209087435156
      - 0.20669938868377358
      - 0.20712092309258878
      - 0.20126275450456887
      - 0.20441108045633882
      - 0.20514498779084533
      - 0.20167746138758957
      - 0.20547427900601178
      - 0.19974958256352693
      - 0.20375265798065811
      - 0.20764124765992165
      - 0.20013907831162214
      - 0.20237426250241697
      - 0.21004093997180462
      - 0.20339941990096122
      - 0.20458733278792351
      - 0.20099467691034079
      - 0.2028868340421468
      - 0.21481833420693874
      - 0.19994011940434575
      - 0.19907141372095793
      - 0.20153076003771275
      - 0.19889969599898905
      - 0.2256908668205142
      - 0.19900224974844605
      - 0.21229649079032242
      - 0.20125206385273486
      - 0.19963303057011217
      - 0.20003229496069252
      - 0.197991281747818
      - 0.19478042947594076
      - 0.2122217764845118
      - 0.19645283778663725
      - 0.1973337009549141
      - 0.1996554594952613
      - 0.19949178281240165
      - 0.19674362381920218
      - 0.19763955753296614
      - 0.19238515791948885
      - 0.19756287639029324
      - 0.22598944138735533
      - 0.1949975525494665
      - 0.19089915708173066
      - 0.19877066009212285
      - 0.1935004215920344
      - 0.19846421357942745
      - 0.19616717181634158
      - 0.20376180415041745
      - 0.19522828119806945
      - 0.1966990982182324
      - 0.19756947318091989
      - 0.20043450535740703
      - 0.19390065898187459
      - 0.19635023118462414
      - 0.19301728892605752
      - 0.19515386526472867
      - 0.21092859969940037
      - 0.21844304574187845
      - 0.19560512714087963
      - 0.19101629441138357
      - 0.19438240118324757
      - 0.19463315652683377
      - 0.18945655471179634
      - 0.19191739067900926
      - 0.19793844583909959
      - 0.20331688574515283
      - 0.19263427273835987
      - 0.1928108986467123
      - 0.19236259732861072
      - 0.1945880897110328
      - 0.19137652090284973
      - 0.1965466453693807
      - 0.1962286449270323
      val_loss:
      - 0.5313588529825211
      - 0.5085781335365027
      - 0.5220604711212218
      - 0.368507549748756
      - 0.32144499418791384
      - 0.30696570535656065
      - 0.304536335519515
      - 0.26393952884245664
      - 0.26308974460698664
      - 0.255820722784847
      - 0.24675870744977146
      - 0.24408835894428194
      - 0.24089083599392325
      - 0.24639280268456787
      - 0.23382723855320364
      - 0.2285215777810663
      - 0.23409735481254756
      - 0.22787662385962903
      - 0.2223169362405315
      - 0.22210832498967648
      - 0.22093710978515446
      - 0.21516938030254096
      - 0.21487296698614955
      - 0.21930868574418128
      - 0.23177896672859788
      - 0.21539869590196759
      - 0.21535530069377273
      - 0.2146411562571302
      - 0.21247501531615853
      - 0.22792163689155132
      - 0.21962564636487514
      - 0.22798660525586456
      - 0.21180331916548312
      - 0.21332230744883418
      - 0.21230666537303478
      - 0.20818749017780647
      - 0.21013936318922788
      - 0.24581115855835378
      - 0.20853730523958802
      - 0.2131857017520815
      - 0.20915836980566382
      - 0.20418996893567964
      - 0.2072173625929281
      - 0.2067230719840154
      - 0.2121413517743349
      - 0.23122897488065064
      - 0.20722359523642808
      - 0.20356217282824218
      - 0.21599209087435156
      - 0.20669938868377358
      - 0.21832168102264404
      - 0.20126275450456887
      - 0.20441108045633882
      - 0.20514498779084533
      - 0.20167746138758957
      - 0.20547427900601178
      - 0.19974958256352693
      - 0.20375265798065811
      - 0.20764124765992165
      - 0.20013907831162214
      - 0.20237426250241697
      - 0.21004093997180462
      - 0.20339941990096122
      - 0.20458733278792351
      - 0.20099467691034079
      - 0.2028868340421468
      - 0.21481833420693874
      - 0.19994011940434575
      - 0.19907141372095793
      - 0.20153076003771275
      - 0.19889969599898905
      - 0.2256908668205142
      - 0.19900224974844605
      - 0.21229649079032242
      - 0.20125206385273486
      - 0.19963303057011217
      - 0.20003229496069252
      - 0.197991281747818
      - 0.19478042947594076
      - 0.2122217764845118
      - 0.19645283778663725
      - 0.1973337009549141
      - 0.1996554594952613
      - 0.19949178281240165
      - 0.19674362381920218
      - 0.19763955753296614
      - 0.19238515791948885
      - 0.19756287639029324
      - 0.22598944138735533
      - 0.1949975525494665
      - 0.19089915708173066
      - 0.19877066009212285
      - 0.1935004215920344
      - 0.19846421357942745
      - 0.19616717181634158
      - 0.20376180415041745
      - 0.19522828119806945
      - 0.1966990982182324
      - 0.19756947318091989
      - 0.20043450535740703
      - 0.16765937954187393
      - 0.19635023118462414
      - 0.19301728892605752
      - 0.19515386526472867
      - 0.21092859969940037
      - 0.21844304574187845
      - 0.19560512714087963
      - 0.19101629441138357
      - 0.19438240118324757
      - 0.19463315652683377
      - 0.18945655471179634
      - 0.19191739067900926
      - 0.19793844583909959
      - 0.20331688574515283
      - 0.19263427273835987
      - 0.1928108986467123
      - 0.19236259732861072
      - 0.1945880897110328
      - 0.19137652090284973
      - 0.1965466453693807
      - 0.1962286449270323
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_1_kdd_cup_pedestrian_counts_uber_tlc/method_score_l2/lambda_reg_1.0/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.0
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 1.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3229455862297247
      NRMSE: 4.200153096604257
      mean_wQuantileLoss: 0.47113863121866867
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      train_loss:
      - 2.013142407871783
      - 1.6246879096142948
      - 1.5505294045433402
      - 1.5157175180502236
      - 1.4955472466535866
      - 1.4633528729900718
      - 1.493445174768567
      - 1.4255582629702985
      - 1.415467904880643
      - 1.4592560571618378
      - 1.4542230935767293
      - 1.4296085820533335
      - 1.4267072482034564
      - 1.424087395425886
      - 1.475928416941315
      - 1.4208558895625174
      - 1.4293849780224264
      - 1.3940021097660065
      - 1.4145134827122092
      - 1.4532699957489967
      - 1.391558094881475
      val_loss:
      - 0.6814937651157379
      - 1.6246879096142948
      - 1.5505294045433402
      - 1.5157175180502236
      - 1.4955472466535866
      - 1.4633528729900718
      - 1.493445174768567
      - 1.4255582629702985
      - 1.415467904880643
      - 1.4592560571618378
      - 1.4542230935767293
      - 1.4296085820533335
      - 1.4267072482034564
      - 1.424087395425886
      - 1.475928416941315
      - 1.4208558895625174
      - 1.4293849780224264
      - 1.3940021097660065
      - 1.4145134827122092
      - 1.4532699957489967
      - 1.391558094881475
  task_id: 3
