continual_learning_setup:
  method: dropout
  method_params:
    dropout_rate: 0.5
  num_tasks: 3
  task_sequence:
  - train_pedestrian_counts.yaml
  - train_kdd_cup.yaml
  - train_uber_tlc.yaml
task_results:
- config_path: ./configs/train_tsdiff/train_pedestrian_counts.yaml
  dataset: pedestrian_counts
  results:
    best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_dropout/dropout_rate_0.5/task_1_pedestrian_counts/pedestrian_counts_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: pedestrian_counts
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.5
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.0005
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.2109028221645026
      NRMSE: 0.7400395334873304
      mean_wQuantileLoss: 0.17447583632708627
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      - 72
      - 73
      - 74
      - 75
      - 76
      - 77
      - 78
      - 79
      - 80
      - 81
      - 82
      - 83
      - 84
      - 85
      - 86
      - 87
      - 88
      - 89
      - 90
      - 91
      - 92
      - 93
      - 94
      - 95
      - 96
      - 97
      - 98
      - 99
      - 100
      - 101
      - 102
      - 103
      - 104
      - 105
      - 106
      - 107
      - 108
      - 109
      - 110
      - 111
      - 112
      - 113
      - 114
      - 115
      - 116
      - 117
      - 118
      - 119
      - 120
      - 121
      train_loss:
      - 0.5482839872129261
      - 0.388842626940459
      - 0.3573176693171263
      - 0.3222215448040515
      - 0.3178964260732755
      - 0.30999777477700263
      - 0.2986717183375731
      - 0.3002015488455072
      - 0.29182758438400924
      - 0.3041830783477053
      - 0.26344914990477264
      - 0.2626096463063732
      - 0.250768756843172
      - 0.2581223745364696
      - 0.24862067238427699
      - 0.2392198396846652
      - 0.2334354791091755
      - 0.2760201273486018
      - 0.24013446213211864
      - 0.23155352589674294
      - 0.23209383245557547
      - 0.23901298188138753
      - 0.22575811890419573
      - 0.22305882396176457
      - 0.23009782074950635
      - 0.22104545100592077
      - 0.23013337212614715
      - 0.22808751242700964
      - 0.22209790034685284
      - 0.21762027707882226
      - 0.2272077117813751
      - 0.21889239968732
      - 0.21719665825366974
      - 0.21320030221249908
      - 0.20829263923224062
      - 0.21329475799575448
      - 0.2123297732323408
      - 0.21075360127724707
      - 0.21783281152602285
      - 0.2133609834127128
      - 0.20909390249289572
      - 0.21521937451325357
      - 0.21539260947611183
      - 0.20857408118899912
      - 0.20764558541122824
      - 0.22507177421357483
      - 0.21163376071490347
      - 0.2118892192374915
      - 0.20721623278222978
      - 0.20683502784231678
      - 0.21800070261815563
      - 0.2039993362268433
      - 0.20708899025339633
      - 0.21089884650427848
      - 0.20950588828418404
      - 0.20541330124251544
      - 0.21009479579515755
      - 0.21056265046354383
      - 0.20284728321712464
      - 0.20684575953055173
      - 0.2050045895157382
      - 0.20424874033778906
      - 0.20546767103951424
      - 0.2036492053885013
      - 0.20382429123856127
      - 0.2033863157266751
      - 0.2154434930998832
      - 0.19974245852790773
      - 0.20427998690865934
      - 0.20409272034885362
      - 0.19842464156681672
      - 0.20308395335450768
      - 0.20426095859147608
      - 0.200350331957452
      - 0.2135359818348661
      - 0.2023903228691779
      - 0.20013869047397748
      - 0.19655150221660733
      - 0.20924528106115758
      - 0.2025473880348727
      - 0.19612218206748366
      - 0.2011095789493993
      - 0.1971749821677804
      - 0.19938417058438063
      - 0.20144175039604306
      - 0.19365207775263116
      - 0.197264151123818
      - 0.20099017780739814
      - 0.1969016917864792
      - 0.1965827961685136
      - 0.19666559621691704
      - 0.19483258289983496
      - 0.19912945124087855
      - 0.19062275730539113
      - 0.19651396642439067
      - 0.19476570357801393
      - 0.1969482630956918
      - 0.22078269545454532
      - 0.19958567875437438
      - 0.19705087901093066
      - 0.1959672134835273
      - 0.20223013765644282
      - 0.201658817473799
      - 0.1944854815956205
      - 0.1944009525468573
      - 0.19770286546554416
      - 0.19964437768794596
      - 0.1995544782257639
      - 0.204024362959899
      - 0.1937578318757005
      - 0.19330183626152575
      - 0.1902048479532823
      - 0.1943166145356372
      - 0.19364614866208285
      - 0.19472469796892256
      - 0.19496396789327264
      - 0.1926824725815095
      - 0.18975121469702572
      - 0.19204282289138064
      - 0.19872723799198866
      - 0.19076842349022627
      val_loss:
      - 0.3687748312950134
      - 0.388842626940459
      - 0.3573176693171263
      - 0.3222215448040515
      - 0.3178964260732755
      - 0.30999777477700263
      - 0.2986717183375731
      - 0.3002015488455072
      - 0.29182758438400924
      - 0.3041830783477053
      - 0.26344914990477264
      - 0.2626096463063732
      - 0.250768756843172
      - 0.2581223745364696
      - 0.24862067238427699
      - 0.2392198396846652
      - 0.2334354791091755
      - 0.2760201273486018
      - 0.24013446213211864
      - 0.23155352589674294
      - 0.23209383245557547
      - 0.23901298188138753
      - 0.22575811890419573
      - 0.22305882396176457
      - 0.23009782074950635
      - 0.22104545100592077
      - 0.23013337212614715
      - 0.22808751242700964
      - 0.22209790034685284
      - 0.21762027707882226
      - 0.2272077117813751
      - 0.21889239968732
      - 0.21719665825366974
      - 0.21320030221249908
      - 0.20829263923224062
      - 0.21329475799575448
      - 0.2123297732323408
      - 0.21075360127724707
      - 0.21783281152602285
      - 0.2133609834127128
      - 0.20909390249289572
      - 0.21521937451325357
      - 0.21539260947611183
      - 0.20857408118899912
      - 0.20764558541122824
      - 0.22507177421357483
      - 0.21163376071490347
      - 0.2118892192374915
      - 0.20721623278222978
      - 0.20683502784231678
      - 0.2802188843488693
      - 0.2039993362268433
      - 0.20708899025339633
      - 0.21089884650427848
      - 0.20950588828418404
      - 0.20541330124251544
      - 0.21009479579515755
      - 0.21056265046354383
      - 0.20284728321712464
      - 0.20684575953055173
      - 0.2050045895157382
      - 0.20424874033778906
      - 0.20546767103951424
      - 0.2036492053885013
      - 0.20382429123856127
      - 0.2033863157266751
      - 0.2154434930998832
      - 0.19974245852790773
      - 0.20427998690865934
      - 0.20409272034885362
      - 0.19842464156681672
      - 0.20308395335450768
      - 0.20426095859147608
      - 0.200350331957452
      - 0.2135359818348661
      - 0.2023903228691779
      - 0.20013869047397748
      - 0.19655150221660733
      - 0.20924528106115758
      - 0.2025473880348727
      - 0.19612218206748366
      - 0.2011095789493993
      - 0.1971749821677804
      - 0.19938417058438063
      - 0.20144175039604306
      - 0.19365207775263116
      - 0.197264151123818
      - 0.20099017780739814
      - 0.1969016917864792
      - 0.1965827961685136
      - 0.19666559621691704
      - 0.19483258289983496
      - 0.19912945124087855
      - 0.19062275730539113
      - 0.19651396642439067
      - 0.19476570357801393
      - 0.1969482630956918
      - 0.22078269545454532
      - 0.19958567875437438
      - 0.19705087901093066
      - 0.11158780939877033
      - 0.20223013765644282
      - 0.201658817473799
      - 0.1944854815956205
      - 0.1944009525468573
      - 0.19770286546554416
      - 0.19964437768794596
      - 0.1995544782257639
      - 0.204024362959899
      - 0.1937578318757005
      - 0.19330183626152575
      - 0.1902048479532823
      - 0.1943166145356372
      - 0.19364614866208285
      - 0.19472469796892256
      - 0.19496396789327264
      - 0.1926824725815095
      - 0.18975121469702572
      - 0.19204282289138064
      - 0.19872723799198866
      - 0.19076842349022627
  task_id: 1
- config_path: ./configs/train_tsdiff/train_kdd_cup.yaml
  dataset: kdd_cup_2018_without_missing
  results:
    best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_dropout/dropout_rate_0.5/task_2_kdd_cup_2018_without_missing/kdd_cup_2018_without_missing_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 312
      dataset: kdd_cup_2018_without_missing
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.5
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: true
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 1
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.3373096015428023
      NRMSE: 0.7019642866908472
      mean_wQuantileLoss: 0.27188630479983233
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.4624321816954762
      - 0.3628725645830855
      - 0.3171561500057578
      - 0.29900866583921015
      - 0.2789539961377159
      - 0.25889660557731986
      - 0.2632619122741744
      - 0.24407980346586555
      - 0.2516389775555581
      - 0.24211239302530885
      - 0.24286305718123913
      - 0.24128412641584873
      - 0.2366723142331466
      - 0.23448558256495744
      - 0.24225825152825564
      - 0.23658770637121052
      - 0.23162393434904516
      - 0.22574491635896266
      - 0.2348035495961085
      - 0.23156809818465263
      - 0.22938546282239258
      - 0.22174495016224682
      - 0.22803826711606234
      - 0.21930431039072573
      - 0.22764545015525073
      - 0.23052849131636322
      - 0.22397190879564732
      - 0.22192852350417525
      - 0.22316272219177336
      - 0.22198104916606098
      - 0.22521322639659047
      - 0.2212083397898823
      - 0.22796394932083786
      - 0.2227468608180061
      - 0.22776623454410583
      - 0.22539380786474794
      - 0.22529709711670876
      - 0.22126067645149305
      - 0.22253605723381042
      - 0.222256914479658
      - 0.22224867541808635
      - 0.2228218086529523
      - 0.22495570802129805
      - 0.21740568045061082
      - 0.2216779360314831
      - 0.22426536690909415
      - 0.2185912384884432
      - 0.22515080799348652
      - 0.22455122019164264
      - 0.2195713219116442
      - 0.21990337362512946
      - 0.21581878140568733
      - 0.21798192290589213
      - 0.22028692218009382
      - 0.2191884402418509
      - 0.2164698639535345
      - 0.21663296723272651
      - 0.21906409226357937
      - 0.21900309110060334
      - 0.21988794615026563
      - 0.2198216945398599
      - 0.2189524251734838
      - 0.21660275594331324
      - 0.22546541329938918
      - 0.22594959940761328
      - 0.21450532297603786
      - 0.2158403202192858
      - 0.21712461474817246
      - 0.21344355784822255
      - 0.21331704687327147
      - 0.2116701154736802
      val_loss:
      - 0.3025003135204315
      - 0.3628725645830855
      - 0.3171561500057578
      - 0.29900866583921015
      - 0.2789539961377159
      - 0.25889660557731986
      - 0.2632619122741744
      - 0.24407980346586555
      - 0.2516389775555581
      - 0.24211239302530885
      - 0.24286305718123913
      - 0.24128412641584873
      - 0.2366723142331466
      - 0.23448558256495744
      - 0.24225825152825564
      - 0.23658770637121052
      - 0.23162393434904516
      - 0.22574491635896266
      - 0.2348035495961085
      - 0.23156809818465263
      - 0.22938546282239258
      - 0.22174495016224682
      - 0.22803826711606234
      - 0.21930431039072573
      - 0.22764545015525073
      - 0.23052849131636322
      - 0.22397190879564732
      - 0.22192852350417525
      - 0.22316272219177336
      - 0.22198104916606098
      - 0.22521322639659047
      - 0.2212083397898823
      - 0.22796394932083786
      - 0.2227468608180061
      - 0.22776623454410583
      - 0.22539380786474794
      - 0.22529709711670876
      - 0.22126067645149305
      - 0.22253605723381042
      - 0.222256914479658
      - 0.22224867541808635
      - 0.2228218086529523
      - 0.22495570802129805
      - 0.21740568045061082
      - 0.2216779360314831
      - 0.22426536690909415
      - 0.2185912384884432
      - 0.22515080799348652
      - 0.22455122019164264
      - 0.2195713219116442
      - 0.16733499616384506
      - 0.21581878140568733
      - 0.21798192290589213
      - 0.22028692218009382
      - 0.2191884402418509
      - 0.2164698639535345
      - 0.21663296723272651
      - 0.21906409226357937
      - 0.21900309110060334
      - 0.21988794615026563
      - 0.2198216945398599
      - 0.2189524251734838
      - 0.21660275594331324
      - 0.22546541329938918
      - 0.22594959940761328
      - 0.21450532297603786
      - 0.2158403202192858
      - 0.21712461474817246
      - 0.21344355784822255
      - 0.21331704687327147
      - 0.2116701154736802
  task_id: 2
- config_path: ./configs/train_tsdiff/train_uber_tlc.yaml
  dataset: uber_tlc_hourly
  results:
    best_checkpoint: full_experiments_3/order_3_pedestrian_counts_kdd_cup_uber_tlc/method_dropout/dropout_rate_0.5/task_3_uber_tlc_hourly/uber_tlc_hourly_checkpoint_best.pth
    config:
      batch_size: 64
      context_length: 336
      dataset: uber_tlc_hourly
      device: cuda:1
      diffusion_config: diffusion_small_config
      dropout_rate: 0.5
      eval_every: 50
      freq: H
      gradient_clip_val: 0.5
      init_skip: false
      lambda_reg: 0.0
      lr: 0.001
      max_epochs: 1000
      model: unconditional
      normalization: mean
      num_batches_per_epoch: 128
      num_samples: 16
      prediction_length: 24
      sampler: ddpm
      sampler_params:
        guidance: quantile
        scale: 2
      score_loss_type: l2
      setup: forecasting
      use_features: false
      use_lags: true
      use_validation_set: true
    metrics:
    - ND: 0.22482333309152785
      NRMSE: 0.5648454942294016
      mean_wQuantileLoss: 0.2188857055616306
      missing_scenario: none
      missing_values: 0
    training_history:
      epochs:
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
      - 10
      - 11
      - 12
      - 13
      - 14
      - 15
      - 16
      - 17
      - 18
      - 19
      - 20
      - 21
      - 22
      - 23
      - 24
      - 25
      - 26
      - 27
      - 28
      - 29
      - 30
      - 31
      - 32
      - 33
      - 34
      - 35
      - 36
      - 37
      - 38
      - 39
      - 40
      - 41
      - 42
      - 43
      - 44
      - 45
      - 46
      - 47
      - 48
      - 49
      - 50
      - 51
      - 52
      - 53
      - 54
      - 55
      - 56
      - 57
      - 58
      - 59
      - 60
      - 61
      - 62
      - 63
      - 64
      - 65
      - 66
      - 67
      - 68
      - 69
      - 70
      - 71
      train_loss:
      - 0.614834927720949
      - 0.39397172117605805
      - 0.36330513237044215
      - 0.3451197976246476
      - 0.3411021603969857
      - 0.33443602500483394
      - 0.33634612802416086
      - 0.32922798302024603
      - 0.33060354564804584
      - 0.3276146368589252
      - 0.32381102873478085
      - 0.31985116144642234
      - 0.32155903487000614
      - 0.3198238272452727
      - 0.3146786942379549
      - 0.31621940783225
      - 0.3198967649368569
      - 0.3150404173647985
      - 0.31449865363538265
      - 0.3196742683649063
      - 0.31558355886954814
      - 0.31402232218533754
      - 0.3121526464819908
      - 0.3108914525946602
      - 0.31077328347600996
      - 0.3093726618681103
      - 0.3107301751151681
      - 0.3170710453996435
      - 0.3130594561807811
      - 0.31520707276649773
      - 0.3154688876820728
      - 0.3111612608190626
      - 0.31408170866779983
      - 0.3124922792194411
      - 0.30974777147639543
      - 0.3070818354608491
      - 0.3051345720887184
      - 0.3120012659346685
      - 0.31581342255230993
      - 0.3088594488799572
      - 0.3111235913820565
      - 0.30664832913316786
      - 0.3114028135314584
      - 0.3074869238771498
      - 0.30953873007092625
      - 0.30473434331361204
      - 0.30531307589262724
      - 0.30548985628411174
      - 0.30925266444683075
      - 0.2999526619678363
      - 0.30306032637599856
      - 0.30369107355363667
      - 0.30459983681794256
      - 0.3018763264408335
      - 0.30209448700770736
      - 0.29511676530819386
      - 0.29976185597479343
      - 0.303336710203439
      - 0.3039666109252721
      - 0.3016346172662452
      - 0.2993331056786701
      - 0.30771358439233154
      - 0.2987755083013326
      - 0.30373418366070837
      - 0.2972277192166075
      - 0.29978380212560296
      - 0.3012249101884663
      - 0.3000885139917955
      - 0.2985840302426368
      - 0.2912165040615946
      - 0.29833640553988516
      val_loss:
      - 0.328582239151001
      - 0.39397172117605805
      - 0.36330513237044215
      - 0.3451197976246476
      - 0.3411021603969857
      - 0.33443602500483394
      - 0.33634612802416086
      - 0.32922798302024603
      - 0.33060354564804584
      - 0.3276146368589252
      - 0.32381102873478085
      - 0.31985116144642234
      - 0.32155903487000614
      - 0.3198238272452727
      - 0.3146786942379549
      - 0.31621940783225
      - 0.3198967649368569
      - 0.3150404173647985
      - 0.31449865363538265
      - 0.3196742683649063
      - 0.31558355886954814
      - 0.31402232218533754
      - 0.3121526464819908
      - 0.3108914525946602
      - 0.31077328347600996
      - 0.3093726618681103
      - 0.3107301751151681
      - 0.3170710453996435
      - 0.3130594561807811
      - 0.31520707276649773
      - 0.3154688876820728
      - 0.3111612608190626
      - 0.31408170866779983
      - 0.3124922792194411
      - 0.30974777147639543
      - 0.3070818354608491
      - 0.3051345720887184
      - 0.3120012659346685
      - 0.31581342255230993
      - 0.3088594488799572
      - 0.3111235913820565
      - 0.30664832913316786
      - 0.3114028135314584
      - 0.3074869238771498
      - 0.30953873007092625
      - 0.30473434331361204
      - 0.30531307589262724
      - 0.30548985628411174
      - 0.30925266444683075
      - 0.2999526619678363
      - 0.24752054512500762
      - 0.30369107355363667
      - 0.30459983681794256
      - 0.3018763264408335
      - 0.30209448700770736
      - 0.29511676530819386
      - 0.29976185597479343
      - 0.303336710203439
      - 0.3039666109252721
      - 0.3016346172662452
      - 0.2993331056786701
      - 0.30771358439233154
      - 0.2987755083013326
      - 0.30373418366070837
      - 0.2972277192166075
      - 0.29978380212560296
      - 0.3012249101884663
      - 0.3000885139917955
      - 0.2985840302426368
      - 0.2912165040615946
      - 0.29833640553988516
  task_id: 3
